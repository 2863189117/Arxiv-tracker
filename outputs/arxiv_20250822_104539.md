# arXiv 检索结果 / Results

## 1. Scaling Group Inference for Diverse and High-Quality Generation
- Authors：Gaurav Parmar, Or Patashnik, Daniil Ostashev, Kuan-Chieh Wang, Kfir Aberman, Srinivasa Narasimhan, Jun-Yan Zhu
- Venue：www
- Comments：Project website: https://www.cs.cmu.edu/~group-inference, GitHub:
  https://github.com/GaParmar/group-inference
- First：2025-08-21T17:59:57+00:00；Latest：2025-08-21T17:59:57+00:00
- Abs：http://arxiv.org/abs/2508.15773v1
- PDF：http://arxiv.org/pdf/2508.15773v1
- Code：https://github.com/GaParmar/group-inference
### [中文]
**中文翻译**
- 标题：规模化群体推理以实现多样化和高质量生成
- 摘要：生成模型通常独立采样输出，而近期的推理时引导和扩展算法主要关注提升单个样本的质量。然而在实际应用中，用户常需针对每个提示获取一组多张图像（如4-8张），独立采样易导致结果冗余，限制了用户选择并阻碍创意探索。本研究提出一种可扩展的群体推理方法，同步提升样本组的多样性与质量。我们将群体推理构建为二次整数分配问题：候选输出建模为图节点，通过选择子集来优化样本质量（一元项）并最大化群体多样性（二元项）。为显著提升运行效率，我们采用中间预测逐步剪枝候选集，使方法能扩展至大规模候选集。大量实验表明，相较于独立采样基线和最新推理算法，本方法显著提升了群体多样性与质量。该框架可泛化至多种任务，包括文本到图像、图像到图像、图像提示及视频生成，使生成模型能将多个输出视为有机整体而非独立样本。

> **TL;DR**: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples.

**Method Card (方法卡)**
- **Core Idea**: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples.
- **Venue**: www

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples.

**Method Card (方法卡)**
- **Core Idea**: Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples.
- **Venue**: www

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 2. CineScale: Free Lunch in High-Resolution Cinematic Visual Generation
- Authors：Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu
- Venue：ICCV 2025
- Comments：CineScale is an extended work of FreeScale (ICCV 2025). Project Page:
  https://eyeline-labs.github.io/CineScale/, Code Repo:
  https://github.com/Eyeline-Labs/CineScale
- First：2025-08-21T17:59:57+00:00；Latest：2025-08-21T17:59:57+00:00
- Abs：http://arxiv.org/abs/2508.15774v1
- PDF：http://arxiv.org/pdf/2508.15774v1
- Code：https://github.com/Eyeline-Labs/CineScale
- Project：https://eyeline-labs.github.io/CineScale/
### [中文]
**中文翻译**
- 标题：CineScale：高分辨率电影级视觉生成中的免费午餐
- 摘要：视觉扩散模型取得显著进展，但由于缺乏高分辨率数据和有限的计算资源，通常只能在受限分辨率下训练，这限制了其生成高保真高分辨率图像或视频的能力。近期研究探索了无需调参的策略以挖掘预训练模型在高分辨率视觉生成方面的潜力，但这些方法仍易产生带有重复图案的低质量视觉内容。关键障碍在于当模型生成超出训练分辨率的视觉内容时，高频信息不可避免地增加，导致误差累积产生不良重复模式。本研究提出CineScale——一种实现更高分辨率视觉生成的新型推理范式。针对两类视频生成架构的不同问题，我们设计了专用变体方案。与现有局限于高分辨率文生图（T2I）和文生视频（T2V）的基线方法不同，CineScale基于顶尖开源视频生成框架，进一步实现了高分辨率图生视频（I2V）和视频生视频（V2V）的合成。大量实验验证了我们的范式在扩展图像与视频模型高分辨率生成能力方面的优越性。值得注意的是，该方法无需微调即可实现8K图像生成，并通过极少量LoRA微调达成4K视频生成。生成视频样本请访问：https://eyeline-labs.github.io/CineScale/。

> **TL;DR**: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions.

**Method Card (方法卡)**
- **Core Idea**: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions.
- **Venue**: ICCV 2025

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions.

**Method Card (方法卡)**
- **Core Idea**: Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions.
- **Venue**: ICCV 2025

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 3. Visual Autoregressive Modeling for Instruction-Guided Image Editing
- Authors：Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei
- Comments：Source codes and models are available at
  https://github.com/HiDream-ai/VAREdit
- First：2025-08-21T17:59:32+00:00；Latest：2025-08-21T17:59:32+00:00
- Abs：http://arxiv.org/abs/2508.15772v1
- PDF：http://arxiv.org/pdf/2508.15772v1
- Code：https://github.com/HiDream-ai/VAREdit
### [中文]
**中文翻译**
- 标题：指令引导图像编辑的视觉自回归建模
- 摘要：扩散模型的最新进展为指令引导图像编辑带来了显著的视觉保真度。然而，其全局去噪过程本质上将编辑区域与整个图像上下文纠缠在一起，导致意外的伪修改并削弱对编辑指令的遵循性。相比之下，自回归模型通过将图像合成构建为离散视觉标记的序列过程，提供了独特的范式。其因果性和组合机制天然规避了基于扩散方法的遵循性挑战。本文提出VAREdit——一种将图像编辑重构为下一尺度预测问题的视觉自回归（VAR）框架。通过源图像特征和文本指令的条件化，VAREdit生成多尺度目标特征以实现精确编辑。该范式的核心挑战在于如何有效条件化源图像标记。我们发现最精细尺度的源特征无法有效指导较粗糙目标特征的预测。为弥合此差距，我们引入了尺度对齐参考（SAR）模块，将尺度匹配的条件信息注入首个自注意力层。VAREdit在编辑遵循性和效率方面均取得显著进展，在标准基准测试中，其GPT平衡分数比领先的扩散方法高出30%以上，且完成512×512编辑仅需1.2秒，比同等规模的UltraEdit快2.2倍。模型详见https://github.com/HiDream-ai/VAREdit。

> **TL;DR**: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing.

**Method Card (方法卡)**
- **Core Idea**: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing.

**Method Card (方法卡)**
- **Core Idea**: Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 4. SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass
- Authors：Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie
- Comments：Technical Report; Project Page: https://mengmouxu.github.io/SceneGen
- First：2025-08-21T17:59:16+00:00；Latest：2025-08-21T17:59:16+00:00
- Abs：http://arxiv.org/abs/2508.15769v1
- PDF：http://arxiv.org/pdf/2508.15769v1
- Project：https://mengmouxu.github.io/SceneGen
### [中文]
**中文翻译**
- 标题：SceneGen：单图像三维场景单次前馈生成
- 摘要：三维内容生成因其在VR/AR和具身智能中的应用近期引发广泛研究关注。本研究致力于解决从单张场景图像合成多个三维资产的挑战性任务。具体贡献包括：(i)提出SceneGen框架，通过输入场景图像及对应物体掩码，同步生成带几何与纹理的多项三维资产，无需优化过程或资产检索；(ii)设计新型特征聚合模块，在特征提取阶段融合视觉与几何编码器的局部与全局场景信息，结合位置预测头实现单次前馈生成三维资产及其相对空间位置；(iii)展示框架对多图像输入的直接扩展能力——尽管仅使用单图像训练，架构设计可提升多输入下的生成性能；(iv)通过大量定量与定性实验验证方法的高效性与强健生成能力。该范式为高质量三维内容生成提供新解决方案，有望推动下游任务的实际应用。代码与模型将公开于：https://mengmouxu.github.io/SceneGen

> **TL;DR**: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI.

**Method Card (方法卡)**
- **Task / Problem**: 3D Vision
- **Core Idea**: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI.

**Method Card (方法卡)**
- **Task / Problem**: 3D Vision
- **Core Idea**: 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 5. ATLAS: Decoupling Skeletal and Shape Parameters for Expressive   Parametric Human Modeling
- Authors：Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar
- Venue：ICCV 2025
- Comments：ICCV 2025; Website: https://jindapark.github.io/projects/atlas/
- First：2025-08-21T17:58:56+00:00；Latest：2025-08-21T17:58:56+00:00
- Abs：http://arxiv.org/abs/2508.15767v1
- PDF：http://arxiv.org/pdf/2508.15767v1
- Project：https://jindapark.github.io/projects/atlas/
### [中文]
**中文翻译**
- 标题：ATLAS：解耦骨骼与形态参数以实现富有表现力的参数化人体建模
- 摘要：参数化人体模型通过基于配准三维网格学习基向量，实现了跨姿态、体型和面部表情的丰富三维表达。然而，现有方法因训练数据多样性不足和建模假设限制，难以捕捉多样体态下的细节变化。传统范式先通过线性基优化体表，再从表面顶点回归内部骨骼关节点，导致骨骼与软组织间存在不良依赖，限制了直接控制身高和骨长的能力。为此，我们提出ATLAS——一个基于240台同步相机采集的60万次高分辨率扫描构建的高保真人体模型。该方法通过将网格表征锚定于人体骨骼，显式解耦形态与骨骼基向量，从而增强形态表现力、实现细粒度身体属性定制，并支持独立于软组织特征的关键点拟合。定量评估表明，ATLAS能更精准地拟合未知对象的多样姿态，其非线性姿态校正比线性模型更能有效捕捉复杂姿态。

> **TL;DR**: Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes.

**Method Card (方法卡)**
- **Task / Problem**: Grounding, 3D Vision
- **Core Idea**: Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes.
- **Venue**: ICCV 2025

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes.

**Method Card (方法卡)**
- **Task / Problem**: Grounding, 3D Vision
- **Core Idea**: Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes.
- **Venue**: ICCV 2025

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 6. Discovering Hidden Algebraic Structures via Transformers with Rank-Aware   Beam GRPO
- Authors：Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU
- First：2025-08-21T17:58:50+00:00；Latest：2025-08-21T17:58:50+00:00
- Abs：http://arxiv.org/abs/2508.15766v1
- PDF：http://arxiv.org/pdf/2508.15766v1
### [中文]
**中文翻译**
- 标题：通过具有秩感知束GRPO的Transformer发现隐藏代数结构
- 摘要：近期研究扩展了Transformer在逻辑推理和符号计算方面的能力。本文探讨了其在函数分解背景下进行非线性潜在模式发现的能力，重点关注多元多项式分解这一具有挑战性的代数任务。该问题在科学与工程领域应用广泛，被证明是NP难问题，需要精确性与洞察力。我们的贡献有三：首先开发了能精细控制问题复杂度的合成数据生成流程；其次通过监督学习训练Transformer模型，并在涉及缩放行为和泛化能力的四个关键维度进行评估；第三提出了束分组相对策略优化（BGRPO），这是一种适用于困难代数问题的秩感知强化学习方法。使用BGRPO进行微调可在将束宽减半的同时提升准确率，推理计算量降低约75%。此外，我们的模型在多项式简化任务中展现出竞争优势，在多类案例中超越Mathematica。

> **TL;DR**: Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations.

**Method Card (方法卡)**
- **Core Idea**: Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations.

**Method Card (方法卡)**
- **Core Idea**: Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 7. Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space
- Authors：Kiarash Kazari, Ezzeldin Shereen, György Dán
- Comments：Accepted for publication at ECAI 2025
- First：2025-08-21T17:58:36+00:00；Latest：2025-08-21T17:58:36+00:00
- Abs：http://arxiv.org/abs/2508.15764v1
- PDF：http://arxiv.org/pdf/2508.15764v1
### [中文]
**中文翻译**
- 标题：连续动作空间多智能体强化学习中对抗攻击的分布式检测
- 摘要：本文研究连续动作空间下协作多智能体强化学习系统遭受对抗攻击的检测问题。提出一种去中心化检测器，仅依赖智能体的局部观测数据，并利用可观测智能体正常行为的统计特征。该检测器采用深度神经网络将智能体正常行为近似为参数化多元高斯分布。基于预测的概率密度函数定义正态性评分并解析其均值与方差特征，借此采用双端CUSUM算法实时检测正态性评分偏离均值的情况，实现异常行为监测。通过在多种多智能体PettingZoo测试环境中对比不同前沿攻击方法的实验表明，本方法能有效检测具有显著影响的对抗攻击，尤其在所有测试环境中对最具影响力攻击的AUC-ROC分数均超过0.95，性能显著优于离散动作空间方案。

> **TL;DR**: We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space.

**Method Card (方法卡)**
- **Task / Problem**: Detection
- **Core Idea**: We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space.

**Method Card (方法卡)**
- **Task / Problem**: Detection
- **Core Idea**: We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 8. Intern-S1: A Scientific Multimodal Foundation Model
- Authors：Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou
- First：2025-08-21T17:58:00+00:00；Latest：2025-08-21T17:58:00+00:00
- Abs：http://arxiv.org/abs/2508.15763v1
- PDF：http://arxiv.org/pdf/2508.15763v1
- Code：https://huggingface.co/internlm/Intern-S1
### [中文]
**中文翻译**
- 标题：Intern-S1：科学多模态基础模型
- 摘要：近年来，开源基础模型大量涌现，在部分广受关注的领域取得显著进展，性能已十分接近闭源模型。然而，在高价值但更具挑战性的科学专业领域，这些领域要么仍依赖专家模型，要么通用基础模型的进展远落后于热门领域，远不足以变革科学研究，且开源模型与闭源模型在这些科学领域存在巨大差距。为缩小这一差距并探索迈向通用人工智能（AGI）的下一步，我们推出Intern-S1——一个具备通用理解与推理能力，并能分析多科学模态数据的专业通才模型。Intern-S1是多模态混合专家（MoE）模型，拥有280亿激活参数和2410亿总参数，基于5T token（其中包含超过2.5T科学领域token）进行持续预训练。在后训练阶段，该模型通过InternBootCamp先后进行离线和在线强化学习（RL），我们提出混合奖励机制（MoR）以协同推进超过1000项任务的RL训练。通过算法、数据和训练系统的集成创新，Intern-S1在在线RL训练中达到顶级性能。在综合评估基准测试中，Intern-S1在开源模型中展现出通用推理任务的竞争力，并在科学领域显著优于开源模型，在分子合成规划、反应条件预测、晶体热力学稳定性预测等专业任务中超越闭源最先进模型。模型详见：https://huggingface.co/internlm/Intern-S1。

> **TL;DR**: In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models.

**Method Card (方法卡)**
- **Task / Problem**: Vision-Language
- **Core Idea**: In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models.

**Method Card (方法卡)**
- **Task / Problem**: Vision-Language
- **Core Idea**: In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 9. Waver: Wave Your Way to Lifelike Video Generation
- Authors：Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng
- First：2025-08-21T17:56:10+00:00；Latest：2025-08-21T17:56:10+00:00
- Abs：http://arxiv.org/abs/2508.15761v1
- PDF：http://arxiv.org/pdf/2508.15761v1
- Code：https://github.com/FoundationVision/Waver
### [中文]
**中文翻译**
- 标题：Waver：以波动方式实现逼真视频生成
- 摘要：我们推出Waver，一个用于统一图像与视频生成的高性能基础模型。该模型能直接生成长度5至10秒、原生分辨率720p的视频，并支持上采样至1080p。在单一集成框架内同步支持文本生成视频（T2V）、图像生成视频（I2V）及文本生成图像（T2I）功能。通过引入混合流式DiT架构增强模态对齐并加速训练收敛。为确保训练数据质量，我们建立了完整的数据筛选流程，并手动标注训练基于MLLM的视频质量评估模型以筛选最优样本。此外，提供详细的训练与推理方案以促进高质量视频生成。基于这些创新，Waver在捕捉复杂运动、实现卓越运动幅度与时间一致性方面表现突出，在Artificial Analysis平台的T2V和I2V排行榜均位列前三（数据截至2025年7月30日北京时间10时），持续超越现有开源模型并媲美或领先尖端商业方案。本技术报告旨在助力社区更高效训练高质量视频生成模型，加速视频技术发展。官方页面：https://github.com/FoundationVision/Waver。

> **TL;DR**: We present Waver, a high-performance foundation model for unified image and video generation.

**Method Card (方法卡)**
- **Core Idea**: We present Waver, a high-performance foundation model for unified image and video generation.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: We present Waver, a high-performance foundation model for unified image and video generation.

**Method Card (方法卡)**
- **Core Idea**: We present Waver, a high-performance foundation model for unified image and video generation.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 10. LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on   Challenging Queries
- Authors：Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song
- First：2025-08-21T17:55:54+00:00；Latest：2025-08-21T17:55:54+00:00
- Abs：http://arxiv.org/abs/2508.15760v1
- PDF：http://arxiv.org/pdf/2508.15760v1
### [中文]
**中文翻译**
- 标题：LiveMCP-101：对支持MCP的智能体进行压力测试与复杂查询诊断
- 摘要：工具调用已成为AI智能体与现实世界交互并解决复杂任务的关键能力。虽然模型上下文协议（MCP）为工具集成提供了强大的标准化框架，但在真实动态场景中，衡量AI智能体如何有效利用多样化MCP工具完成多步骤任务的基准测试仍存在显著空白。本研究推出LiveMCP-101基准，包含101个精心筛选的真实查询（经过迭代式LLM重写与人工审核），需协调使用包括网络搜索、文件操作、数学推理和数据分析在内的多种MCP工具。此外，我们引入了一种创新评估方法，该方法基于真实执行计划而非原始API输出，更能反映现实环境的动态特性。实验表明，即使前沿LLMs的成功率也低于60%，凸显出工具协调方面的重大挑战。详细的消融实验和错误分析进一步揭示了不同的故障模式及令牌使用效率问题，为改进现有模型指明了具体方向。LiveMCP-101为评估真实场景下的智能体能力设立了严格标准，推动通过工具使用可靠执行复杂任务的自主AI系统发展。

> **TL;DR**: Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks.

**Method Card (方法卡)**
- **Core Idea**: Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks.

**Method Card (方法卡)**
- **Core Idea**: Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

