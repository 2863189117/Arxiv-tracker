# arXiv 检索结果 / Results

## 1. CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in   City-scale Scenarios
- Authors：Jialei Xu, Zizhuang Wei, Weikang You, Linyun Li, Weijian Sun
- First：2025-08-13T03:55:56+00:00；Latest：2025-08-13T03:55:56+00:00
- Abs：http://arxiv.org/abs/2508.09470v1
- PDF：http://arxiv.org/pdf/2508.09470v1
### [中文]
**中文翻译**
- 标题：CitySeg：城市场景下的三维开放词汇语义分割基础模型
- 摘要：城市级点云语义分割是无人机感知系统的关键技术，通过不依赖视觉信息实现对三维点的分类，达成全面三维理解。然而现有模型常受限于三维数据规模有限及数据集间的领域差异，导致泛化能力下降。为此，我们提出CitySeg——一个融合文本模态实现开放词汇分割与零样本推理的城市级点云语义分割基础模型。具体通过定制数据预处理规则解决多领域数据分布不均问题，并提出局部-全局交叉注意力网络增强无人机场景下的点网络感知能力。针对跨数据集语义标签差异，采用分层分类策略：依据数据标注规则建立分层图整合标签，并通过图编码器建模类别间层次关系。此外提出两阶段训练策略并采用铰链损失增强子类特征可分性。实验表明CitySeg在九个封闭集基准测试中达到最先进性能，显著优于现有方法，并首次实现不依赖视觉信息的城市级点云零样本泛化。

> **TL;DR**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achieve comprehensive 3D understanding.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achieve comprehensive 3D understanding.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achieve comprehensive 3D understanding.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achieve comprehensive 3D understanding.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 2. ReferSplat: Referring Segmentation in 3D Gaussian Splatting
- Authors：Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding
- Venue：ICML 2025 Oral
- Comments：ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat
- First：2025-08-11T17:59:30+00:00；Latest：2025-08-11T17:59:30+00:00
- Abs：http://arxiv.org/abs/2508.08252v1
- PDF：http://arxiv.org/pdf/2508.08252v1
- Code：https://github.com/heshuting555/ReferSplat
### [中文]
**中文翻译**
- 标题：ReferSplat：基于3D高斯泼溅的指代分割
- 摘要：本文提出指代式3D高斯泼溅分割（R3DGS）新任务，旨在通过自然语言描述（常包含空间关系或物体属性）对3D高斯场景中的目标物体进行分割。该任务要求模型识别新视角下可能被遮挡或不可见的新描述对象，对3D多模态理解提出重大挑战。发展此能力对推进具身人工智能至关重要。为支持该领域研究，我们构建了首个R3DGS数据集Ref-LERF。分析表明，3D多模态理解与空间关系建模是R3DGS的核心挑战。为此我们提出ReferSplat框架，在空间感知范式下显式建模自然语言表达与3D高斯点的关联。ReferSplat在新建的R3DGS任务和3D开放词汇分割基准上均实现最先进性能。数据集与代码详见https://github.com/heshuting555/ReferSplat。

> **TL;DR**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, Referring / Grounding, 3D Vision
- **Core Idea**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes.
- **Venue**: ICML 2025 Oral

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, Referring / Grounding, 3D Vision
- **Core Idea**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes.
- **Venue**: ICML 2025 Oral

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 3. Correspondence as Video: Test-Time Adaption on SAM2 for Reference   Segmentation in the Wild
- Authors：Haoran Wang, Zekun Li, Jian Zhang, Lei Qi, Yinghuan Shi
- First：2025-08-11T08:42:49+00:00；Latest：2025-08-11T08:42:49+00:00
- Abs：http://arxiv.org/abs/2508.07759v1
- PDF：http://arxiv.org/pdf/2508.07759v1
### [中文]
**中文翻译**
- 标题：作为视频的对应性：基于SAM2的测试时自适应在野外参考分割中的应用
- 摘要：像Segment Anything Model (SAM)这样的大型视觉模型在应用于野外下游任务时表现出显著局限性。因此，参考分割——利用参考图像及其对应掩码向模型传授新知识——成为适应视觉模型的一个有前景的新方向。然而，现有参考分割方法主要依赖元学习，仍需大量元训练过程并带来巨大的数据和计算成本。本研究提出一种创新方法，将参考-目标图像对之间的内在对应性表示为伪视频。这一视角使得具备交互式视频对象分割(iVOS)能力的最新版SAM（即SAM2）能够以轻量级方式适应下游任务。我们将该方法称为CAV-SAM（对应性作为视频的SAM）。CAV-SAM包含两个核心模块：基于扩散的语义转换(DBST)模块采用扩散模型构建语义转换序列，而测试时几何对齐(TTGA)模块通过测试时微调对齐该序列中的几何变化。我们在广泛使用的数据集上评估CAV-SAM，其分割性能较SOTA方法提升超过5%。具体实现详见补充材料。

> **TL;DR**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.

**Method Card (方法卡)**
- **Task / Problem**: Segmentation
- **Core Idea**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.

**Method Card (方法卡)**
- **Task / Problem**: Segmentation
- **Core Idea**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 4. Learning 3D Texture-Aware Representations for Parsing Diverse Human   Clothing and Body Parts
- Authors：Kiran Chhatre, Christopher Peters, Srikrishna Karanam
- Comments：16 pages, 11 figures
- First：2025-08-08T05:36:20+00:00；Latest：2025-08-08T05:36:20+00:00
- Abs：http://arxiv.org/abs/2508.06032v1
- PDF：http://arxiv.org/pdf/2508.06032v1
### [中文]
**中文翻译**
- 标题：学习用于解析多样化人体服装与身体部位的3D纹理感知表示
- 摘要：现有人体解析方法常采用固定掩码类别和宽泛标签，难以区分细粒度服装类型。近期开放词汇分割方法利用预训练文生图扩散模型特征实现强零样本迁移，但通常将整个人体归为单一类别，无法区分多样化服装或细节身体部位。为此，我们提出Spectrum——一个统一网络，实现部件级像素解析（身体部位与服装）和实例级分组。虽然基于扩散的开放词汇模型跨任务泛化能力强，但其内部表示未针对细节人体解析专门优化。我们发现，与具有宽泛表示的扩散模型不同，图像驱动的3D纹理生成器能保持与输入图像的忠实对应，从而为解析多样化服装和身体部位提供更强表示。Spectrum创新性地重构了图像到纹理扩散模型（通过对文生图模型进行3D人体纹理图微调获得），以提升与身体部位和服装的对齐能力。从输入图像中，我们通过该扩散模型提取人体部件内部特征，并通过提示引导 grounding 生成符合语义且对齐多样化服装类别的掩码。训练完成后，Spectrum可为场景中任意数量人体生成每个可见身体部位和服装类别的语义分割图，忽略独立衣物或无关物体。我们进行了广泛跨数据集实验——分别评估身体部位、服装部件、未见服装类别和全身掩码——结果表明Spectrum在基于提示的分割中持续优于基线方法。

> **TL;DR**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, Grounding, 3D Vision
- **Core Idea**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, Grounding, 3D Vision
- **Core Idea**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 5. What Holds Back Open-Vocabulary Segmentation?
- Authors：Josip Šarić, Ivan Martinović, Matej Kristan, Siniša Šegvić
- Venue：ICCV
- Comments：Accepted for publication at ICCV 25 Workshop: What is Next in
  Multimodal Foundation Models?
- First：2025-08-06T08:46:47+00:00；Latest：2025-08-06T08:46:47+00:00
- Abs：http://arxiv.org/abs/2508.04211v1
- PDF：http://arxiv.org/pdf/2508.04211v1
### [中文]
**中文翻译**
- 标题：开放词汇分割面临哪些瓶颈？
- 摘要：标准分割设置无法产生能识别训练分类体系外概念的模型。开放词汇方法承诺通过对数十亿图像-标题对进行语言-图像预训练来弥合这一差距。然而，我们发现由于存在多个导致性能近两年停滞不前的瓶颈，这一承诺未能实现。本文提出新颖的预言组件，通过利用真实标注信息来识别并解耦这些瓶颈。验证实验提供了重要的实证发现，深入揭示了开放词汇模型的失败原因，并为未来研究指明了突破方向。

> **TL;DR**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, Vision-Language
- **Core Idea**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.
- **Venue**: ICCV

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, Vision-Language
- **Core Idea**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.
- **Venue**: ICCV

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 6. Multimodal Referring Segmentation: A Survey
- Authors：Henghui Ding, Song Tang, Shuting He, Chang Liu, Zuxuan Wu, Yu-Gang Jiang
- Comments：Project Page:
  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation
- First：2025-08-01T02:14:00+00:00；Latest：2025-08-05T11:42:44+00:00
- Abs：http://arxiv.org/abs/2508.00265v2
- PDF：http://arxiv.org/pdf/2508.00265v2
- Code：https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation
### [中文]
**中文翻译**
- 标题：多模态指代分割技术综述
- 摘要：多模态指代分割旨在基于文本或音频形式的指代表达，对图像、视频和3D场景等视觉场景中的目标对象进行分割。该任务在需要根据用户指令实现精确对象感知的实际应用中具有关键作用。过去十年间，在卷积神经网络、Transformer架构及大语言模型发展的推动下，该领域在多模态社区获得广泛关注，显著提升了多模态感知能力。本文系统综述了多模态指代分割技术：首先介绍领域背景，包括问题定义与常用数据集；继而总结指代分割的统一元架构，并分别回顾图像、视频和3D场景三大视觉场景中的代表性方法；进一步探讨应对现实复杂性的广义指代表达（GREx）方法及相关任务与实际应用；同时提供标准基准测试的全面性能对比。相关研究持续追踪于：https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation

> **TL;DR**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.

**Method Card (方法卡)**
- **Task / Problem**: Segmentation, Referring / Grounding, 3D Vision, Vision-Language
- **Core Idea**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.

**Method Card (方法卡)**
- **Task / Problem**: Segmentation, Referring / Grounding, 3D Vision, Vision-Language
- **Core Idea**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 7. Enhancing and Accelerating Brain MRI through Deep Learning   Reconstruction Using Prior Subject-Specific Imaging
- Authors：Amirmohammad Shamaei, Alexander Stebner, Salome, Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza
- First：2025-07-28T21:39:36+00:00；Latest：2025-07-28T21:39:36+00:00
- Abs：http://arxiv.org/abs/2507.21349v1
- PDF：http://arxiv.org/pdf/2507.21349v1
- Code：https://github.com/amirshamaei/longitudinal-mri-deep-recon
### [中文]
**中文翻译**
- 标题：利用先验特定对象成像通过深度学习增强和加速脑部MRI重建
- 摘要：磁共振成像（MRI）是一种关键的医学成像技术，但长采集时间仍是重大挑战，导致成本增加和患者舒适度降低。近期研究表明，采用包含先验特定对象MRI扫描信息的深度学习模型可提升当前扫描的重建质量。整合此类先验信息需将既往扫描与当前图像重建进行配准，该过程耗时较长。我们提出了一种新型深度学习MRI重建框架，包含初始重建网络、深度配准模型和基于Transformer的增强网络。我们在包含18名受试者2,808幅T1加权MRI图像的纵向数据集上，以四种加速因子（R5、R10、R15、R20）验证了该方法。定量指标证实本方法优于现有方法（p < 0.05，Wilcoxon符号秩检验）。此外，我们分析了该MRI重建方法对脑部分割下游任务的影响，发现其提升了准确性并与参考分割达成更优的体积一致性。相较于传统配准算法，本方法还显著缩短了总重建时间，更适用于实时临床应用。相关代码已公开于：https://github.com/amirshamaei/longitudinal-mri-deep-recon。

> **TL;DR**: Magnetic resonance imaging (MRI) is a crucial medical imaging modality.

**Method Card (方法卡)**
- **Task / Problem**: Segmentation
- **Core Idea**: Magnetic resonance imaging (MRI) is a crucial medical imaging modality.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Magnetic resonance imaging (MRI) is a crucial medical imaging modality.

**Method Card (方法卡)**
- **Task / Problem**: Segmentation
- **Core Idea**: Magnetic resonance imaging (MRI) is a crucial medical imaging modality.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 8. Taking Language Embedded 3D Gaussian Splatting into the Wild
- Authors：Yuze Wang, Yue Qi
- Comments：Visit our project page at
  https://yuzewang1998.github.io/takinglangsplatw/
- First：2025-07-26T07:00:32+00:00；Latest：2025-08-05T01:40:57+00:00
- Abs：http://arxiv.org/abs/2507.19830v2
- PDF：http://arxiv.org/pdf/2507.19830v2
- Project：https://yuzewang1998.github.io/takinglangsplatw/
### [中文]
**中文翻译**
- 标题：将语言嵌入的3D高斯溅射技术引入野外场景
- 摘要：近年来利用大规模互联网照片集进行三维重建的进展，实现了对全球地标和历史遗址的沉浸式虚拟探索。然而，对于建筑风格与结构知识的沉浸式理解却鲜有关注，目前仍主要局限于浏览静态图文对。为此，我们能否从野外三维重建技术中汲取灵感，利用无约束照片集创建理解建筑构件三维结构的沉浸式方法？本文扩展了语言嵌入的3D高斯溅射技术（3DGS），提出了一种基于无约束照片集的开放词汇场景理解新框架。具体而言，我们首先通过重建辐射场从与无约束图像相同视角渲染多外观图像，继而提取多外观CLIP特征及两种语言特征不确定性图谱——瞬态不确定性和外观不确定性（源自多外观特征）以指导后续优化过程。接着提出瞬态不确定性感知自编码器、多外观语言场3DGS表示及后集成策略，有效压缩、学习并融合多外观语言特征。最后，为量化评估方法，我们引入PT-OVS基准数据集，用于评估无约束照片集上的开放词汇分割性能。实验结果表明，本方法优于现有技术，可实现精确的开放词汇分割，并支持开放词汇查询的交互式漫游、建筑风格模式识别及三维场景编辑等应用。

> **TL;DR**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 9. DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary   queries in NeRF
- Authors：Doriand Petit, Steve Bourgeois, Vincent Gay-Bellile, Florian Chabot, Loïc Barthe
- Venue：ICCV
- Comments：Published at ICCV'25
- First：2025-07-19T12:46:20+00:00；Latest：2025-07-19T12:46:20+00:00
- Abs：http://arxiv.org/abs/2507.14596v1
- PDF：http://arxiv.org/pdf/2507.14596v1
### [中文]
**中文翻译**
- 标题：DiSCO-3D：基于神经辐射场的开放词汇查询中子概念的发现与分割
- 摘要：三维语义分割为机器人技术和自主系统等应用提供高层级场景理解。传统方法仅适配特定任务目标（开放词汇分割）或场景内容（无监督语义分割）。我们提出DiSCO-3D，首个解决三维开放词汇子概念发现这一更广泛问题的方法，旨在提供同时适配场景和用户查询的三维语义分割。基于神经场表示，我们将无监督分割与弱开放词汇指导相结合。评估表明，DiSCO-3D在开放词汇子概念发现中实现有效性能，并在开放词汇与无监督分割的边缘案例中展现最先进成果。

> **TL;DR**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.
- **Venue**: ICCV

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.
- **Venue**: ICCV

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？


## 10. LOSC: LiDAR Open-voc Segmentation Consolidator
- Authors：Nermin Samet, Gilles Puy, Renaud Marlet
- First：2025-07-10T10:10:13+00:00；Latest：2025-07-10T10:10:13+00:00
- Abs：http://arxiv.org/abs/2507.07605v1
- PDF：http://arxiv.org/pdf/2507.07605v1
### [中文]
**中文翻译**
- 标题：LOSC：激光雷达开放词汇分割整合器
- 摘要：本研究探索基于图像的视觉语言模型（VLM）在驾驶场景中实现激光雷达扫描的开放词汇分割。传统方法可将图像语义反投影至3D点云，但产生的点标签存在噪声且稀疏。我们通过整合这些标签，实现时空一致性并对抗图像级增强干扰。基于优化后的标签训练3D网络，该名为LOSC的简易方法在nuScenes和SemanticKITTI数据集上以显著优势超越了零样本开放词汇语义与全景分割的当前最优水平。

> **TL;DR**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision, Vision-Language
- **Core Idea**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.
- **Data / Benchmarks**: KITTI, nuScenes

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

### [English]
> **TL;DR**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.

**Method Card (方法卡)**
- **Task / Problem**: Open-Vocabulary, Segmentation, 3D Vision, Vision-Language
- **Core Idea**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.
- **Data / Benchmarks**: KITTI, nuScenes

**Discussion (讨论问题)**
1. 相比强基线，优势是否稳定显著？
2. 代价/延迟与内存开销如何，复现细节是否充分？
3. 失败模式与局限？可能改进方向？
4. 数据与指标是否充分支撑结论，是否存在偏置/重叠？
5. 是否可迁移到真实应用或边缘设备？

