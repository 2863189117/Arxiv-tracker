# arXiv 检索结果 / Results

## 1. CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in   City-scale Scenarios
- Authors：Jialei Xu, Zizhuang Wei, Weikang You, Linyun Li, Weijian Sun
- First：2025-08-13T03:55:56+00:00；Latest：2025-08-13T03:55:56+00:00
- Abs：http://arxiv.org/abs/2508.09470v1
- PDF：http://arxiv.org/pdf/2508.09470v1
### [中文]
**中文翻译**
- 标题：CitySeg：城市场景下的三维开放词汇语义分割基础模型
- 摘要：城市级点云语义分割是无人机感知系统的关键技术，通过不依赖视觉信息实现对三维点的分类，达成全面三维理解。然而现有模型常受限于三维数据规模有限及数据集间的领域差异，导致泛化能力下降。为此我们提出CitySeg——一个融合文本模态实现开放词汇分割与零样本推理的城市级点云语义分割基础模型。具体通过定制数据预处理规则解决多领域数据分布不均问题，并提出局部-全局交叉注意力网络增强无人机场景下的点网络感知能力。针对跨数据集语义标签差异，采用分层分类策略：依据数据标注规则建立分层图整合标签，并通过图编码器建模类别间层次关系。此外提出两阶段训练策略并采用铰链损失增强子类特征可分性。实验表明CitySeg在九个封闭集基准上达到最先进性能，显著优于现有方法，并首次实现不依赖视觉信息的城市级点云零样本泛化。

> **TL;DR**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achiev…

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achieve comprehensive 3D understanding.
- **Links**: PDF: http://arxiv.org/pdf/2508.09470v1 | Abs: http://arxiv.org/abs/2508.09470v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achiev…

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Semantic segmentation of city-scale point clouds is a critical technology for Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification of 3D points without relying on any visual information to achieve comprehensive 3D understanding.
- **Links**: PDF: http://arxiv.org/pdf/2508.09470v1 | Abs: http://arxiv.org/abs/2508.09470v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 2. ReferSplat: Referring Segmentation in 3D Gaussian Splatting
- Authors：Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding
- Venue：ICML 2025 Oral
- Comments：ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat
- First：2025-08-11T17:59:30+00:00；Latest：2025-08-11T17:59:30+00:00
- Abs：http://arxiv.org/abs/2508.08252v1
- PDF：http://arxiv.org/pdf/2508.08252v1
- Code：https://github.com/heshuting555/ReferSplat
### [中文]
**中文翻译**
- 标题：ReferSplat：基于3D高斯泼溅的指代分割
- 摘要：我们提出了指代式3D高斯泼溅分割（R3DGS）这一新任务，旨在通过自然语言描述（常包含空间关系或物体属性）对3D高斯场景中的目标物体进行分割。该任务要求模型识别在新视角下可能被遮挡或不可见的新描述物体，这对3D多模态理解提出了重大挑战。发展此能力对推进具身AI至关重要。为支持该领域研究，我们构建了首个R3DGS数据集Ref-LERF。分析表明，3D多模态理解与空间关系建模是R3DGS的核心挑战。为此，我们提出ReferSplat框架，在空间感知范式下显式建模自然语言表达与3D高斯点的关联。ReferSplat在新建的R3DGS任务和3D开放词汇分割基准上均实现了最先进性能。数据集与代码详见https://github.com/heshuting555/ReferSplat。

> **TL;DR**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships o…

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, Referring / Grounding, 3D Vision
- **Core Idea**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes.
- **Venue**: ICML 2025 Oral
- **Links**: PDF: http://arxiv.org/pdf/2508.08252v1 | Abs: http://arxiv.org/abs/2508.08252v1 | Code: https://github.com/heshuting555/ReferSplat

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships o…

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, Referring / Grounding, 3D Vision
- **Core Idea**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task that aims to segment target objects in a 3D Gaussian scene based on natural language descriptions, which often contain spatial relationships or object attributes.
- **Venue**: ICML 2025 Oral
- **Links**: PDF: http://arxiv.org/pdf/2508.08252v1 | Abs: http://arxiv.org/abs/2508.08252v1 | Code: https://github.com/heshuting555/ReferSplat

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 3. Correspondence as Video: Test-Time Adaption on SAM2 for Reference   Segmentation in the Wild
- Authors：Haoran Wang, Zekun Li, Jian Zhang, Lei Qi, Yinghuan Shi
- First：2025-08-11T08:42:49+00:00；Latest：2025-08-11T08:42:49+00:00
- Abs：http://arxiv.org/abs/2508.07759v1
- PDF：http://arxiv.org/pdf/2508.07759v1
### [中文]
**中文翻译**
- 标题：作为视频的对应关系：SAM2在野外参考分割中的测试时适应
- 摘要：像Segment Anything Model (SAM)这样的大型视觉模型在应用于野外下游任务时表现出显著局限性。因此，参考分割——利用参考图像及其对应掩码向模型传授新知识——成为适应视觉模型的一个有前景的新方向。然而，现有参考分割方法主要依赖元学习，仍需要大量元训练过程并带来巨大的数据和计算成本。本研究提出一种创新方法，将参考-目标图像对之间的内在对应关系表示为伪视频。这一视角使得具备交互式视频对象分割(iVOS)能力的最新版SAM（即SAM2）能够以轻量级方式适应下游任务。我们将该方法称为SAM的对应关系视频化(CAV-SAM)。CAV-SAM包含两个核心模块：基于扩散的语义转换(DBST)模块采用扩散模型构建语义转换序列，而测试时几何对齐(TTGA)模块通过测试时微调对齐该序列中的几何变化。我们在广泛使用的数据集上评估CAV-SAM，其分割性能较SOTA方法提升超过5%。实现细节详见补充材料。

> **TL;DR**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.

**Method Card**
- **Task/Problem**: Segmentation
- **Core Idea**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.
- **Links**: PDF: http://arxiv.org/pdf/2508.07759v1 | Abs: http://arxiv.org/abs/2508.07759v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.

**Method Card**
- **Task/Problem**: Segmentation
- **Core Idea**: Large vision models like the Segment Anything Model (SAM) exhibit significant limitations when applied to downstream tasks in the wild.
- **Links**: PDF: http://arxiv.org/pdf/2508.07759v1 | Abs: http://arxiv.org/abs/2508.07759v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 4. Learning 3D Texture-Aware Representations for Parsing Diverse Human   Clothing and Body Parts
- Authors：Kiran Chhatre, Christopher Peters, Srikrishna Karanam
- Comments：16 pages, 11 figures
- First：2025-08-08T05:36:20+00:00；Latest：2025-08-08T05:36:20+00:00
- Abs：http://arxiv.org/abs/2508.06032v1
- PDF：http://arxiv.org/pdf/2508.06032v1
### [中文]
**中文翻译**
- 标题：学习用于解析多样化人体服装与身体部位的3D纹理感知表示
- 摘要：现有人体解析方法常采用固定掩码类别和宽泛标签，难以区分细粒度服装类型。近期开放词汇分割方法利用预训练文生图扩散模型特征实现强零样本迁移，但通常将整个人体归为单一类别，无法区分多样化服装或细节身体部位。为此，我们提出Spectrum框架——一个统一网络，实现部件级像素解析（身体部位与服装）和实例级分组。虽然基于扩散的开放词汇模型跨任务泛化能力强，但其内部表示未针对细节人体解析专门优化。我们发现，与具有宽泛表示的扩散模型不同，图像驱动的3D纹理生成器能保持与输入图像的忠实对应，从而为解析多样化服装和身体部位提供更强表征。Spectrum创新性地重构了图像到纹理扩散模型（通过对文生图模型进行3D人体纹理图微调获得），以提升与身体部位和服装的对齐能力。通过该模型从输入图像提取人体部件内部特征，并基于提示引导生成符合多样服装类别的语义有效掩码。训练完成后，Spectrum可为场景中任意数量人体生成每个可见身体部位和服装类别的语义分割图，忽略独立衣物或无关物体。通过跨数据集实验（分别评估身体部位、服装部件、未见服装类别和全身掩码），我们证明Spectrum在基于提示的分割中持续优于基线方法。

> **TL;DR**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, Grounding, 3D Vision
- **Core Idea**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.
- **Links**: PDF: http://arxiv.org/pdf/2508.06032v1 | Abs: http://arxiv.org/abs/2508.06032v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, Grounding, 3D Vision
- **Core Idea**: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types.
- **Links**: PDF: http://arxiv.org/pdf/2508.06032v1 | Abs: http://arxiv.org/abs/2508.06032v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 5. What Holds Back Open-Vocabulary Segmentation?
- Authors：Josip Šarić, Ivan Martinović, Matej Kristan, Siniša Šegvić
- Venue：ICCV
- Comments：Accepted for publication at ICCV 25 Workshop: What is Next in
  Multimodal Foundation Models?
- First：2025-08-06T08:46:47+00:00；Latest：2025-08-06T08:46:47+00:00
- Abs：http://arxiv.org/abs/2508.04211v1
- PDF：http://arxiv.org/pdf/2508.04211v1
### [中文]
**中文翻译**
- 标题：开放词汇分割的瓶颈何在？
- 摘要：标准分割设置无法产生能识别训练分类外概念的模型。开放词汇方法承诺通过数十亿图像-标题对的语言-图像预训练来弥合这一差距。然而，我们发现由于多个瓶颈导致性能近两年停滞不前，这一承诺未能实现。本文提出新颖的预言组件，利用真实标注信息识别并解耦这些瓶颈。验证实验提供了重要实证发现，深入揭示了开放词汇模型的失败原因，并为未来研究指明了突破方向。

> **TL;DR**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, Vision-Language
- **Core Idea**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.
- **Venue**: ICCV
- **Links**: PDF: http://arxiv.org/pdf/2508.04211v1 | Abs: http://arxiv.org/abs/2508.04211v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, Vision-Language
- **Core Idea**: Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy.
- **Venue**: ICCV
- **Links**: PDF: http://arxiv.org/pdf/2508.04211v1 | Abs: http://arxiv.org/abs/2508.04211v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 6. Multimodal Referring Segmentation: A Survey
- Authors：Henghui Ding, Song Tang, Shuting He, Chang Liu, Zuxuan Wu, Yu-Gang Jiang
- Comments：Project Page:
  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation
- First：2025-08-01T02:14:00+00:00；Latest：2025-08-05T11:42:44+00:00
- Abs：http://arxiv.org/abs/2508.00265v2
- PDF：http://arxiv.org/pdf/2508.00265v2
- Code：https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation
### [中文]
**中文翻译**
- 标题：多模态指代分割：综述
- 摘要：多模态指代分割旨在基于文本或音频形式的指代表达，对图像、视频和3D场景等视觉场景中的目标对象进行分割。该任务在需要根据用户指令实现精确对象感知的实际应用中起着关键作用。过去十年间，在卷积神经网络、Transformer架构和大语言模型发展的推动下，该领域在多模态社区获得广泛关注，这些技术显著提升了多模态感知能力。本文全面综述了多模态指代分割领域：首先介绍该领域的背景知识，包括问题定义和常用数据集；随后总结指代分割的统一元架构，并回顾图像、视频和3D场景三大视觉场景中的代表性方法；进一步讨论应对现实世界复杂性的广义指代表达（GREx）方法，以及相关任务与实际应用；同时提供标准基准测试的广泛性能比较。相关研究持续追踪于https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation。

> **TL;DR**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.

**Method Card**
- **Task/Problem**: Segmentation, Referring / Grounding, 3D Vision, Vision-Language
- **Core Idea**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.
- **Links**: PDF: http://arxiv.org/pdf/2508.00265v2 | Abs: http://arxiv.org/abs/2508.00265v2 | Code: https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.

**Method Card**
- **Task/Problem**: Segmentation, Referring / Grounding, 3D Vision, Vision-Language
- **Core Idea**: Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format.
- **Links**: PDF: http://arxiv.org/pdf/2508.00265v2 | Abs: http://arxiv.org/abs/2508.00265v2 | Code: https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 7. Enhancing and Accelerating Brain MRI through Deep Learning   Reconstruction Using Prior Subject-Specific Imaging
- Authors：Amirmohammad Shamaei, Alexander Stebner, Salome, Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza
- First：2025-07-28T21:39:36+00:00；Latest：2025-07-28T21:39:36+00:00
- Abs：http://arxiv.org/abs/2507.21349v1
- PDF：http://arxiv.org/pdf/2507.21349v1
- Code：https://github.com/amirshamaei/longitudinal-mri-deep-recon
### [中文]
**中文翻译**
- 标题：利用先验特定对象成像通过深度学习重建增强和加速脑部MRI
- 摘要：磁共振成像（MRI）是一种关键的医学成像技术，但长采集时间仍是重大挑战，导致成本增加和患者舒适度降低。近期研究表明，采用包含先验特定对象MRI扫描信息的深度学习模型可提升当前扫描的重建质量。整合此类先验信息需将既往扫描与当前图像重建配准，这一过程可能耗时。我们提出了一种新型深度学习MRI重建框架，包含初始重建网络、深度配准模型和基于Transformer的增强网络。我们在包含18名受试者2,808幅T1加权MRI图像的纵向数据集上，以四种加速因子（R5、R10、R15、R20）验证了该方法。定量指标证实本方法优于现有方法（p < 0.05，Wilcoxon符号秩检验）。此外，我们分析了该MRI重建方法对脑部分割下游任务的影响，发现其与参考分割的准确性和体积一致性均获提升。相较于传统配准算法，本方法还实现了总重建时间的大幅缩减，更适用于实时临床应用。相关代码已公开于：https://github.com/amirshamaei/longitudinal-mri-deep-recon。

> **TL;DR**: Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans.

**Method Card**
- **Task/Problem**: Segmentation
- **Core Idea**: Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans.
- **Links**: PDF: http://arxiv.org/pdf/2507.21349v1 | Abs: http://arxiv.org/abs/2507.21349v1 | Code: https://github.com/amirshamaei/longitudinal-mri-deep-recon

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans.

**Method Card**
- **Task/Problem**: Segmentation
- **Core Idea**: Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans.
- **Links**: PDF: http://arxiv.org/pdf/2507.21349v1 | Abs: http://arxiv.org/abs/2507.21349v1 | Code: https://github.com/amirshamaei/longitudinal-mri-deep-recon

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 8. Taking Language Embedded 3D Gaussian Splatting into the Wild
- Authors：Yuze Wang, Yue Qi
- Comments：Visit our project page at
  https://yuzewang1998.github.io/takinglangsplatw/
- First：2025-07-26T07:00:32+00:00；Latest：2025-08-05T01:40:57+00:00
- Abs：http://arxiv.org/abs/2507.19830v2
- PDF：http://arxiv.org/pdf/2507.19830v2
- Project：https://yuzewang1998.github.io/takinglangsplatw/
### [中文]
**中文翻译**
- 标题：将语言嵌入的3D高斯溅射技术引入野外场景
- 摘要：近年来利用大规模互联网照片集进行三维重建的进展，实现了对全球地标和历史遗址的沉浸式虚拟探索。然而，对建筑风格和结构知识的沉浸式理解仍较少受到关注，大多局限于浏览静态图文对。为此，我们能否从野外三维重建技术中汲取灵感，利用无约束照片集创建理解建筑构件三维结构的沉浸式方法？我们扩展了语言嵌入的3D高斯溅射技术（3DGS），提出了一种基于无约束照片集的开放词汇场景理解新框架。具体而言，首先通过重建辐射场从与无约束图像相同视角渲染多幅外观图像，继而提取多外观CLIP特征及两种语言特征不确定性图谱——瞬态不确定性和外观不确定性（源自多外观特征）以指导后续优化过程。接着提出瞬态不确定性感知自编码器、多外观语言场3DGS表示及后集成策略，有效压缩、学习和融合多外观语言特征。最后引入PT-OVS基准数据集，定量评估无约束照片集上的开放词汇分割性能。实验结果表明，本方法优于现有技术，可实现精确的开放词汇分割，并支持开放词汇查询交互漫游、建筑风格模式识别及三维场景编辑等应用。

> **TL;DR**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.
- **Links**: PDF: http://arxiv.org/pdf/2507.19830v2 | Abs: http://arxiv.org/abs/2507.19830v2 | Project: https://yuzewang1998.github.io/takinglangsplatw/

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: Recent advances in leveraging large-scale Internet photo collections for 3D reconstruction have enabled immersive virtual exploration of landmarks and historic sites worldwide.
- **Links**: PDF: http://arxiv.org/pdf/2507.19830v2 | Abs: http://arxiv.org/abs/2507.19830v2 | Project: https://yuzewang1998.github.io/takinglangsplatw/

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 9. DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary   queries in NeRF
- Authors：Doriand Petit, Steve Bourgeois, Vincent Gay-Bellile, Florian Chabot, Loïc Barthe
- Venue：ICCV
- Comments：Published at ICCV'25
- First：2025-07-19T12:46:20+00:00；Latest：2025-07-19T12:46:20+00:00
- Abs：http://arxiv.org/abs/2507.14596v1
- PDF：http://arxiv.org/pdf/2507.14596v1
### [中文]
**中文翻译**
- 标题：DiSCO-3D：基于神经辐射场的开放词汇查询中子概念的发现与分割
- 摘要：三维语义分割为机器人技术和自主系统等应用提供高层场景理解。传统方法仅适用于特定任务目标（开放词汇分割）或场景内容（无监督语义分割）。我们提出DiSCO-3D，这是首个解决三维开放词汇子概念发现这一更广泛问题的方法，旨在提供同时适应场景和用户查询的三维语义分割。DiSCO-3D基于神经场表示构建，将无监督分割与弱开放词汇指导相结合。评估表明，DiSCO-3D在开放词汇子概念发现中实现有效性能，并在开放词汇和无监督分割的边缘案例中展现出最先进的结果。

> **TL;DR**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.
- **Venue**: ICCV
- **Links**: PDF: http://arxiv.org/pdf/2507.14596v1 | Abs: http://arxiv.org/abs/2507.14596v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision
- **Core Idea**: 3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}.
- **Venue**: ICCV
- **Links**: PDF: http://arxiv.org/pdf/2507.14596v1 | Abs: http://arxiv.org/abs/2507.14596v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？


## 10. LOSC: LiDAR Open-voc Segmentation Consolidator
- Authors：Nermin Samet, Gilles Puy, Renaud Marlet
- First：2025-07-10T10:10:13+00:00；Latest：2025-07-10T10:10:13+00:00
- Abs：http://arxiv.org/abs/2507.07605v1
- PDF：http://arxiv.org/pdf/2507.07605v1
### [中文]
**中文翻译**
- 标题：LOSC：激光雷达开放词汇分割整合器
- 摘要：本研究探索基于图像的视觉语言模型（VLM）在驾驶场景中实现激光雷达扫描的开放词汇分割。传统方法可将图像语义反投影至3D点云，但产生的点标签存在噪声且稀疏。我们通过整合这些标签，实现时空一致性并对抗图像级增强干扰。基于优化后的标签训练3D网络，该名为LOSC的简易方法在nuScenes和SemanticKITTI数据集上以显著优势超越了零样本开放词汇语义与全景分割的当前最优水平。

> **TL;DR**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision, Vision-Language
- **Core Idea**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.
- **Data/Benchmarks**: KITTI, nuScenes
- **Links**: PDF: http://arxiv.org/pdf/2507.07605v1 | Abs: http://arxiv.org/abs/2507.07605v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

### [English]
> **TL;DR**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.

**Method Card**
- **Task/Problem**: Open-Vocabulary, Segmentation, 3D Vision, Vision-Language
- **Core Idea**: We study the use of image-based Vision-Language Models (VLMs) for open-vocabulary segmentation of lidar scans in driving settings.
- **Data/Benchmarks**: KITTI, nuScenes
- **Links**: PDF: http://arxiv.org/pdf/2507.07605v1 | Abs: http://arxiv.org/abs/2507.07605v1

**Discussion Questions**
1. 与强基线相比，该方法优势是否稳定显著？
2. 代价/推理延迟与内存开销如何，是否有复现实验细节？
3. 失败模式与局限是什么？可能的改进方向？
4. 数据与指标是否充分支撑结论？是否存在偏置或重叠？
5. 能否迁移到真实应用或边缘设备场景？

