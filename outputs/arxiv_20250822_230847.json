[
  {
    "id": "http://arxiv.org/abs/2508.15773v1",
    "title": "Scaling Group Inference for Diverse and High-Quality Generation",
    "authors": [
      "Gaurav Parmar",
      "Or Patashnik",
      "Daniil Ostashev",
      "Kuan-Chieh Wang",
      "Kfir Aberman",
      "Srinivasa Narasimhan",
      "Jun-Yan Zhu"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2025-08-21T17:59:57+00:00",
    "updated": "2025-08-21T17:59:57+00:00",
    "comments": "Project website: https://www.cs.cmu.edu/~group-inference, GitHub:\n  https://github.com/GaParmar/group-inference",
    "journal_ref": null,
    "venue_inferred": "www",
    "summary": "Generative models typically sample outputs independently, and recent\ninference-time guidance and scaling algorithms focus on improving the quality\nof individual samples. However, in real-world applications, users are often\npresented with a set of multiple images (e.g., 4-8) for each prompt, where\nindependent sampling tends to lead to redundant results, limiting user choices\nand hindering idea exploration. In this work, we introduce a scalable group\ninference method that improves both the diversity and quality of a group of\nsamples. We formulate group inference as a quadratic integer assignment\nproblem: candidate outputs are modeled as graph nodes, and a subset is selected\nto optimize sample quality (unary term) while maximizing group diversity\n(binary term). To substantially improve runtime efficiency, we progressively\nprune the candidate set using intermediate predictions, allowing our method to\nscale up to large candidate sets. Extensive experiments show that our method\nsignificantly improves group diversity and quality compared to independent\nsampling baselines and recent inference algorithms. Our framework generalizes\nacross a wide range of tasks, including text-to-image, image-to-image, image\nprompting, and video generation, enabling generative models to treat multiple\noutputs as cohesive groups rather than independent samples.",
    "html_url": "http://arxiv.org/abs/2508.15773v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15773v1",
    "code_urls": [
      "https://github.com/GaParmar/group-inference"
    ],
    "project_urls": [],
    "other_urls": [
      "https://www.cs.cmu.edu/~group-inference"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15774v1",
    "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
    "authors": [
      "Haonan Qiu",
      "Ning Yu",
      "Ziqi Huang",
      "Paul Debevec",
      "Ziwei Liu"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:59:57+00:00",
    "updated": "2025-08-21T17:59:57+00:00",
    "comments": "CineScale is an extended work of FreeScale (ICCV 2025). Project Page:\n  https://eyeline-labs.github.io/CineScale/, Code Repo:\n  https://github.com/Eyeline-Labs/CineScale",
    "journal_ref": null,
    "venue_inferred": "ICCV 2025",
    "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
    "html_url": "http://arxiv.org/abs/2508.15774v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15774v1",
    "code_urls": [
      "https://github.com/Eyeline-Labs/CineScale"
    ],
    "project_urls": [
      "https://eyeline-labs.github.io/CineScale/"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15772v1",
    "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
    "authors": [
      "Qingyang Mao",
      "Qi Cai",
      "Yehao Li",
      "Yingwei Pan",
      "Mingyue Cheng",
      "Ting Yao",
      "Qi Liu",
      "Tao Mei"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2025-08-21T17:59:32+00:00",
    "updated": "2025-08-21T17:59:32+00:00",
    "comments": "Source codes and models are available at\n  https://github.com/HiDream-ai/VAREdit",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Recent advances in diffusion models have brought remarkable visual fidelity\nto instruction-guided image editing. However, their global denoising process\ninherently entangles the edited region with the entire image context, leading\nto unintended spurious modifications and compromised adherence to editing\ninstructions. In contrast, autoregressive models offer a distinct paradigm by\nformulating image synthesis as a sequential process over discrete visual\ntokens. Their causal and compositional mechanism naturally circumvents the\nadherence challenges of diffusion-based methods. In this paper, we present\nVAREdit, a visual autoregressive (VAR) framework that reframes image editing as\na next-scale prediction problem. Conditioned on source image features and text\ninstructions, VAREdit generates multi-scale target features to achieve precise\nedits. A core challenge in this paradigm is how to effectively condition the\nsource image tokens. We observe that finest-scale source features cannot\neffectively guide the prediction of coarser target features. To bridge this\ngap, we introduce a Scale-Aligned Reference (SAR) module, which injects\nscale-matched conditioning information into the first self-attention layer.\nVAREdit demonstrates significant advancements in both editing adherence and\nefficiency. On standard benchmarks, it outperforms leading diffusion-based\nmethods by 30\\%+ higher GPT-Balance score. Moreover, it completes a\n$512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the\nsimilarly sized UltraEdit. The models are available at\nhttps://github.com/HiDream-ai/VAREdit.",
    "html_url": "http://arxiv.org/abs/2508.15772v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15772v1",
    "code_urls": [
      "https://github.com/HiDream-ai/VAREdit"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15769v1",
    "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
    "authors": [
      "Yanxu Meng",
      "Haoning Wu",
      "Ya Zhang",
      "Weidi Xie"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-08-21T17:59:16+00:00",
    "updated": "2025-08-21T17:59:16+00:00",
    "comments": "Technical Report; Project Page: https://mengmouxu.github.io/SceneGen",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
    "html_url": "http://arxiv.org/abs/2508.15769v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15769v1",
    "code_urls": [],
    "project_urls": [
      "https://mengmouxu.github.io/SceneGen"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15767v1",
    "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive   Parametric Human Modeling",
    "authors": [
      "Jinhyung Park",
      "Javier Romero",
      "Shunsuke Saito",
      "Fabian Prada",
      "Takaaki Shiratori",
      "Yichen Xu",
      "Federica Bogo",
      "Shoou-I Yu",
      "Kris Kitani",
      "Rawal Khirodkar"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:58:56+00:00",
    "updated": "2025-08-21T17:58:56+00:00",
    "comments": "ICCV 2025; Website: https://jindapark.github.io/projects/atlas/",
    "journal_ref": null,
    "venue_inferred": "ICCV 2025",
    "summary": "Parametric body models offer expressive 3D representation of humans across a\nwide range of poses, shapes, and facial expressions, typically derived by\nlearning a basis over registered 3D meshes. However, existing human mesh\nmodeling approaches struggle to capture detailed variations across diverse body\nposes and shapes, largely due to limited training data diversity and\nrestrictive modeling assumptions. Moreover, the common paradigm first optimizes\nthe external body surface using a linear basis, then regresses internal\nskeletal joints from surface vertices. This approach introduces problematic\ndependencies between internal skeleton and outer soft tissue, limiting direct\ncontrol over body height and bone lengths. To address these issues, we present\nATLAS, a high-fidelity body model learned from 600k high-resolution scans\ncaptured using 240 synchronized cameras. Unlike previous methods, we explicitly\ndecouple the shape and skeleton bases by grounding our mesh representation in\nthe human skeleton. This decoupling enables enhanced shape expressivity,\nfine-grained customization of body attributes, and keypoint fitting independent\nof external soft-tissue characteristics. ATLAS outperforms existing methods by\nfitting unseen subjects in diverse poses more accurately, and quantitative\nevaluations show that our non-linear pose correctives more effectively capture\ncomplex poses compared to linear models.",
    "html_url": "http://arxiv.org/abs/2508.15767v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15767v1",
    "code_urls": [],
    "project_urls": [
      "https://jindapark.github.io/projects/atlas/"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15766v1",
    "title": "Discovering Hidden Algebraic Structures via Transformers with Rank-Aware   Beam GRPO",
    "authors": [
      "Jaeha Lee",
      "Gio Huh",
      "Ning Su",
      "Tony Yue YU"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-21T17:58:50+00:00",
    "updated": "2025-08-21T17:58:50+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Recent efforts have extended the capabilities of transformers in logical\nreasoning and symbolic computations. In this work, we investigate their\ncapacity for non-linear latent pattern discovery in the context of functional\ndecomposition, focusing on the challenging algebraic task of multivariate\npolynomial decomposition. This problem, with widespread applications in science\nand engineering, is proved to be NP-hard, and demands both precision and\ninsight. Our contributions are threefold: First, we develop a synthetic data\ngeneration pipeline providing fine-grained control over problem complexity.\nSecond, we train transformer models via supervised learning and evaluate them\nacross four key dimensions involving scaling behavior and generalizability.\nThird, we propose Beam Grouped Relative Policy Optimization (BGRPO), a\nrank-aware reinforcement learning method suitable for hard algebraic problems.\nFinetuning with BGRPO improves accuracy while reducing beam width by up to\nhalf, resulting in approximately 75% lower inference compute. Additionally, our\nmodel demonstrates competitive performance in polynomial simplification,\noutperforming Mathematica in various cases.",
    "html_url": "http://arxiv.org/abs/2508.15766v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15766v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15764v1",
    "title": "Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space",
    "authors": [
      "Kiarash Kazari",
      "Ezzeldin Shereen",
      "György Dán"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-08-21T17:58:36+00:00",
    "updated": "2025-08-21T17:58:36+00:00",
    "comments": "Accepted for publication at ECAI 2025",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We address the problem of detecting adversarial attacks against cooperative\nmulti-agent reinforcement learning with continuous action space. We propose a\ndecentralized detector that relies solely on the local observations of the\nagents and makes use of a statistical characterization of the normal behavior\nof observable agents. The proposed detector utilizes deep neural networks to\napproximate the normal behavior of agents as parametric multivariate Gaussian\ndistributions. Based on the predicted density functions, we define a normality\nscore and provide a characterization of its mean and variance. This\ncharacterization allows us to employ a two-sided CUSUM procedure for detecting\ndeviations of the normality score from its mean, serving as a detector of\nanomalous behavior in real-time. We evaluate our scheme on various multi-agent\nPettingZoo benchmarks against different state-of-the-art attack methods, and\nour results demonstrate the effectiveness of our method in detecting impactful\nadversarial attacks. Particularly, it outperforms the discrete counterpart by\nachieving AUC-ROC scores of over 0.95 against the most impactful attacks in all\nevaluated environments.",
    "html_url": "http://arxiv.org/abs/2508.15764v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15764v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15763v1",
    "title": "Intern-S1: A Scientific Multimodal Foundation Model",
    "authors": [
      "Lei Bai",
      "Zhongrui Cai",
      "Maosong Cao",
      "Weihan Cao",
      "Chiyu Chen",
      "Haojiong Chen",
      "Kai Chen",
      "Pengcheng Chen",
      "Ying Chen",
      "Yongkang Chen",
      "Yu Cheng",
      "Yu Cheng",
      "Pei Chu",
      "Tao Chu",
      "Erfei Cui",
      "Ganqu Cui",
      "Long Cui",
      "Ziyun Cui",
      "Nianchen Deng",
      "Ning Ding",
      "Nanqin Dong",
      "Peijie Dong",
      "Shihan Dou",
      "Sinan Du",
      "Haodong Duan",
      "Caihua Fan",
      "Ben Gao",
      "Changjiang Gao",
      "Jianfei Gao",
      "Songyang Gao",
      "Yang Gao",
      "Zhangwei Gao",
      "Jiaye Ge",
      "Qiming Ge",
      "Lixin Gu",
      "Yuzhe Gu",
      "Aijia Guo",
      "Qipeng Guo",
      "Xu Guo",
      "Conghui He",
      "Junjun He",
      "Yili Hong",
      "Siyuan Hou",
      "Caiyu Hu",
      "Hanglei Hu",
      "Jucheng Hu",
      "Ming Hu",
      "Zhouqi Hua",
      "Haian Huang",
      "Junhao Huang",
      "Xu Huang",
      "Zixian Huang",
      "Zhe Jiang",
      "Lingkai Kong",
      "Linyang Li",
      "Peiji Li",
      "Pengze Li",
      "Shuaibin Li",
      "Tianbin Li",
      "Wei Li",
      "Yuqiang Li",
      "Dahua Lin",
      "Junyao Lin",
      "Tianyi Lin",
      "Zhishan Lin",
      "Hongwei Liu",
      "Jiangning Liu",
      "Jiyao Liu",
      "Junnan Liu",
      "Kai Liu",
      "Kaiwen Liu",
      "Kuikun Liu",
      "Shichun Liu",
      "Shudong Liu",
      "Wei Liu",
      "Xinyao Liu",
      "Yuhong Liu",
      "Zhan Liu",
      "Yinquan Lu",
      "Haijun Lv",
      "Hongxia Lv",
      "Huijie Lv",
      "Qidang Lv",
      "Ying Lv",
      "Chengqi Lyu",
      "Chenglong Ma",
      "Jianpeng Ma",
      "Ren Ma",
      "Runmin Ma",
      "Runyuan Ma",
      "Xinzhu Ma",
      "Yichuan Ma",
      "Zihan Ma",
      "Sixuan Mi",
      "Junzhi Ning",
      "Wenchang Ning",
      "Xinle Pang",
      "Jiahui Peng",
      "Runyu Peng",
      "Yu Qiao",
      "Jiantao Qiu",
      "Xiaoye Qu",
      "Yuan Qu",
      "Yuchen Ren",
      "Fukai Shang",
      "Wenqi Shao",
      "Junhao Shen",
      "Shuaike Shen",
      "Chunfeng Song",
      "Demin Song",
      "Diping Song",
      "Chenlin Su",
      "Weijie Su",
      "Weigao Sun",
      "Yu Sun",
      "Qian Tan",
      "Cheng Tang",
      "Huanze Tang",
      "Kexian Tang",
      "Shixiang Tang",
      "Jian Tong",
      "Aoran Wang",
      "Bin Wang",
      "Dong Wang",
      "Lintao Wang",
      "Rui Wang",
      "Weiyun Wang",
      "Wenhai Wang",
      "Yi Wang",
      "Ziyi Wang",
      "Ling-I Wu",
      "Wen Wu",
      "Yue Wu",
      "Zijian Wu",
      "Linchen Xiao",
      "Shuhao Xing",
      "Chao Xu",
      "Huihui Xu",
      "Jun Xu",
      "Ruiliang Xu",
      "Wanghan Xu",
      "GanLin Yang",
      "Yuming Yang",
      "Haochen Ye",
      "Jin Ye",
      "Shenglong Ye",
      "Jia Yu",
      "Jiashuo Yu",
      "Jing Yu",
      "Fei Yuan",
      "Bo Zhang",
      "Chao Zhang",
      "Chen Zhang",
      "Hongjie Zhang",
      "Jin Zhang",
      "Qiaosheng Zhang",
      "Qiuyinzhe Zhang",
      "Songyang Zhang",
      "Taolin Zhang",
      "Wenlong Zhang",
      "Wenwei Zhang",
      "Yechen Zhang",
      "Ziyang Zhang",
      "Haiteng Zhao",
      "Qian Zhao",
      "Xiangyu Zhao",
      "Xiangyu Zhao",
      "Bowen Zhou",
      "Dongzhan Zhou",
      "Peiheng Zhou",
      "Yuhao Zhou",
      "Yunhua Zhou",
      "Dongsheng Zhu",
      "Lin Zhu",
      "Yicheng Zou"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-08-21T17:58:00+00:00",
    "updated": "2025-08-21T17:58:00+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.",
    "html_url": "http://arxiv.org/abs/2508.15763v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15763v1",
    "code_urls": [
      "https://huggingface.co/internlm/Intern-S1"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15761v1",
    "title": "Waver: Wave Your Way to Lifelike Video Generation",
    "authors": [
      "Yifu Zhang",
      "Hao Yang",
      "Yuqi Zhang",
      "Yifei Hu",
      "Fengda Zhu",
      "Chuang Lin",
      "Xiaofeng Mei",
      "Yi Jiang",
      "Zehuan Yuan",
      "Bingyue Peng"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:56:10+00:00",
    "updated": "2025-08-21T17:56:10+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
    "html_url": "http://arxiv.org/abs/2508.15761v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15761v1",
    "code_urls": [
      "https://github.com/FoundationVision/Waver"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15760v1",
    "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on   Challenging Queries",
    "authors": [
      "Ming Yin",
      "Dinghan Shen",
      "Silei Xu",
      "Jianbing Han",
      "Sixun Dong",
      "Mian Zhang",
      "Yebowen Hu",
      "Shujian Liu",
      "Simin Ma",
      "Song Wang",
      "Sathish Reddy Indurthi",
      "Xun Wang",
      "Yiran Chen",
      "Kaiqiang Song"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-21T17:55:54+00:00",
    "updated": "2025-08-21T17:55:54+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.",
    "html_url": "http://arxiv.org/abs/2508.15760v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15760v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  }
]