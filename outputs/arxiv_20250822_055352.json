[
  {
    "id": "http://arxiv.org/abs/2508.09470v1",
    "title": "CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in   City-scale Scenarios",
    "authors": [
      "Jialei Xu",
      "Zizhuang Wei",
      "Weikang You",
      "Linyun Li",
      "Weijian Sun"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-13T03:55:56+00:00",
    "updated": "2025-08-13T03:55:56+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Semantic segmentation of city-scale point clouds is a critical technology for\nUnmanned Aerial Vehicle (UAV) perception systems, enabling the classification\nof 3D points without relying on any visual information to achieve comprehensive\n3D understanding. However, existing models are frequently constrained by the\nlimited scale of 3D data and the domain gap between datasets, which lead to\nreduced generalization capability. To address these challenges, we propose\nCitySeg, a foundation model for city-scale point cloud semantic segmentation\nthat incorporates text modality to achieve open vocabulary segmentation and\nzero-shot inference. Specifically, in order to mitigate the issue of\nnon-uniform data distribution across multiple domains, we customize the data\npreprocessing rules, and propose a local-global cross-attention network to\nenhance the perception capabilities of point networks in UAV scenarios. To\nresolve semantic label discrepancies across datasets, we introduce a\nhierarchical classification strategy. A hierarchical graph established\naccording to the data annotation rules consolidates the data labels, and the\ngraph encoder is used to model the hierarchical relationships between\ncategories. In addition, we propose a two-stage training strategy and employ\nhinge loss to increase the feature separability of subcategories. Experimental\nresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)\nperformance on nine closed-set benchmarks, significantly outperforming existing\napproaches. Moreover, for the first time, CitySeg enables zero-shot\ngeneralization in city-scale point cloud scenarios without relying on visual\ninformation.",
    "html_url": "http://arxiv.org/abs/2508.09470v1",
    "pdf_url": "http://arxiv.org/pdf/2508.09470v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.08252v1",
    "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
    "authors": [
      "Shuting He",
      "Guangquan Jie",
      "Changshuo Wang",
      "Yun Zhou",
      "Shuming Hu",
      "Guanbin Li",
      "Henghui Ding"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-11T17:59:30+00:00",
    "updated": "2025-08-11T17:59:30+00:00",
    "comments": "ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat",
    "journal_ref": null,
    "venue_inferred": "ICML 2025 Oral",
    "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.",
    "html_url": "http://arxiv.org/abs/2508.08252v1",
    "pdf_url": "http://arxiv.org/pdf/2508.08252v1",
    "code_urls": [
      "https://github.com/heshuting555/ReferSplat"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.07759v1",
    "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference   Segmentation in the Wild",
    "authors": [
      "Haoran Wang",
      "Zekun Li",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-11T08:42:49+00:00",
    "updated": "2025-08-11T08:42:49+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant\nlimitations when applied to downstream tasks in the wild. Consequently,\nreference segmentation, which leverages reference images and their\ncorresponding masks to impart novel knowledge to the model, emerges as a\npromising new direction for adapting vision models. However, existing reference\nsegmentation approaches predominantly rely on meta-learning, which still\nnecessitates an extensive meta-training process and brings massive data and\ncomputational cost. In this study, we propose a novel approach by representing\nthe inherent correspondence between reference-target image pairs as a pseudo\nvideo. This perspective allows the latest version of SAM, known as SAM2, which\nis equipped with interactive video object segmentation (iVOS) capabilities, to\nbe adapted to downstream tasks in a lightweight manner. We term this approach\nCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:\nthe Diffusion-Based Semantic Transition (DBST) module employs a diffusion model\nto construct a semantic transformation sequence, while the Test-Time Geometric\nAlignment (TTGA) module aligns the geometric changes within this sequence\nthrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,\nachieving segmentation performance improvements exceeding 5% over SOTA methods.\nImplementation is provided in the supplementary materials.",
    "html_url": "http://arxiv.org/abs/2508.07759v1",
    "pdf_url": "http://arxiv.org/pdf/2508.07759v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.06032v1",
    "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human   Clothing and Body Parts",
    "authors": [
      "Kiran Chhatre",
      "Christopher Peters",
      "Srikrishna Karanam"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-08T05:36:20+00:00",
    "updated": "2025-08-08T05:36:20+00:00",
    "comments": "16 pages, 11 figures",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Existing methods for human parsing into body parts and clothing often use\nfixed mask categories with broad labels that obscure fine-grained clothing\ntypes. Recent open-vocabulary segmentation approaches leverage pretrained\ntext-to-image (T2I) diffusion model features for strong zero-shot transfer, but\ntypically group entire humans into a single person category, failing to\ndistinguish diverse clothing or detailed body parts. To address this, we\npropose Spectrum, a unified network for part-level pixel parsing (body parts\nand clothing) and instance-level grouping. While diffusion-based\nopen-vocabulary models generalize well across tasks, their internal\nrepresentations are not specialized for detailed human parsing. We observe\nthat, unlike diffusion models with broad representations, image-driven 3D\ntexture generators maintain faithful correspondence to input images, enabling\nstronger representations for parsing diverse clothing and body parts. Spectrum\nintroduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --\nobtained by fine-tuning a T2I model on 3D human texture maps -- for improved\nalignment with body parts and clothing. From an input image, we extract\nhuman-part internal features via the I2Tx diffusion model and generate\nsemantically valid masks aligned to diverse clothing categories through\nprompt-guided grounding. Once trained, Spectrum produces semantic segmentation\nmaps for every visible body part and clothing category, ignoring standalone\ngarments or irrelevant objects, for any number of humans in the scene. We\nconduct extensive cross-dataset experiments -- separately assessing body parts,\nclothing parts, unseen clothing categories, and full-body masks -- and\ndemonstrate that Spectrum consistently outperforms baseline methods in\nprompt-based segmentation.",
    "html_url": "http://arxiv.org/abs/2508.06032v1",
    "pdf_url": "http://arxiv.org/pdf/2508.06032v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.04211v1",
    "title": "What Holds Back Open-Vocabulary Segmentation?",
    "authors": [
      "Josip Šarić",
      "Ivan Martinović",
      "Matej Kristan",
      "Siniša Šegvić"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-06T08:46:47+00:00",
    "updated": "2025-08-06T08:46:47+00:00",
    "comments": "Accepted for publication at ICCV 25 Workshop: What is Next in\n  Multimodal Foundation Models?",
    "journal_ref": null,
    "venue_inferred": "ICCV",
    "summary": "Standard segmentation setups are unable to deliver models that can recognize\nconcepts outside the training taxonomy. Open-vocabulary approaches promise to\nclose this gap through language-image pretraining on billions of image-caption\npairs. Unfortunately, we observe that the promise is not delivered due to\nseveral bottlenecks that have caused the performance to plateau for almost two\nyears. This paper proposes novel oracle components that identify and decouple\nthese bottlenecks by taking advantage of the groundtruth information. The\npresented validation experiments deliver important empirical findings that\nprovide a deeper insight into the failures of open-vocabulary models and\nsuggest prominent approaches to unlock the future research.",
    "html_url": "http://arxiv.org/abs/2508.04211v1",
    "pdf_url": "http://arxiv.org/pdf/2508.04211v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.00265v2",
    "title": "Multimodal Referring Segmentation: A Survey",
    "authors": [
      "Henghui Ding",
      "Song Tang",
      "Shuting He",
      "Chang Liu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-01T02:14:00+00:00",
    "updated": "2025-08-05T11:42:44+00:00",
    "comments": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
    "html_url": "http://arxiv.org/abs/2508.00265v2",
    "pdf_url": "http://arxiv.org/pdf/2508.00265v2",
    "code_urls": [
      "https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2507.21349v1",
    "title": "Enhancing and Accelerating Brain MRI through Deep Learning   Reconstruction Using Prior Subject-Specific Imaging",
    "authors": [
      "Amirmohammad Shamaei",
      "Alexander Stebner",
      "Salome",
      "Bosshart",
      "Johanna Ospel",
      "Gouri Ginde",
      "Mariana Bento",
      "Roberto Souza"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "physics.med-ph"
    ],
    "published": "2025-07-28T21:39:36+00:00",
    "updated": "2025-07-28T21:39:36+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Magnetic resonance imaging (MRI) is a crucial medical imaging modality.\nHowever, long acquisition times remain a significant challenge, leading to\nincreased costs, and reduced patient comfort. Recent studies have shown the\npotential of using deep learning models that incorporate information from prior\nsubject-specific MRI scans to improve reconstruction quality of present scans.\nIntegrating this prior information requires registration of the previous scan\nto the current image reconstruction, which can be time-consuming. We propose a\nnovel deep-learning-based MRI reconstruction framework which consists of an\ninitial reconstruction network, a deep registration model, and a\ntransformer-based enhancement network. We validated our method on a\nlongitudinal dataset of T1-weighted MRI scans with 2,808 images from 18\nsubjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics\nconfirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon\nsigned-rank test). Furthermore, we analyzed the impact of our MRI\nreconstruction method on the downstream task of brain segmentation and observed\nimproved accuracy and volumetric agreement with reference segmentations. Our\napproach also achieved a substantial reduction in total reconstruction time\ncompared to methods that use traditional registration algorithms, making it\nmore suitable for real-time clinical applications. The code associated with\nthis work is publicly available at\nhttps://github.com/amirshamaei/longitudinal-mri-deep-recon.",
    "html_url": "http://arxiv.org/abs/2507.21349v1",
    "pdf_url": "http://arxiv.org/pdf/2507.21349v1",
    "code_urls": [
      "https://github.com/amirshamaei/longitudinal-mri-deep-recon"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2507.19830v2",
    "title": "Taking Language Embedded 3D Gaussian Splatting into the Wild",
    "authors": [
      "Yuze Wang",
      "Yue Qi"
    ],
    "primary_category": null,
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "published": "2025-07-26T07:00:32+00:00",
    "updated": "2025-08-05T01:40:57+00:00",
    "comments": "Visit our project page at\n  https://yuzewang1998.github.io/takinglangsplatw/",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Recent advances in leveraging large-scale Internet photo collections for 3D\nreconstruction have enabled immersive virtual exploration of landmarks and\nhistoric sites worldwide. However, little attention has been given to the\nimmersive understanding of architectural styles and structural knowledge, which\nremains largely confined to browsing static text-image pairs. Therefore, can we\ndraw inspiration from 3D in-the-wild reconstruction techniques and use\nunconstrained photo collections to create an immersive approach for\nunderstanding the 3D structure of architectural components? To this end, we\nextend language embedded 3D Gaussian splatting (3DGS) and propose a novel\nframework for open-vocabulary scene understanding from unconstrained photo\ncollections. Specifically, we first render multiple appearance images from the\nsame viewpoint as the unconstrained image with the reconstructed radiance\nfield, then extract multi-appearance CLIP features and two types of language\nfeature uncertainty maps-transient and appearance uncertainty-derived from the\nmulti-appearance features to guide the subsequent optimization process. Next,\nwe propose a transient uncertainty-aware autoencoder, a multi-appearance\nlanguage field 3DGS representation, and a post-ensemble strategy to effectively\ncompress, learn, and fuse language features from multiple appearances. Finally,\nto quantitatively evaluate our method, we introduce PT-OVS, a new benchmark\ndataset for assessing open-vocabulary segmentation performance on unconstrained\nphoto collections. Experimental results show that our method outperforms\nexisting methods, delivering accurate open-vocabulary segmentation and enabling\napplications such as interactive roaming with open-vocabulary queries,\narchitectural style pattern recognition, and 3D scene editing.",
    "html_url": "http://arxiv.org/abs/2507.19830v2",
    "pdf_url": "http://arxiv.org/pdf/2507.19830v2",
    "code_urls": [],
    "project_urls": [
      "https://yuzewang1998.github.io/takinglangsplatw/"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2507.14596v1",
    "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary   queries in NeRF",
    "authors": [
      "Doriand Petit",
      "Steve Bourgeois",
      "Vincent Gay-Bellile",
      "Florian Chabot",
      "Loïc Barthe"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-07-19T12:46:20+00:00",
    "updated": "2025-07-19T12:46:20+00:00",
    "comments": "Published at ICCV'25",
    "journal_ref": null,
    "venue_inferred": "ICCV",
    "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.",
    "html_url": "http://arxiv.org/abs/2507.14596v1",
    "pdf_url": "http://arxiv.org/pdf/2507.14596v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2507.07605v1",
    "title": "LOSC: LiDAR Open-voc Segmentation Consolidator",
    "authors": [
      "Nermin Samet",
      "Gilles Puy",
      "Renaud Marlet"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-07-10T10:10:13+00:00",
    "updated": "2025-07-10T10:10:13+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We study the use of image-based Vision-Language Models (VLMs) for\nopen-vocabulary segmentation of lidar scans in driving settings. Classically,\nimage semantics can be back-projected onto 3D point clouds. Yet, resulting\npoint labels are noisy and sparse. We consolidate these labels to enforce both\nspatio-temporal consistency and robustness to image-level augmentations. We\nthen train a 3D network based on these refined labels. This simple method,\ncalled LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and\npanoptic segmentation on both nuScenes and SemanticKITTI, with significant\nmargins.",
    "html_url": "http://arxiv.org/abs/2507.07605v1",
    "pdf_url": "http://arxiv.org/pdf/2507.07605v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  }
]