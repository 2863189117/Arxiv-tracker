[
  {
    "id": "http://arxiv.org/abs/2508.15773v1",
    "title": "Scaling Group Inference for Diverse and High-Quality Generation",
    "authors": [
      "Gaurav Parmar",
      "Or Patashnik",
      "Daniil Ostashev",
      "Kuan-Chieh Wang",
      "Kfir Aberman",
      "Srinivasa Narasimhan",
      "Jun-Yan Zhu"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2025-08-21T17:59:57+00:00",
    "updated": "2025-08-21T17:59:57+00:00",
    "comments": "Project website: https://www.cs.cmu.edu/~group-inference, GitHub:\n  https://github.com/GaParmar/group-inference",
    "journal_ref": null,
    "venue_inferred": "www",
    "summary": "Generative models typically sample outputs independently, and recent\ninference-time guidance and scaling algorithms focus on improving the quality\nof individual samples. However, in real-world applications, users are often\npresented with a set of multiple images (e.g., 4-8) for each prompt, where\nindependent sampling tends to lead to redundant results, limiting user choices\nand hindering idea exploration. In this work, we introduce a scalable group\ninference method that improves both the diversity and quality of a group of\nsamples. We formulate group inference as a quadratic integer assignment\nproblem: candidate outputs are modeled as graph nodes, and a subset is selected\nto optimize sample quality (unary term) while maximizing group diversity\n(binary term). To substantially improve runtime efficiency, we progressively\nprune the candidate set using intermediate predictions, allowing our method to\nscale up to large candidate sets. Extensive experiments show that our method\nsignificantly improves group diversity and quality compared to independent\nsampling baselines and recent inference algorithms. Our framework generalizes\nacross a wide range of tasks, including text-to-image, image-to-image, image\nprompting, and video generation, enabling generative models to treat multiple\noutputs as cohesive groups rather than independent samples.",
    "html_url": "http://arxiv.org/abs/2508.15773v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15773v1",
    "code_urls": [
      "https://github.com/GaParmar/group-inference"
    ],
    "project_urls": [],
    "other_urls": [
      "https://www.cs.cmu.edu/~group-inference"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15774v1",
    "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
    "authors": [
      "Haonan Qiu",
      "Ning Yu",
      "Ziqi Huang",
      "Paul Debevec",
      "Ziwei Liu"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:59:57+00:00",
    "updated": "2025-08-21T17:59:57+00:00",
    "comments": "CineScale is an extended work of FreeScale (ICCV 2025). Project Page:\n  https://eyeline-labs.github.io/CineScale/, Code Repo:\n  https://github.com/Eyeline-Labs/CineScale",
    "journal_ref": null,
    "venue_inferred": "ICCV 2025",
    "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
    "html_url": "http://arxiv.org/abs/2508.15774v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15774v1",
    "code_urls": [
      "https://github.com/Eyeline-Labs/CineScale"
    ],
    "project_urls": [
      "https://eyeline-labs.github.io/CineScale/"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15772v1",
    "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
    "authors": [
      "Qingyang Mao",
      "Qi Cai",
      "Yehao Li",
      "Yingwei Pan",
      "Mingyue Cheng",
      "Ting Yao",
      "Qi Liu",
      "Tao Mei"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2025-08-21T17:59:32+00:00",
    "updated": "2025-08-21T17:59:32+00:00",
    "comments": "Source codes and models are available at\n  https://github.com/HiDream-ai/VAREdit",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Recent advances in diffusion models have brought remarkable visual fidelity\nto instruction-guided image editing. However, their global denoising process\ninherently entangles the edited region with the entire image context, leading\nto unintended spurious modifications and compromised adherence to editing\ninstructions. In contrast, autoregressive models offer a distinct paradigm by\nformulating image synthesis as a sequential process over discrete visual\ntokens. Their causal and compositional mechanism naturally circumvents the\nadherence challenges of diffusion-based methods. In this paper, we present\nVAREdit, a visual autoregressive (VAR) framework that reframes image editing as\na next-scale prediction problem. Conditioned on source image features and text\ninstructions, VAREdit generates multi-scale target features to achieve precise\nedits. A core challenge in this paradigm is how to effectively condition the\nsource image tokens. We observe that finest-scale source features cannot\neffectively guide the prediction of coarser target features. To bridge this\ngap, we introduce a Scale-Aligned Reference (SAR) module, which injects\nscale-matched conditioning information into the first self-attention layer.\nVAREdit demonstrates significant advancements in both editing adherence and\nefficiency. On standard benchmarks, it outperforms leading diffusion-based\nmethods by 30\\%+ higher GPT-Balance score. Moreover, it completes a\n$512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the\nsimilarly sized UltraEdit. The models are available at\nhttps://github.com/HiDream-ai/VAREdit.",
    "html_url": "http://arxiv.org/abs/2508.15772v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15772v1",
    "code_urls": [
      "https://github.com/HiDream-ai/VAREdit"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15769v1",
    "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
    "authors": [
      "Yanxu Meng",
      "Haoning Wu",
      "Ya Zhang",
      "Weidi Xie"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-08-21T17:59:16+00:00",
    "updated": "2025-08-21T17:59:16+00:00",
    "comments": "Technical Report; Project Page: https://mengmouxu.github.io/SceneGen",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
    "html_url": "http://arxiv.org/abs/2508.15769v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15769v1",
    "code_urls": [],
    "project_urls": [
      "https://mengmouxu.github.io/SceneGen"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15767v1",
    "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive   Parametric Human Modeling",
    "authors": [
      "Jinhyung Park",
      "Javier Romero",
      "Shunsuke Saito",
      "Fabian Prada",
      "Takaaki Shiratori",
      "Yichen Xu",
      "Federica Bogo",
      "Shoou-I Yu",
      "Kris Kitani",
      "Rawal Khirodkar"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:58:56+00:00",
    "updated": "2025-08-21T17:58:56+00:00",
    "comments": "ICCV 2025; Website: https://jindapark.github.io/projects/atlas/",
    "journal_ref": null,
    "venue_inferred": "ICCV 2025",
    "summary": "Parametric body models offer expressive 3D representation of humans across a\nwide range of poses, shapes, and facial expressions, typically derived by\nlearning a basis over registered 3D meshes. However, existing human mesh\nmodeling approaches struggle to capture detailed variations across diverse body\nposes and shapes, largely due to limited training data diversity and\nrestrictive modeling assumptions. Moreover, the common paradigm first optimizes\nthe external body surface using a linear basis, then regresses internal\nskeletal joints from surface vertices. This approach introduces problematic\ndependencies between internal skeleton and outer soft tissue, limiting direct\ncontrol over body height and bone lengths. To address these issues, we present\nATLAS, a high-fidelity body model learned from 600k high-resolution scans\ncaptured using 240 synchronized cameras. Unlike previous methods, we explicitly\ndecouple the shape and skeleton bases by grounding our mesh representation in\nthe human skeleton. This decoupling enables enhanced shape expressivity,\nfine-grained customization of body attributes, and keypoint fitting independent\nof external soft-tissue characteristics. ATLAS outperforms existing methods by\nfitting unseen subjects in diverse poses more accurately, and quantitative\nevaluations show that our non-linear pose correctives more effectively capture\ncomplex poses compared to linear models.",
    "html_url": "http://arxiv.org/abs/2508.15767v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15767v1",
    "code_urls": [],
    "project_urls": [
      "https://jindapark.github.io/projects/atlas/"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15766v1",
    "title": "Discovering Hidden Algebraic Structures via Transformers with Rank-Aware   Beam GRPO",
    "authors": [
      "Jaeha Lee",
      "Gio Huh",
      "Ning Su",
      "Tony Yue YU"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-21T17:58:50+00:00",
    "updated": "2025-08-21T17:58:50+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Recent efforts have extended the capabilities of transformers in logical\nreasoning and symbolic computations. In this work, we investigate their\ncapacity for non-linear latent pattern discovery in the context of functional\ndecomposition, focusing on the challenging algebraic task of multivariate\npolynomial decomposition. This problem, with widespread applications in science\nand engineering, is proved to be NP-hard, and demands both precision and\ninsight. Our contributions are threefold: First, we develop a synthetic data\ngeneration pipeline providing fine-grained control over problem complexity.\nSecond, we train transformer models via supervised learning and evaluate them\nacross four key dimensions involving scaling behavior and generalizability.\nThird, we propose Beam Grouped Relative Policy Optimization (BGRPO), a\nrank-aware reinforcement learning method suitable for hard algebraic problems.\nFinetuning with BGRPO improves accuracy while reducing beam width by up to\nhalf, resulting in approximately 75% lower inference compute. Additionally, our\nmodel demonstrates competitive performance in polynomial simplification,\noutperforming Mathematica in various cases.",
    "html_url": "http://arxiv.org/abs/2508.15766v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15766v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15764v1",
    "title": "Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space",
    "authors": [
      "Kiarash Kazari",
      "Ezzeldin Shereen",
      "György Dán"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-08-21T17:58:36+00:00",
    "updated": "2025-08-21T17:58:36+00:00",
    "comments": "Accepted for publication at ECAI 2025",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We address the problem of detecting adversarial attacks against cooperative\nmulti-agent reinforcement learning with continuous action space. We propose a\ndecentralized detector that relies solely on the local observations of the\nagents and makes use of a statistical characterization of the normal behavior\nof observable agents. The proposed detector utilizes deep neural networks to\napproximate the normal behavior of agents as parametric multivariate Gaussian\ndistributions. Based on the predicted density functions, we define a normality\nscore and provide a characterization of its mean and variance. This\ncharacterization allows us to employ a two-sided CUSUM procedure for detecting\ndeviations of the normality score from its mean, serving as a detector of\nanomalous behavior in real-time. We evaluate our scheme on various multi-agent\nPettingZoo benchmarks against different state-of-the-art attack methods, and\nour results demonstrate the effectiveness of our method in detecting impactful\nadversarial attacks. Particularly, it outperforms the discrete counterpart by\nachieving AUC-ROC scores of over 0.95 against the most impactful attacks in all\nevaluated environments.",
    "html_url": "http://arxiv.org/abs/2508.15764v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15764v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15763v1",
    "title": "Intern-S1: A Scientific Multimodal Foundation Model",
    "authors": [
      "Lei Bai",
      "Zhongrui Cai",
      "Maosong Cao",
      "Weihan Cao",
      "Chiyu Chen",
      "Haojiong Chen",
      "Kai Chen",
      "Pengcheng Chen",
      "Ying Chen",
      "Yongkang Chen",
      "Yu Cheng",
      "Yu Cheng",
      "Pei Chu",
      "Tao Chu",
      "Erfei Cui",
      "Ganqu Cui",
      "Long Cui",
      "Ziyun Cui",
      "Nianchen Deng",
      "Ning Ding",
      "Nanqin Dong",
      "Peijie Dong",
      "Shihan Dou",
      "Sinan Du",
      "Haodong Duan",
      "Caihua Fan",
      "Ben Gao",
      "Changjiang Gao",
      "Jianfei Gao",
      "Songyang Gao",
      "Yang Gao",
      "Zhangwei Gao",
      "Jiaye Ge",
      "Qiming Ge",
      "Lixin Gu",
      "Yuzhe Gu",
      "Aijia Guo",
      "Qipeng Guo",
      "Xu Guo",
      "Conghui He",
      "Junjun He",
      "Yili Hong",
      "Siyuan Hou",
      "Caiyu Hu",
      "Hanglei Hu",
      "Jucheng Hu",
      "Ming Hu",
      "Zhouqi Hua",
      "Haian Huang",
      "Junhao Huang",
      "Xu Huang",
      "Zixian Huang",
      "Zhe Jiang",
      "Lingkai Kong",
      "Linyang Li",
      "Peiji Li",
      "Pengze Li",
      "Shuaibin Li",
      "Tianbin Li",
      "Wei Li",
      "Yuqiang Li",
      "Dahua Lin",
      "Junyao Lin",
      "Tianyi Lin",
      "Zhishan Lin",
      "Hongwei Liu",
      "Jiangning Liu",
      "Jiyao Liu",
      "Junnan Liu",
      "Kai Liu",
      "Kaiwen Liu",
      "Kuikun Liu",
      "Shichun Liu",
      "Shudong Liu",
      "Wei Liu",
      "Xinyao Liu",
      "Yuhong Liu",
      "Zhan Liu",
      "Yinquan Lu",
      "Haijun Lv",
      "Hongxia Lv",
      "Huijie Lv",
      "Qidang Lv",
      "Ying Lv",
      "Chengqi Lyu",
      "Chenglong Ma",
      "Jianpeng Ma",
      "Ren Ma",
      "Runmin Ma",
      "Runyuan Ma",
      "Xinzhu Ma",
      "Yichuan Ma",
      "Zihan Ma",
      "Sixuan Mi",
      "Junzhi Ning",
      "Wenchang Ning",
      "Xinle Pang",
      "Jiahui Peng",
      "Runyu Peng",
      "Yu Qiao",
      "Jiantao Qiu",
      "Xiaoye Qu",
      "Yuan Qu",
      "Yuchen Ren",
      "Fukai Shang",
      "Wenqi Shao",
      "Junhao Shen",
      "Shuaike Shen",
      "Chunfeng Song",
      "Demin Song",
      "Diping Song",
      "Chenlin Su",
      "Weijie Su",
      "Weigao Sun",
      "Yu Sun",
      "Qian Tan",
      "Cheng Tang",
      "Huanze Tang",
      "Kexian Tang",
      "Shixiang Tang",
      "Jian Tong",
      "Aoran Wang",
      "Bin Wang",
      "Dong Wang",
      "Lintao Wang",
      "Rui Wang",
      "Weiyun Wang",
      "Wenhai Wang",
      "Yi Wang",
      "Ziyi Wang",
      "Ling-I Wu",
      "Wen Wu",
      "Yue Wu",
      "Zijian Wu",
      "Linchen Xiao",
      "Shuhao Xing",
      "Chao Xu",
      "Huihui Xu",
      "Jun Xu",
      "Ruiliang Xu",
      "Wanghan Xu",
      "GanLin Yang",
      "Yuming Yang",
      "Haochen Ye",
      "Jin Ye",
      "Shenglong Ye",
      "Jia Yu",
      "Jiashuo Yu",
      "Jing Yu",
      "Fei Yuan",
      "Bo Zhang",
      "Chao Zhang",
      "Chen Zhang",
      "Hongjie Zhang",
      "Jin Zhang",
      "Qiaosheng Zhang",
      "Qiuyinzhe Zhang",
      "Songyang Zhang",
      "Taolin Zhang",
      "Wenlong Zhang",
      "Wenwei Zhang",
      "Yechen Zhang",
      "Ziyang Zhang",
      "Haiteng Zhao",
      "Qian Zhao",
      "Xiangyu Zhao",
      "Xiangyu Zhao",
      "Bowen Zhou",
      "Dongzhan Zhou",
      "Peiheng Zhou",
      "Yuhao Zhou",
      "Yunhua Zhou",
      "Dongsheng Zhu",
      "Lin Zhu",
      "Yicheng Zou"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-08-21T17:58:00+00:00",
    "updated": "2025-08-21T17:58:00+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "In recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)\nmodel with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen online reinforcement learning (RL) in InternBootCamp, where we propose\nMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks\nsimultaneously. Through integrated innovations in algorithms, data, and\ntraining systems, Intern-S1 achieved top-tier performance in online RL\ntraining.On comprehensive evaluation benchmarks, Intern-S1 demonstrates\ncompetitive performance on general reasoning tasks among open-source models and\nsignificantly outperforms open-source models in scientific domains, surpassing\nclosed-source state-of-the-art models in professional tasks, such as molecular\nsynthesis planning, reaction condition prediction, predicting thermodynamic\nstabilities for crystals. Our models are available at\nhttps://huggingface.co/internlm/Intern-S1.",
    "html_url": "http://arxiv.org/abs/2508.15763v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15763v1",
    "code_urls": [
      "https://huggingface.co/internlm/Intern-S1"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15761v1",
    "title": "Waver: Wave Your Way to Lifelike Video Generation",
    "authors": [
      "Yifu Zhang",
      "Hao Yang",
      "Yuqi Zhang",
      "Yifei Hu",
      "Fengda Zhu",
      "Chuang Lin",
      "Xiaofeng Mei",
      "Yi Jiang",
      "Zehuan Yuan",
      "Bingyue Peng"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:56:10+00:00",
    "updated": "2025-08-21T17:56:10+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
    "html_url": "http://arxiv.org/abs/2508.15761v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15761v1",
    "code_urls": [
      "https://github.com/FoundationVision/Waver"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15760v1",
    "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on   Challenging Queries",
    "authors": [
      "Ming Yin",
      "Dinghan Shen",
      "Silei Xu",
      "Jianbing Han",
      "Sixun Dong",
      "Mian Zhang",
      "Yebowen Hu",
      "Shujian Liu",
      "Simin Ma",
      "Song Wang",
      "Sathish Reddy Indurthi",
      "Xun Wang",
      "Yiran Chen",
      "Kaiqiang Song"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-21T17:55:54+00:00",
    "updated": "2025-08-21T17:55:54+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.",
    "html_url": "http://arxiv.org/abs/2508.15760v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15760v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15757v1",
    "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual   Feedback",
    "authors": [
      "Yuxing Lu",
      "Yucheng Hu",
      "Nan Sun",
      "Xukai Zhao"
    ],
    "primary_category": null,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-08-21T17:55:07+00:00",
    "updated": "2025-08-21T17:55:07+00:00",
    "comments": "9 pages, 4 figures, 4 tables",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability.",
    "html_url": "http://arxiv.org/abs/2508.15757v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15757v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15755v1",
    "title": "Neural Robot Dynamics",
    "authors": [
      "Jie Xu",
      "Eric Heiden",
      "Iretiayo Akinola",
      "Dieter Fox",
      "Miles Macklin",
      "Yashraj Narang"
    ],
    "primary_category": null,
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2025-08-21T17:54:41+00:00",
    "updated": "2025-08-21T17:54:41+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Accurate and efficient simulation of modern robots remains challenging due to\ntheir high degrees of freedom and intricate mechanisms. Neural simulators have\nemerged as a promising alternative to traditional analytical simulators,\ncapable of efficiently predicting complex dynamics and adapting to real-world\ndata; however, existing neural simulators typically require\napplication-specific training and fail to generalize to novel tasks and/or\nenvironments, primarily due to inadequate representations of the global state.\nIn this work, we address the problem of learning generalizable neural\nsimulators for robots that are structured as articulated rigid bodies. We\npropose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models\nfor predicting future states for articulated rigid bodies under contact\nconstraints. NeRD uniquely replaces the low-level dynamics and contact solvers\nin an analytical simulator and employs a robot-centric and spatially-invariant\nsimulation state representation. We integrate the learned NeRD models as an\ninterchangeable backend solver within a state-of-the-art robotics simulator. We\nconduct extensive experiments to show that the NeRD simulators are stable and\naccurate over a thousand simulation steps; generalize across tasks and\nenvironment configurations; enable policy learning exclusively in a neural\nengine; and, unlike most classical simulators, can be fine-tuned from\nreal-world data to bridge the gap between simulation and reality.",
    "html_url": "http://arxiv.org/abs/2508.15755v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15755v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15754v1",
    "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis",
    "authors": [
      "Yufeng Zhao",
      "Junnan Liu",
      "Hongwei Liu",
      "Dongsheng Zhu",
      "Yuan Shen",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-21T17:50:24+00:00",
    "updated": "2025-08-21T17:50:24+00:00",
    "comments": "Preprint, working in progress",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks.",
    "html_url": "http://arxiv.org/abs/2508.15754v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15754v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15752v1",
    "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards   Geospatial AI Agents for Visual Inquiries",
    "authors": [
      "Jon E. Froehlich",
      "Jared Hwang",
      "Zeyu Wang",
      "John S. O'Meara",
      "Xia Su",
      "William Huang",
      "Yang Zhang",
      "Alex Fiannaca",
      "Philip Nelson",
      "Shaun Kane"
    ],
    "primary_category": null,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "H.5; I.2"
    ],
    "published": "2025-08-21T17:49:52+00:00",
    "updated": "2025-08-21T17:49:52+00:00",
    "comments": "Accepted to the ICCV'25 Workshop \"Vision Foundation Models and\n  Generative AI for Accessibility: Challenges and Opportunities\"",
    "journal_ref": null,
    "venue_inferred": "ICCV",
    "summary": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.",
    "html_url": "http://arxiv.org/abs/2508.15752v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15752v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15751v1",
    "title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered   All-in-SAM Model",
    "authors": [
      "Xueyuan Li",
      "Can Cui",
      "Ruining Deng",
      "Yucheng Tang",
      "Quan Liu",
      "Tianyuan Yao",
      "Shunxing Bao",
      "Naweed Chowdhury",
      "Haichun Yang",
      "Yuankai Huo"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T17:49:21+00:00",
    "updated": "2025-08-21T17:49:21+00:00",
    "comments": "25 pages, 3 figures, accepted by Journal of Medical Imaging",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use of cell-specific SAM models for\ndirect segmentation. These approaches enable effective segmentation across a\nrange of nuclei and cells. However, general vision foundation models often face\nchallenges with fine-grained semantic segmentation, such as identifying\nspecific nuclei subtypes or particular cells. Approach: In this paper, we\npropose the molecular-empowered All-in-SAM Model to advance computational\npathology by leveraging the capabilities of vision foundation models. This\nmodel incorporates a full-stack approach, focusing on: (1) annotation-engaging\nlay annotators through molecular-empowered learning to reduce the need for\ndetailed pixel-level annotations, (2) learning-adapting the SAM model to\nemphasize specific semantics, which utilizes its strong generalizability with\nSAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating\nMolecular-Oriented Corrective Learning (MOCL). Results: Experimental results\nfrom both in-house and public datasets show that the All-in-SAM model\nsignificantly improves cell classification performance, even when faced with\nvarying annotation quality. Conclusions: Our approach not only reduces the\nworkload for annotators but also extends the accessibility of precise\nbiomedical image analysis to resource-limited settings, thereby advancing\nmedical diagnostics and automating pathology image analysis.",
    "html_url": "http://arxiv.org/abs/2508.15751v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15751v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15746v1",
    "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic   Reasoning",
    "authors": [
      "Qiaoyu Zheng",
      "Yuze Sun",
      "Chaoyi Wu",
      "Weike Zhao",
      "Pengcheng Qiu",
      "Yongguo Yu",
      "Kun Sun",
      "Yanfeng Wang",
      "Ya Zhang",
      "Weidi Xie"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-08-21T17:42:47+00:00",
    "updated": "2025-08-21T17:42:47+00:00",
    "comments": "35 pages, 5 figures, 3 tables",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch.",
    "html_url": "http://arxiv.org/abs/2508.15746v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15746v1",
    "code_urls": [
      "https://github.com/MAGIC-AI4Med/Deep-DxSearch"
    ],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15737v1",
    "title": "Probability Density from Latent Diffusion Models for Out-of-Distribution   Detection",
    "authors": [
      "Joonas Järve",
      "Karl Kaspar Haavel",
      "Meelis Kull"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-08-21T17:27:35+00:00",
    "updated": "2025-08-21T17:27:35+00:00",
    "comments": "ECAI 2025",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Despite rapid advances in AI, safety remains the main bottleneck to deploying\nmachine-learning systems. A critical safety component is out-of-distribution\ndetection: given an input, decide whether it comes from the same distribution\nas the training data. In generative models, the most natural OOD score is the\ndata likelihood. Actually, under the assumption of uniformly distributed OOD\ndata, the likelihood is even the optimal OOD detector, as we show in this work.\nHowever, earlier work reported that likelihood often fails in practice, raising\ndoubts about its usefulness. We explore whether, in practice, the\nrepresentation space also suffers from the inability to learn good density\nestimation for OOD detection, or if it is merely a problem of the pixel space\ntypically used in generative models. To test this, we trained a Variational\nDiffusion Model not on images, but on the representation space of a pre-trained\nResNet-18 to assess the performance of our likelihood-based detector in\ncomparison to state-of-the-art methods from the OpenOOD suite.",
    "html_url": "http://arxiv.org/abs/2508.15737v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15737v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15726v1",
    "title": "Exploring the Landscape of Non-Equilibrium Memories with Neural Cellular   Automata",
    "authors": [
      "Ethan Lake",
      "Ehsan Pajouheshgar"
    ],
    "primary_category": null,
    "categories": [
      "cond-mat.stat-mech",
      "cs.CV",
      "cs.LG",
      "nlin.CG"
    ],
    "published": "2025-08-21T17:09:07+00:00",
    "updated": "2025-08-21T17:09:07+00:00",
    "comments": "4+9 pages",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We investigate the landscape of many-body memories: families of local\nnon-equilibrium dynamics that retain information about their initial conditions\nfor thermodynamically long time scales, even in the presence of arbitrary\nperturbations. In two dimensions, the only well-studied memory is Toom's rule.\nUsing a combination of rigorous proofs and machine learning methods, we show\nthat the landscape of 2D memories is in fact quite vast. We discover memories\nthat correct errors in ways qualitatively distinct from Toom's rule, have\nordered phases stabilized by fluctuations, and preserve information only in the\npresence of noise. Taken together, our results show that physical systems can\nperform robust information storage in many distinct ways, and demonstrate that\nthe physics of many-body memories is richer than previously realized.\nInteractive visualizations of the dynamics studied in this work are available\nat https://memorynca.github.io/2D.",
    "html_url": "http://arxiv.org/abs/2508.15726v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15726v1",
    "code_urls": [],
    "project_urls": [
      "https://memorynca.github.io/2D"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15721v1",
    "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal   E-Commerce Models",
    "authors": [
      "Xinyi Ling",
      "Hanwen Du",
      "Zhihui Zhu",
      "Xia Ning"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-21T17:01:12+00:00",
    "updated": "2025-08-21T17:01:12+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25.",
    "html_url": "http://arxiv.org/abs/2508.15721v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15721v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": [
      "https://anonymous.4open.science/r/submission25"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2508.15719v1",
    "title": "Tutorial on the Probabilistic Unification of Estimation Theory, Machine   Learning, and Generative AI",
    "authors": [
      "Mohammed Elmusrati"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-21T16:57:33+00:00",
    "updated": "2025-08-21T16:57:33+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Extracting meaning from uncertain, noisy data is a fundamental problem across\ntime series analysis, pattern recognition, and language modeling. This survey\npresents a unified mathematical framework that connects classical estimation\ntheory, statistical inference, and modern machine learning, including deep\nlearning and large language models. By analyzing how techniques such as maximum\nlikelihood estimation, Bayesian inference, and attention mechanisms address\nuncertainty, the paper illustrates that many AI methods are rooted in shared\nprobabilistic principles. Through illustrative scenarios including system\nidentification, image classification, and language generation, we show how\nincreasingly complex models build upon these foundations to tackle practical\nchallenges like overfitting, data sparsity, and interpretability. In other\nwords, the work demonstrates that maximum likelihood, MAP estimation, Bayesian\nclassification, and deep learning all represent different facets of a shared\ngoal: inferring hidden causes from noisy and/or biased observations. It serves\nas both a theoretical synthesis and a practical guide for students and\nresearchers navigating the evolving landscape of machine learning.",
    "html_url": "http://arxiv.org/abs/2508.15719v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15719v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15720v1",
    "title": "WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception",
    "authors": [
      "Zhiheng Liu",
      "Xueqing Deng",
      "Shoufa Chen",
      "Angtian Wang",
      "Qiushan Guo",
      "Mingfei Han",
      "Zeyue Xue",
      "Mengzhao Chen",
      "Ping Luo",
      "Linjie Yang"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV"
    ],
    "published": "2025-08-21T16:57:33+00:00",
    "updated": "2025-08-21T16:57:33+00:00",
    "comments": "Project page: https://johanan528.github.io/worldweaver_web/",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Generative video modeling has made significant strides, yet ensuring\nstructural and temporal consistency over long sequences remains a challenge.\nCurrent methods predominantly rely on RGB signals, leading to accumulated\nerrors in object structure and motion over extended durations. To address these\nissues, we introduce WorldWeaver, a robust framework for long video generation\nthat jointly models RGB frames and perceptual conditions within a unified\nlong-horizon modeling scheme. Our training framework offers three key\nadvantages. First, by jointly predicting perceptual conditions and color\ninformation from a unified representation, it significantly enhances temporal\nconsistency and motion dynamics. Second, by leveraging depth cues, which we\nobserve to be more resistant to drift than RGB, we construct a memory bank that\npreserves clearer contextual information, improving quality in long-horizon\nvideo generation. Third, we employ segmented noise scheduling for training\nprediction groups, which further mitigates drift and reduces computational\ncost. Extensive experiments on both diffusion- and rectified flow-based models\ndemonstrate the effectiveness of WorldWeaver in reducing temporal drift and\nimproving the fidelity of generated videos.",
    "html_url": "http://arxiv.org/abs/2508.15720v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15720v1",
    "code_urls": [],
    "project_urls": [
      "https://johanan528.github.io/worldweaver_web/"
    ],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15717v1",
    "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video   Understanding",
    "authors": [
      "Yanlai Yang",
      "Zhuokai Zhao",
      "Satya Narayan Shukla",
      "Aashu Singh",
      "Shlok Kumar Mishra",
      "Lizhu Zhang",
      "Mengye Ren"
    ],
    "primary_category": null,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-08-21T16:56:29+00:00",
    "updated": "2025-08-21T16:56:29+00:00",
    "comments": "15 pages, 3 figures",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
    "html_url": "http://arxiv.org/abs/2508.15717v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15717v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15711v1",
    "title": "Stemming -- The Evolution and Current State with a Focus on Bangla",
    "authors": [
      "Abhijit Paul",
      "Mashiat Amin Farin",
      "Sharif Md. Abdullah",
      "Ahmedul Kabir",
      "Zarif Masud",
      "Shebuti Rayana"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "published": "2025-08-21T16:54:24+00:00",
    "updated": "2025-08-21T16:54:24+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Bangla, the seventh most widely spoken language worldwide with 300 million\nnative speakers, faces digital under-representation due to limited resources\nand lack of annotated datasets. Stemming, a critical preprocessing step in\nlanguage analysis, is essential for low-resource, highly-inflectional languages\nlike Bangla, because it can reduce the complexity of algorithms and models by\nsignificantly reducing the number of words the algorithm needs to consider.\nThis paper conducts a comprehensive survey of stemming approaches, emphasizing\nthe importance of handling morphological variants effectively. While exploring\nthe landscape of Bangla stemming, it becomes evident that there is a\nsignificant gap in the existing literature. The paper highlights the\ndiscontinuity from previous research and the scarcity of accessible\nimplementations for replication. Furthermore, it critiques the evaluation\nmethodologies, stressing the need for more relevant metrics. In the context of\nBangla's rich morphology and diverse dialects, the paper acknowledges the\nchallenges it poses. To address these challenges, the paper suggests directions\nfor Bangla stemmer development. It concludes by advocating for robust Bangla\nstemmers and continued research in the field to enhance language analysis and\nprocessing.",
    "html_url": "http://arxiv.org/abs/2508.15711v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15711v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15710v1",
    "title": "End-to-End Analysis of Charge Stability Diagrams with Transformers",
    "authors": [
      "Rahul Marchand",
      "Lucas Schorling",
      "Cornelius Carlsson",
      "Jonas Schuff",
      "Barnaby van Straaten",
      "Taylor L. Patti",
      "Federico Fedele",
      "Joshua Ziegler",
      "Parth Girdhar",
      "Pranav Vaidhyanathan",
      "Natalia Ares"
    ],
    "primary_category": null,
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cs.LG",
      "quant-ph"
    ],
    "published": "2025-08-21T16:54:22+00:00",
    "updated": "2025-08-21T16:54:22+00:00",
    "comments": "8 pages, 2 figures, RM and LS contributed equally",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Transformer models and end-to-end learning frameworks are rapidly\nrevolutionizing the field of artificial intelligence. In this work, we apply\nobject detection transformers to analyze charge stability diagrams in\nsemiconductor quantum dot arrays, a key task for achieving scalability with\nspin-based quantum computing. Specifically, our model identifies triple points\nand their connectivity, which is crucial for virtual gate calibration, charge\nstate initialization, drift correction, and pulse sequencing. We show that it\nsurpasses convolutional neural networks in performance on three different spin\nqubit architectures, all without the need for retraining. In contrast to\nexisting approaches, our method significantly reduces complexity and runtime,\nwhile enhancing generalizability. The results highlight the potential of\ntransformer-based end-to-end learning frameworks as a foundation for a\nscalable, device- and architecture-agnostic tool for control and tuning of\nquantum dot devices.",
    "html_url": "http://arxiv.org/abs/2508.15710v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15710v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15709v1",
    "title": "Position Bias Mitigates Position Bias:Mitigate Position Bias Through   Inter-Position Knowledge Distillation",
    "authors": [
      "Yifei Wang",
      "Feng Xiong",
      "Yong Wang",
      "Linjing Li",
      "Xiangxiang Chu",
      "Daniel Dajun Zeng"
    ],
    "primary_category": null,
    "categories": [
      "cs.CL"
    ],
    "published": "2025-08-21T16:54:04+00:00",
    "updated": "2025-08-21T16:54:04+00:00",
    "comments": "EMNLP2025 Main",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Positional bias (PB), manifesting as non-uniform sensitivity across different\ncontextual locations, significantly impairs long-context comprehension and\nprocessing capabilities. While prior work seeks to mitigate PB through\nmodifying the architectures causing its emergence, significant PB still\npersists. To address PB effectively, we introduce \\textbf{Pos2Distill}, a\nposition to position knowledge distillation framework. Pos2Distill transfers\nthe superior capabilities from advantageous positions to less favorable ones,\nthereby reducing the huge performance gaps. The conceptual principle is to\nleverage the inherent, position-induced disparity to counteract the PB itself.\nWe identify distinct manifestations of PB under \\textbf{\\textsc{r}}etrieval and\n\\textbf{\\textsc{r}}easoning paradigms, thereby designing two specialized\ninstantiations: \\emph{Pos2Distill-R\\textsuperscript{1}} and\n\\emph{Pos2Distill-R\\textsuperscript{2}} respectively, both grounded in this\ncore principle. By employing the Pos2Distill approach, we achieve enhanced\nuniformity and significant performance gains across all contextual positions in\nlong-context retrieval and reasoning tasks. Crucially, both specialized systems\nexhibit strong cross-task generalization mutually, while achieving superior\nperformance on their respective tasks.",
    "html_url": "http://arxiv.org/abs/2508.15709v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15709v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15706v1",
    "title": "Communication Efficient LLM Pre-training with SparseLoCo",
    "authors": [
      "Amir Sarfi",
      "Benjamin Thérien",
      "Joel Lidin",
      "Eugene Belilovsky"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-21T16:48:19+00:00",
    "updated": "2025-08-21T16:48:19+00:00",
    "comments": "15 pages, 9 tables, 2 figures",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Communication-efficient distributed training algorithms have received\nconsiderable interest recently due to their benefits for training Large\nLanguage Models (LLMs) in bandwidth-constrained settings, such as across data\ncenters and over the internet. Despite reducing communication frequency, these\nmethods still typically require communicating a full copy of the model's\ngradients-resulting in a communication bottleneck even for cross-datacenter\nlinks. Furthermore, they can slightly degrade performance compared to a naive\nAdamW DDP baseline. While quantization and error feedback are often applied to\nreduce the pseudo-gradient's size, in the context of LLM pre-training, existing\napproaches have been unable to additionally leverage sparsification and have\nobtained limited quantization. In this work, we introduce SparseLoCo, a\ncommunication-efficient training algorithm for LLMs that effectively leverages\nTop-k sparsification and quantization to reach extreme compression ratios of up\nto 1-3% sparsity and 2-bit quantization while outperforming full-precision\nDiLoCo. Our key observations are that outer momentum can be locally\napproximated by an error feedback combined with aggressive sparsity and that\nsparse aggregation can actually improve model performance. We empirically\ndemonstrate in a range of communication-constrained LLM training settings that\nSparseLoCo provides significant benefits in both performance and communication\ncost.",
    "html_url": "http://arxiv.org/abs/2508.15706v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15706v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15697v1",
    "title": "Investigation of D-Wave quantum annealing for training Restricted   Boltzmann Machines and mitigating catastrophic forgetting",
    "authors": [
      "Abdelmoula El-Yazizi",
      "Yaroslav Koshka"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG",
      "quant-ph",
      "stat.ML"
    ],
    "published": "2025-08-21T16:26:58+00:00",
    "updated": "2025-08-21T16:26:58+00:00",
    "comments": "26 pages, 5 figures",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "Modest statistical differences between the sampling performances of the\nD-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC),\nwhen applied to Restricted Boltzmann Machines (RBMs), are explored to explain,\nand possibly address, the absence of significant and consistent improvements in\nRBM trainability when the D-Wave sampling was used in previous investigations.\nA novel hybrid sampling approach, combining the classical and the QA\ncontributions, is investigated as a promising way to benefit from the modest\ndifferences between the two sampling methods. No improvements in the RBM\ntraining are achieved in this work, thereby suggesting that the differences\nbetween the QA-based and MCMC sampling, mainly found in the medium-to-low\nprobability regions of the distribution, which are less important for the\nquality of the sample, are insufficient to benefit the training. Difficulties\nin achieving sufficiently high quality of embedding RBMs into the lattice of\nthe newer generation of D-Wave hardware could be further complicating the task.\nOn the other hand, the ability to generate samples of sufficient variety from\nlower-probability parts of the distribution has a potential to benefit other\nmachine learning applications, such as the mitigation of catastrophic\nforgetting (CF) during incremental learning. The feasibility of using\nQA-generated patterns of desirable classes for CF mitigation by the generative\nreplay is demonstrated in this work for the first time. While the efficiency of\nthe CF mitigation using the D-Wave QA was comparable to that of the classical\nmitigation, both the speed of generating a large number of distinct desirable\npatterns and the potential for further improvement make this approach promising\nfor a variety of challenging machine learning applications.",
    "html_url": "http://arxiv.org/abs/2508.15697v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15697v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15695v1",
    "title": "Conditionally adaptive augmented Lagrangian method for physics-informed   learning of forward and inverse problems using artificial neural networks",
    "authors": [
      "Qifeng Hu",
      "Shamsulhaq Basir",
      "Inanc Senocak"
    ],
    "primary_category": null,
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-21T16:22:40+00:00",
    "updated": "2025-08-21T16:22:40+00:00",
    "comments": "37 pages, 23 figures",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "We present several advances to the physics and equality constrained\nartificial neural networks (PECANN) framework that substantially improve its\ncapability to learn solutions of canonical partial differential equations\n(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support\nmultiple independent penalty parameters, enabling simultaneous enforcement of\nheterogeneous constraints. Second, we reformulate pointwise constraint\nenforcement and Lagrange multipliers as expectations over constraint terms,\nreducing memory overhead and permitting efficient mini-batch training. Third,\nto address PDEs with oscillatory, multi-scale features, we incorporate Fourier\nfeature mappings and show that a single mapping suffices where multiple\nmappings or more costly architectures were required in related methods. Fourth,\nwe introduce a time-windowing strategy for long-time evolution in which the\nterminal state of each window is enforced as an initial-condition constraint\nfor the next, ensuring continuity without discrete time models. Crucially, we\npropose a conditionally adaptive penalty update (CAPU) strategy for ALM, which\npreserves the principle that larger constraint violations incur stronger\npenalties. CAPU accelerates the growth of Lagrange multipliers for selectively\nchallenging constraints, enhancing constraint enforcement during training. We\ndemonstrate the effectiveness of PECANN-CAPU on problems including the\ntransonic rarefaction problem, reversible advection of a passive by a vortex,\nhigh-wavenumber Helmholtz and Poisson equations, and inverse identification of\nspatially varying heat sources. Comparisons with established methods and recent\nKolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive\naccuracy across all cases. Collectively, these advances improve PECANN's\nrobustness, efficiency, and applicability to demanding problems in scientific\ncomputing.",
    "html_url": "http://arxiv.org/abs/2508.15695v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15695v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15692v1",
    "title": "Effect Identification and Unit Categorization in the Multi-Score   Regression Discontinuity Design with Application to LED Manufacturing",
    "authors": [
      "Philipp Alexander Schwarz",
      "Oliver Schacht",
      "Sven Klaassen",
      "Johannes Oberpriller",
      "Martin Spindler"
    ],
    "primary_category": null,
    "categories": [
      "stat.ME",
      "cs.LG",
      "econ.EM"
    ],
    "published": "2025-08-21T16:17:15+00:00",
    "updated": "2025-08-21T16:17:15+00:00",
    "comments": "",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "The RDD (regression discontinuity design) is a widely used framework for\nidentification and estimation of causal effects at a cutoff of a single running\nvariable. Practical settings, in particular those encountered in production\nsystems, often involve decision-making defined by multiple thresholds and\ncriteria. Common MRD (multi-score RDD) approaches transform these to a\none-dimensional design, to employ identification and estimation results.\nHowever, this practice can introduce non-compliant behavior. We develop\ntheoretical tools to identify and reduce some of this \"fuzziness\" when\nestimating the cutoff-effect on compliers of sub-rules. We provide a sound\ndefinition and categorization of unit behavior types for multi-dimensional\ncutoff-rules, extending existing categorizations. We identify conditions for\nthe existence and identification of the cutoff-effect on complier in multiple\ndimensions, and specify when identification remains stable after excluding\nnevertaker and alwaystaker. Further, we investigate how decomposing\ncutoff-rules into simpler parts alters the unit behavior. This allows\nidentification and removal of non-compliant units potentially improving\nestimates. We validate our framework on simulated and real-world data from\nopto-electronic semiconductor manufacturing. Our empirical results demonstrate\nthe usability for refining production policies. Particularly we show that our\napproach decreases the estimation variance, highlighting the practical value of\nthe MRD framework in manufacturing.",
    "html_url": "http://arxiv.org/abs/2508.15692v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15692v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  },
  {
    "id": "http://arxiv.org/abs/2508.15690v1",
    "title": "GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark   for Structured Instruction Following and Visual Reasoning",
    "authors": [
      "Abhigya Verma",
      "Sriram Puttagunta",
      "Seganrasan Subramanian",
      "Sravan Ramachandran"
    ],
    "primary_category": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-08-21T16:13:49+00:00",
    "updated": "2025-08-21T16:13:49+00:00",
    "comments": "23 pages, 9 tables, 3 figures",
    "journal_ref": null,
    "venue_inferred": null,
    "summary": "GRAFT is a structured multimodal benchmark for evaluating models on\ninstruction-following, visual reasoning, and visual-textual alignment tasks. It\nfeatures programmatically generated charts and synthetically rendered tables,\ncreated with Python visualization libraries to ensure control over data\nsemantics, structure, and clarity. Each GRAFT instance pairs a chart or table\nimage with a systematically generated, multi-step analytical question based\nsolely on visual content. Answers are provided in structured formats such as\nJSON or YAML, supporting consistent evaluation of both reasoning and output\nformat. The benchmark introduces a taxonomy of reasoning types including\ncomparison, trend identification, ranking, aggregation, proportion estimation,\nand anomaly detection to enable comprehensive assessment. Reference answers\nfollow strict factual and formatting guidelines for precise, aspect-based\nevaluation. GRAFT offers a unified, scalable framework for fine-grained\nbenchmarking of multimodal models on visually grounded, structured reasoning\ntasks, setting a new evaluation standard in this field.",
    "html_url": "http://arxiv.org/abs/2508.15690v1",
    "pdf_url": "http://arxiv.org/pdf/2508.15690v1",
    "code_urls": [],
    "project_urls": [],
    "other_urls": []
  }
]