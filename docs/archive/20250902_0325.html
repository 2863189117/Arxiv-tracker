<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-02 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250902_0325</div>
    <div class="row"><div class="card">
<div class="title">VoCap: Video Object Captioning and Segmentation from Any Prompt</div>
<div class="meta-line">Authors: Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid</div>
<div class="meta-line">First: 2025-08-29T17:43:58+00:00 · Latest: 2025-08-29T17:43:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.21809v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.21809v1">PDF</a> · <a href="https://github.com/google-deepmind/vocap">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding objects in videos in terms of fine-grained localization masks
and detailed semantic properties is a fundamental task in video understanding.
In this paper, we propose VoCap, a flexible video model that consumes a video
and a prompt of various modalities (text, box or mask), and produces a
spatio-temporal masklet with a corresponding object-centric caption. As such
our model addresses simultaneously the tasks of promptable video object
segmentation, referring expression segmentation, and object captioning. Since
obtaining data for this task is tedious and expensive, we propose to annotate
an existing large-scale segmentation dataset (SAV) with pseudo object captions.
We do so by preprocessing videos with their ground-truth masks to highlight the
object of interest and feed this to a large Vision Language Model (VLM). For an
unbiased evaluation, we collect manual annotations on the validation set. We
call the resulting dataset SAV-Caption. We train our VoCap model at scale on a
SAV-Caption together with a mix of other image and video datasets. Our model
yields state-of-the-art results on referring expression video object
segmentation, is competitive on semi-supervised video object segmentation, and
establishes a benchmark for video object captioning. Our dataset will be made
available at https://github.com/google-deepmind/vocap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VoCap：从任意提示进行视频对象描述和分割</div>
<div class="mono" style="margin-top:8px">理解视频中的对象，以精细粒度的定位掩码和详细的语义属性为单位，是视频理解中的基本任务。在本文中，我们提出了一种灵活的视频模型VoCap，该模型可以消耗视频和各种模态（文本、框或掩码）的提示，并生成相应的时空掩码片段和对象为中心的描述。因此，我们的模型同时解决了可提示视频对象分割、指示表达分割和对象描述的任务。由于获取此类任务的数据既繁琐又昂贵，我们建议对现有的大规模分割数据集（SAV）进行伪对象描述注释。我们通过预处理带有真实掩码的视频以突出显示目标对象，并将其输入大型视觉语言模型（VLM）来实现这一点。为了进行公平的评估，我们在验证集上收集了人工注释。我们称该数据集为SAV-描述。我们在SAV-描述以及多种图像和视频数据集的混合数据集上大规模训练我们的VoCap模型。我们的模型在指示表达视频对象分割上达到了最先进的结果，在半监督视频对象分割上具有竞争力，并建立了视频对象描述的基准。我们的数据集将在https://github.com/google-deepmind/vocap/提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VoCap is a video model that takes a video and a prompt (text, box, or mask) and generates a spatio-temporal mask and an object-centric caption. It addresses video object segmentation, referring expression segmentation, and object captioning. VoCap was trained on a new dataset, SAV-Caption, created by annotating an existing segmentation dataset with pseudo object captions. The model outperforms previous methods on referring expression video object segmentation and achieves competitive results on semi-supervised video object segmentation, setting a new benchmark for video object captioning.</div>
<div class="mono" style="margin-top:8px">VoCap 是一种视频模型，它接受视频和提示（文本、框或掩码），并生成时空掩码和对象中心的描述。它同时解决了视频对象分割、引用表达视频对象分割和对象描述的任务。VoCap 在一个新数据集 SAV-Caption 上进行了训练，该数据集是通过为现有分割数据集添加伪对象描述注释而创建的。该模型在引用表达视频对象分割上超越了先前的方法，并在半监督视频对象分割上取得了竞争力的结果，为视频对象描述设定了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Tree-Guided Diffusion Planner</div>
<div class="meta-line">Authors: Hyeonseong Jeon, Cheolhong Min, Jaesik Park</div>
<div class="meta-line">First: 2025-08-29T17:27:44+00:00 · Latest: 2025-08-29T17:27:44+00:00</div>
<div class="meta-line">Comments: 20 pages, 11 figures, 14 tables (main paper + appendix) / under
  review / project page will be available after the paper becomes public in
  arxiv</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.21800v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.21800v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Tree-Guided Diffusion Planner (TDP) addresses the limitations of gradient guidance in solving complex, non-convex problems by balancing exploration and exploitation through structured trajectory generation. TDP uses a bi-level sampling process where diverse parent trajectories are generated via particle guidance and then refined through fast conditional denoising. On three diverse tasks, TDP outperforms state-of-the-art approaches, demonstrating its effectiveness in real-world scenarios.</div>
<div class="mono" style="margin-top:8px">研究旨在解决梯度指导在处理非凸目标和非可微约束的实际规划问题中的局限性。TDP 是一种零样本测试时规划框架，采用树引导方法平衡探索和利用。它通过生成多样化的父轨迹进行广泛探索，并通过快速条件去噪进行细化。TDP 在迷宫金币收集、机器人手臂块操作和AntMaze多目标探索任务中均优于现有最佳方法。</div>
</details>
</div>
<div class="card">
<div class="title">PiCSAR: Probabilistic Confidence Selection And Ranking</div>
<div class="meta-line">Authors: Joshua Ong Jun Leang, Zheng Zhao, Aryo Pradipta Gema, Sohee Yang, Wai-Chung Kwan, Xuanli He, Wenda Li, Pasquale Minervini, Eleonora Giunchiglia, Shay B. Cohen</div>
<div class="meta-line">First: 2025-08-29T17:03:47+00:00 · Latest: 2025-08-29T17:03:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.21787v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.21787v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PiCSAR：概率置信度选择与排序</div>
<div class="mono" style="margin-top:8px">最佳-n采样通过生成多个候选解决方案并选择具有最高奖励的解决方案，提高了大型语言模型（LLMs）和大型推理模型（LRMs）的准确性。推理任务的关键挑战是设计一个评分函数，能够在不访问正确答案的情况下识别正确的推理链。我们提出了一种简单且无需训练的方法——概率置信度选择与排序（PiCSAR）：该方法使用推理和最终答案的联合对数似然性对每个候选生成进行评分。推理和最终答案的联合对数似然性自然分解为推理置信度和答案置信度。PiCSAR 在多个基准测试中取得了显著的改进（在 MATH500 上 +10.18，在 AIME2025 上 +9.81），在 20 次比较中有 16 次优于基线，使用至少少 2 倍的样本。我们的分析表明，正确的推理链在推理和答案置信度方面表现出显著更高的值，这证明了 PiCSAR 的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PiCSAR is a training-free method that improves the accuracy of large language models and reasoning models by selecting the best candidate solution based on the joint log-likelihood of reasoning and final answer. It outperforms baselines with fewer samples across various benchmarks, achieving significant gains such as +10.18 on MATH500 and +9.81 on AIME2025.</div>
<div class="mono" style="margin-top:8px">PiCSAR 是一种无需训练的方法，通过基于推理和最终答案的联合对数似然性来选择最佳候选解决方案，从而提高大型语言模型和推理模型的准确性。它在各种基准测试中优于基线模型，尤其是在 MATH500 上提高了 +10.18，在 AIME2025 上提高了 +9.81。</div>
</details>
</div>
<div class="card">
<div class="title">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD   Model Datasets for fine-tuning Large Vision-Language Models</div>
<div class="meta-line">Authors: João Valente, Atabak Dehban, Rodrigo Ventura</div>
<div class="meta-line">First: 2025-08-29T15:57:43+00:00 · Latest: 2025-08-29T15:57:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.21732v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.21732v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA&#x27;s of these models with
CAD2DMD-SET&#x27;s generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of Large Vision-Language Models (LVLMs) in reading values from Digital Measurement Devices (DMDs) under challenging conditions. It introduces CAD2DMD-SET, a synthetic data generation tool that uses 3D CAD models and advanced rendering to create diverse, VQA-labelled datasets. Benchmarking three state-of-the-art LVLMs with CAD2DMD-SET and DMDBench showed significant improvements, particularly for InternVL, which saw a 200% score increase without degrading on other tasks. The tool aims to enhance LVLM robustness in real-world conditions.</div>
<div class="mono" style="margin-top:8px">该研究针对大型视觉-语言模型（LVLM）在处理数字测量设备（DMD）读取值时在现实条件下的局限性。引入了CAD2DMD-SET，这是一种从3D CAD模型生成多样化的、带有VQA标签的合成数据集的工具。通过使用CAD2DMD-SET和DMDBench对三种最先进的LVLM进行基准测试，显示出显著的改进，特别是InternVL，其得分提高了200%，且在其他任务上没有退步。这表明CAD2DMD-SET能够增强LVLM在挑战性条件下的鲁棒性和性能。</div>
</details>
</div>
<div class="card">
<div class="title">PosterForest: Hierarchical Multi-Agent Collaboration for Scientific   Poster Generation</div>
<div class="meta-line">Authors: Jiho Choi, Seojeong Park, Seongjong Song, Hyunjung Shim</div>
<div class="meta-line">First: 2025-08-29T15:36:06+00:00 · Latest: 2025-08-29T15:36:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.21720v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.21720v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel training-free framework, \textit{PosterForest}, for
automated scientific poster generation. Unlike prior approaches, which largely
neglect the hierarchical structure of scientific documents and the semantic
integration of textual and visual elements, our method addresses both
challenges directly. We introduce the \textit{Poster Tree}, a hierarchical
intermediate representation that jointly encodes document structure and
visual-textual relationships at multiple levels. Our framework employs a
multi-agent collaboration strategy, where agents specializing in content
summarization and layout planning iteratively coordinate and provide mutual
feedback. This approach enables the joint optimization of logical consistency,
content fidelity, and visual coherence. Extensive experiments on multiple
academic domains show that our method outperforms existing baselines in both
qualitative and quantitative evaluations. The resulting posters achieve quality
closest to expert-designed ground truth and deliver superior information
preservation, structural clarity, and user preference.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research motivation is to develop a training-free framework, PosterForest, for automated scientific poster generation that addresses the hierarchical structure and semantic integration challenges. The method uses a Poster Tree as a hierarchical intermediate representation and a multi-agent collaboration strategy involving content summarization and layout planning agents. The key experimental findings show that PosterForest outperforms existing baselines in both qualitative and quantitative evaluations, achieving quality close to expert-designed posters and superior information preservation, structural clarity, and user preference.</div>
<div class="mono" style="margin-top:8px">研究动机是开发一个无需训练的框架PosterForest，用于自动化生成科学海报，解决科学文档的层次结构和语义集成问题。方法使用Poster Tree作为中间的层次表示，并采用内容摘要和布局规划的多智能体协作策略。关键实验发现表明，PosterForest在定性和定量评估中均优于现有基线，生成的海报质量接近专家设计的原型，并且具有更好的信息保留、结构清晰度和用户偏好。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
