<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-08-25 17:09</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250825_1709</div>
    <div class="row"><div class="card">
<div class="title">DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language   Navigation in Dynamic Changing Scenes</div>
<div class="meta-line">Authors: Jiajun Jiang, Yiming Zhu, Zirui Wu, Jie Song</div>
<div class="meta-line">First: 2025-06-02T17:59:10+00:00 · Latest: 2025-08-13T07:21:25+00:00</div>
<div class="meta-line">Comments: 14 pages, 14 figures. Code: https://github.com/Eku127/DualMap Project
  page: https://eku127.github.io/DualMap/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2506.01950v3">Abs</a> · <a href="http://arxiv.org/pdf/2506.01950v3">PDF</a> · <a href="https://github.com/Eku127/DualMap">Code1</a> · <a href="https://eku127.github.io/DualMap/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce DualMap, an online open-vocabulary mapping system that enables
robots to understand and navigate dynamically changing environments through
natural language queries. Designed for efficient semantic mapping and
adaptability to changing environments, DualMap meets the essential requirements
for real-world robot navigation applications. Our proposed hybrid segmentation
frontend and object-level status check eliminate the costly 3D object merging
required by prior methods, enabling efficient online scene mapping. The
dual-map representation combines a global abstract map for high-level candidate
selection with a local concrete map for precise goal-reaching, effectively
managing and updating dynamic changes in the environment. Through extensive
experiments in both simulation and real-world scenarios, we demonstrate
state-of-the-art performance in 3D open-vocabulary segmentation, efficient
scene mapping, and online language-guided navigation.Project page:
https://eku127.github.io/DualMap/</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DualMap is an online open-vocabulary mapping system that allows robots to navigate dynamically changing environments using natural language. It uses a hybrid segmentation frontend and object-level status check to avoid costly 3D object merging, enabling efficient online scene mapping. The system employs a dual-map representation combining a global abstract map for high-level candidate selection and a local concrete map for precise goal-reaching, demonstrating state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation through extensive experiments in both simulation and real-world scenarios.</div>
<div class="mono" style="margin-top:8px">DualMap 是一种在线开放词汇映射系统，使机器人能够通过自然语言在动态变化的环境中导航。它使用混合分割前端和对象级状态检查来高效地映射场景，无需进行昂贵的 3D 对象合并。该系统采用双重地图表示，结合全局抽象地图进行高层次候选选择和局部具体地图进行精确目标导航，有效管理动态变化。实验表明，其在 3D 开放词汇分割、高效场景映射和在线语言引导导航方面表现出最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in   City-scale Scenarios</div>
<div class="meta-line">Authors: Jialei Xu, Zizhuang Wei, Weikang You, Linyun Li, Weijian Sun</div>
<div class="meta-line">First: 2025-08-13T03:55:56+00:00 · Latest: 2025-08-13T03:55:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.09470v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.09470v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CitySeg is a foundation model for city-scale point cloud semantic segmentation that incorporates text modality to achieve open vocabulary segmentation and zero-shot inference. It addresses the challenges of limited data scale and domain gap by customizing data preprocessing rules and proposing a local-global cross-attention network. The model introduces a hierarchical classification strategy and a two-stage training strategy, achieving state-of-the-art performance on nine closed-set benchmarks and enabling zero-shot generalization in city-scale scenarios without visual information.</div>
<div class="mono" style="margin-top:8px">CitySeg 是一种用于城市规模点云语义分割的基础模型，通过引入文本模态实现开放词汇分割和零样本推理。该模型通过定制数据预处理规则和提出局部-全局交叉注意力网络来解决数据规模有限和领域差异的问题。此外，模型引入了层次分类策略和两阶段训练策略，实现了在九个封闭集基准上的最先进性能，并首次在城市规模点云场景中实现了无需视觉信息的零样本泛化。</div>
</details>
</div>
<div class="card">
<div class="title">ReferSplat: Referring Segmentation in 3D Gaussian Splatting</div>
<div class="meta-line">Authors: Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding</div>
<div class="meta-line">Venue: ICML 2025 Oral</div>
<div class="meta-line">First: 2025-08-11T17:59:30+00:00 · Latest: 2025-08-11T17:59:30+00:00</div>
<div class="meta-line">Comments: ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.08252v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.08252v1">PDF</a> · <a href="https://github.com/heshuting555/ReferSplat">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReferSplat: 3D 高斯点云分割中的引用分割</div>
<div class="mono" style="margin-top:8px">我们引入了基于自然语言描述的3D 高斯点云分割 (R3DGS) 新任务，该任务旨在根据自然语言描述对3D 高斯场景中的目标物体进行分割，这些描述通常包含空间关系或物体属性。该任务要求模型识别新描述的可能被遮挡或在新视角中不可见的物体，这为3D 多模态理解带来了重大挑战。开发这种能力对于推进具身人工智能至关重要。为了支持该领域的研究，我们构建了第一个R3DGS数据集Ref-LERF。我们的分析表明，3D 多模态理解和空间关系建模是R3DGS的关键挑战。为了解决这些挑战，我们提出了ReferSplat框架，该框架在空间感知范式中使用自然语言表达明确建模3D 高斯点。ReferSplat在新提出的R3DGS任务和3D 开放词汇分割基准测试中均实现了最先进的性能。数据集和代码可在https://github.com/heshuting555/ReferSplat获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces R3DGS, a task for segmenting target objects in 3D Gaussian scenes based on natural language descriptions, which often include spatial relationships or object attributes. The authors propose ReferSplat, a framework that models 3D Gaussian points with natural language expressions, achieving state-of-the-art performance on both the new R3DGS task and 3D open-vocabulary segmentation benchmarks. The work also includes the first R3DGS dataset, Ref-LERF, to support research in 3D multi-modal understanding and spatial relationship modeling.</div>
<div class="mono" style="margin-top:8px">论文介绍了R3DGS任务，该任务基于自然语言描述（通常包含空间关系或物体属性）对3D高斯场景中的目标物体进行分割。作者提出了ReferSplat框架，该框架使用自然语言表达来建模3D高斯点，实现了在新提出的R3DGS任务和3D开放词汇分割基准上的最佳性能。此外，还提供了第一个R3DGS数据集Ref-LERF，以支持3D多模态理解和空间关系建模的研究。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
