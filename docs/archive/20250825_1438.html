<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-08-25 14:38</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250825_1438</div>
    <div class="row"><div class="card">
<div class="title">MV-RAG: Retrieval Augmented Multiview Diffusion</div>
<div class="meta-line">Authors: Yosef Dayani, Omer Benishu, Sagie Benaim</div>
<div class="meta-line">First: 2025-08-22T17:59:40+00:00 · Latest: 2025-08-22T17:59:40+00:00</div>
<div class="meta-line">Comments: Project page: https://yosefdayani.github.io/MV-RAG</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16577v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16577v1">PDF</a> · <a href="https://yosefdayani.github.io/MV-RAG">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-3D generation approaches have advanced significantly by leveraging
pretrained 2D diffusion priors, producing high-quality and 3D-consistent
outputs. However, they often fail to produce out-of-domain (OOD) or rare
concepts, yielding inconsistent or inaccurate results. To this end, we propose
MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images
from a large in-the-wild 2D database and then conditions a multiview diffusion
model on these images to synthesize consistent and accurate multiview outputs.
Training such a retrieval-conditioned model is achieved via a novel hybrid
strategy bridging structured multiview data and diverse 2D image collections.
This involves training on multiview data using augmented conditioning views
that simulate retrieval variance for view-specific reconstruction, alongside
training on sets of retrieved real-world 2D images using a distinctive held-out
view prediction objective: the model predicts the held-out view from the other
views to infer 3D consistency from 2D data. To facilitate a rigorous OOD
evaluation, we introduce a new collection of challenging OOD prompts.
Experiments against state-of-the-art text-to-3D, image-to-3D, and
personalization baselines show that our approach significantly improves 3D
consistency, photorealism, and text adherence for OOD/rare concepts, while
maintaining competitive performance on standard benchmarks.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-3D generation approaches have advanced significantly by leveraging pretrained 2D diffusion priors, producing high-quality and 3D-consistent outputs.</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Training Paradigms, Dataset Composition, and Model Scaling   for Child ASR in ESPnet</div>
<div class="meta-line">Authors: Anyu Ying, Natarajan Balaji Shankar, Chyi-Jiunn Lin, Mohan Shi, Pu Wang, Hye-jin Shim, Siddhant Arora, Hugo Van hamme, Abeer Alwan, Shinji Watanabe</div>
<div class="meta-line">First: 2025-08-22T17:59:35+00:00 · Latest: 2025-08-22T17:59:35+00:00</div>
<div class="meta-line">Comments: 5 pages, 3 figures, presented at WOCCI 2025 (Workshop on Child
  Computer Interaction), satellite workshop of Interspeech 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16576v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16576v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite advancements in ASR, child speech recognition remains challenging due to acoustic variability and limited annotated data.</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep   Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and   Driving Systems</div>
<div class="meta-line">Authors: Yizhi Wang, Degang Xu, Yongfang Xie, Shuzhong Tan, Xianan Zhou, Peng Chen</div>
<div class="meta-line">First: 2025-08-22T17:57:56+00:00 · Latest: 2025-08-22T17:57:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16574v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16574v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a hierarchical decision-making framework for autonomous
navigation in four-wheel independent steering and driving (4WISD) systems. The
proposed approach integrates deep reinforcement learning (DRL) for high-level
navigation with fuzzy logic for low-level control to ensure both task
performance and physical feasibility. The DRL agent generates global motion
commands, while the fuzzy logic controller enforces kinematic constraints to
prevent mechanical strain and wheel slippage. Simulation experiments
demonstrate that the proposed framework outperforms traditional navigation
methods, offering enhanced training efficiency and stability and mitigating
erratic behaviors compared to purely DRL-based solutions. Real-world
validations further confirm the framework&#x27;s ability to navigate safely and
effectively in dynamic industrial settings. Overall, this work provides a
scalable and reliable solution for deploying 4WISD mobile robots in complex,
real-world scenarios.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a hierarchical decision-making framework for autonomous navigation in four-wheel independent steering and driving (4WISD) systems.</div>
</details>
</div>
<div class="card">
<div class="title">Are LLM-Powered Social Media Bots Realistic?</div>
<div class="meta-line">Authors: Lynnette Hui Xian Ng, Kathleen M. Carley</div>
<div class="meta-line">First: 2025-08-01T18:06:13+00:00 · Latest: 2025-08-22T17:56:26+00:00</div>
<div class="meta-line">Comments: Accepted into SBP-BRiMS 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.00998v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.00998v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models (LLMs) become more sophisticated, there is a
possibility to harness LLMs to power social media bots. This work investigates
the realism of generating LLM-Powered social media bot networks. Through a
combination of manual effort, network science and LLMs, we create synthetic bot
agent personas, their tweets and their interactions, thereby simulating social
media networks. We compare the generated networks against empirical bot/human
data, observing that both network and linguistic properties of LLM-Powered Bots
differ from Wild Bots/Humans. This has implications towards the detection and
effectiveness of LLM-Powered Bots.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots.</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence</div>
<div class="meta-line">Authors: Alisa Vinogradova, Vlad Vinogradov, Dmitrii Radkevich, Ilya Yasny, Dmitry Kobyzev, Ivan Izmailov, Katsiaryna Yanchanka, Andrey Doronichev</div>
<div class="meta-line">First: 2025-08-22T17:50:00+00:00 · Latest: 2025-08-22T17:50:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16571v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16571v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren&#x27;t capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
