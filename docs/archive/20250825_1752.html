<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-08-25 17:52</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250825_1752</div>
    <div class="row"><div class="card">
<div class="title">Flow Matching-Based Generative Modeling for Efficient and Scalable Data   Assimilation</div>
<div class="meta-line">Authors: Taos Transue, Bohan Chen, So Takao, Bao Wang</div>
<div class="meta-line">First: 2025-08-18T19:00:45+00:00 · Latest: 2025-08-22T15:54:49+00:00</div>
<div class="meta-line">Comments: correcting authorship footnote, reformatting figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.13313v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.13313v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data assimilation (DA) is the problem of sequentially estimating the state of
a dynamical system from noisy observations. Recent advances in generative
modeling have inspired new approaches to DA in high-dimensional nonlinear
settings, especially the ensemble score filter (EnSF). However, these come at a
significant computational burden due to slow sampling. In this paper, we
introduce a new filtering framework based on flow matching (FM) -- called the
ensemble flow filter (EnFF) -- to accelerate sampling and enable flexible
design of probability paths. EnFF -- a training-free DA approach -- integrates
MC estimators for the marginal FM vector field (VF) and a localized guidance to
assimilate observations. EnFF has faster sampling and more flexibility in VF
design compared to existing generative modeling for DA. Theoretically, we show
that EnFF encompasses classical filtering methods such as the bootstrap
particle filter and the ensemble Kalman filter as special cases. Experiments on
high-dimensional filtering benchmarks demonstrate improved cost-accuracy
tradeoffs and the ability to leverage larger ensembles than prior methods. Our
results highlight the promise of FM as a scalable tool for filtering in
high-dimensional applications that enable the use of large ensembles.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Data assimilation (DA) is the problem of sequentially estimating the state of a dynamical system from noisy observations.</div>
</details>
</div>
<div class="card">
<div class="title">Modular Embedding Recomposition for Incremental Learning</div>
<div class="meta-line">Authors: Aniello Panariello, Emanuele Frascaroli, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</div>
<div class="meta-line">First: 2025-08-22T15:25:40+00:00 · Latest: 2025-08-22T15:25:40+00:00</div>
<div class="meta-line">Comments: Accepted to the 36th British Machine Vision Conference (BMVC 2025),
  Sheffield, UK</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16463v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16463v1">PDF</a> · <a href="https://github.com/aimagelab/mammoth">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities.</div>
</details>
</div>
<div class="card">
<div class="title">PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</div>
<div class="meta-line">Authors: Adil Bahaj, Mounir Ghogho</div>
<div class="meta-line">First: 2025-08-22T14:50:55+00:00 · Latest: 2025-08-22T14:50:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16439v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16439v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) and vision-augmented LLMs (VLMs) have
significantly advanced medical informatics, diagnostics, and decision support.
However, these models exhibit systematic biases, particularly age bias,
compromising their reliability and equity. This is evident in their poorer
performance on pediatric-focused text and visual question-answering tasks. This
bias reflects a broader imbalance in medical research, where pediatric studies
receive less funding and representation despite the significant disease burden
in children. To address these issues, a new comprehensive multi-modal pediatric
question-answering benchmark, PediatricsMQA, has been introduced. It consists
of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric
topics across seven developmental stages (prenatal to adolescent) and 2,067
vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256
anatomical regions. The dataset was developed using a hybrid manual-automatic
pipeline, incorporating peer-reviewed pediatric literature, validated question
banks, existing benchmarks, and existing QA resources. Evaluating
state-of-the-art open models, we find dramatic performance drops in younger
cohorts, highlighting the need for age-aware methods to ensure equitable AI
support in pediatric care.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support.</div>
</details>
</div>
<div class="card">
<div class="title">CAMA: Enhancing Multimodal In-Context Learning with Context-Aware   Modulated Attention</div>
<div class="meta-line">Authors: Yanshu Li, Jianjiang Yang, Ziteng Yang, Bozheng Li, Hongyang He, Zhengtao Yao, Ligong Han, Yingjie Victor Chen, Songlin Fei, Dongfang Liu, Ruixiang Tang</div>
<div class="meta-line">First: 2025-05-21T04:25:23+00:00 · Latest: 2025-08-22T14:44:22+00:00</div>
<div class="meta-line">Comments: 14 pages, 8 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17097v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17097v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal in-context learning (ICL) is emerging as a key capability that
enables large vision-language models (LVLMs) to adapt to novel tasks without
parameter updates, expanding their utility across various real-world
applications. However, ICL remains unstable, even with well-matched in-context
demonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the
provided context. While existing efforts focus on prompt engineering or
post-hoc logit calibration, we instead investigate the underlying attention
dynamics to overcome LVLMs&#x27; inherent limitations. We identify two critical
deficits in their self-attention that impair effective ICL. To bridge the gap,
we propose \textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play
and training-free method that dynamically modulates LVLM&#x27;s attention logits
based on the input in-context sequence. CAMA employs a two-stage attention
modulation to address both identified deficits, enhancing the focus on
semantically significant tokens, particularly visual ones. Across four LVLMs
and seven benchmarks, CAMA consistently outperforms vanilla models and
baselines, demonstrating great effectiveness and generalization. It can also
activate the desired effects of prompt engineering methods and remains robust
under diverse sequence configurations. Thus, CAMA paves the way for deeper
explorations of attention dynamics to advance multimodal reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal in-context learning (ICL) is emerging as a key capability that enables large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, expanding their utility across various real-world applications.</div>
</details>
</div>
<div class="card">
<div class="title">Retrieval Enhanced Feedback via In-context Neural Error-book</div>
<div class="meta-line">Authors: Jongyeop Hyun, Bumsoo Kim</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-08-22T11:50:04+00:00 · Latest: 2025-08-22T11:50:04+00:00</div>
<div class="meta-line">Comments: Accepted at EMNLP 2025 main conference</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16313v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16313v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE&#x27;s potential for
enhancing multimodal reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining.</div>
</details>
</div>
<div class="card">
<div class="title">Structuring GUI Elements through Vision Language Models: Towards Action   Space Generation</div>
<div class="meta-line">Authors: Yi Xu, Yesheng Zhang, jiajia Liu, Jingdong Chen</div>
<div class="meta-line">First: 2025-08-22T10:14:15+00:00 · Latest: 2025-08-22T10:14:15+00:00</div>
<div class="meta-line">Comments: 10pageV0</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16271v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16271v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have emerged as pivotal tools in
enhancing human-computer interaction. In this paper we focus on the application
of MLLMs in the field of graphical user interface (GUI) elements structuring,
where they assist in processing user instructions based on screen contents.
Despite the promise of MLLMs, their performance in precisely generating UI
element coordinates, a critical aspect of GUI understanding, is hindered by the
nature of next-token prediction training. This challenge arises from the
semantic void surrounding numerical UI coordinates in language representation
spaces, necessitating a substantial and diverse dataset to bolster visual
module capabilities. To address these limitations, we introduce an
IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our
approach involves a novel pipeline for IoU-based coordinate sampling to augment
the training data, which considers the proximity to ground truth coordinates.
This data augmentation strategy is then employed to fine-tune MLLMs under the
IAML paradigm, which is designed to mitigate the exposure bias problem inherent
in traditional maximum likelihood estimation. Through extensive experiments, we
demonstrate the superior performance of our IAML training approach over
traditional training paradigms.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have emerged as pivotal tools in enhancing human-computer interaction.</div>
</details>
</div>
<div class="card">
<div class="title">Top-Theta Attention: Sparsifying Transformers by Compensated   Thresholding</div>
<div class="meta-line">Authors: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli</div>
<div class="meta-line">First: 2025-02-12T12:50:15+00:00 · Latest: 2025-08-22T09:24:39+00:00</div>
<div class="meta-line">Comments: 11 pages, 11 figures + Appendix. work under submission</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.08363v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.08363v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Top-Theta (Top-$\theta$) Attention, a training-free method for
sparsifying transformer attention during inference. Our key insight is that
static, per-head thresholds can be calibrated to retain the desired constant
number of significant elements per attention row. This approach enables
content-based sparsity without retraining, and it remains robust across data
domains. We further introduce compensation techniques to preserve accuracy
under aggressive sparsification, establishing attention thresholding as a
practical and principled alternative to top-k attention. We provide extensive
evaluation on natural language processing tasks, showing that Top-$\theta$
achieves 3-10x reduction in V-cache usage and up to 10x fewer attention
elements during inference while degrading no more than 1% in accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Top-Theta (Top-$\theta$) Attention, a training-free method for sparsifying transformer attention during inference.</div>
</details>
</div>
<div class="card">
<div class="title">HPSv3: Towards Wide-Spectrum Human Preference Score</div>
<div class="meta-line">Authors: Yuhang Ma, Yunhao Shui, Xiaoshi Wu, Keqiang Sun, Hongsheng Li</div>
<div class="meta-line">First: 2025-08-05T17:17:13+00:00 · Latest: 2025-08-22T08:53:37+00:00</div>
<div class="meta-line">Comments: ICCV2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.03789v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.03789v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating text-to-image generation models requires alignment with human
perception, yet existing human-centric metrics are constrained by limited data
coverage, suboptimal feature extraction, and inefficient loss functions. To
address these challenges, we introduce Human Preference Score v3 (HPSv3). (1)
We release HPDv3, the first wide-spectrum human preference dataset integrating
1.08M text-image pairs and 1.17M annotated pairwise comparisons from
state-of-the-art generative models and low to high-quality real-world images.
(2) We introduce a VLM-based preference model trained using an
uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose
Chain-of-Human-Preference (CoHP), an iterative image refinement method that
enhances quality without extra data, using HPSv3 to select the best image at
each step. Extensive experiments demonstrate that HPSv3 serves as a robust
metric for wide-spectrum image evaluation, and CoHP offers an efficient and
human-aligned approach to improve image generation quality. The code and
dataset are available at the HPSv3 Homepage.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions.</div>
</details>
</div>
<div class="card">
<div class="title">OmniCache: A Trajectory-Oriented Global Perspective on Training-Free   Cache Reuse for Diffusion Transformer Models</div>
<div class="meta-line">Authors: Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-22T08:36:58+00:00 · Latest: 2025-08-22T08:36:58+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16212v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16212v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as a powerful paradigm for generative tasks
such as image synthesis and video generation, with Transformer architectures
further enhancing performance. However, the high computational cost of
diffusion Transformers-stemming from a large number of sampling steps and
complex per-step computations-presents significant challenges for real-time
deployment. In this paper, we introduce OmniCache, a training-free acceleration
method that exploits the global redundancy inherent in the denoising process.
Unlike existing methods that determine caching strategies based on inter-step
similarities and tend to prioritize reusing later sampling steps, our approach
originates from the sampling perspective of DIT models. We systematically
analyze the model&#x27;s sampling trajectories and strategically distribute cache
reuse across the entire sampling process. This global perspective enables more
effective utilization of cached computations throughout the diffusion
trajectory, rather than concentrating reuse within limited segments of the
sampling procedure.In addition, during cache reuse, we dynamically estimate the
corresponding noise and filter it out to reduce its impact on the sampling
direction.Extensive experiments demonstrate that our approach accelerates the
sampling process while maintaining competitive generative quality, offering a
promising and practical solution for efficient deployment of diffusion-based
generative models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance.</div>
</details>
</div>
<div class="card">
<div class="title">SpecVLM: Enhancing Speculative Decoding of Video LLMs via   Verifier-Guided Token Pruning</div>
<div class="meta-line">Authors: Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li</div>
<div class="meta-line">Venue: EMNLP 2025</div>
<div class="meta-line">First: 2025-08-22T08:23:09+00:00 · Latest: 2025-08-22T08:23:09+00:00</div>
<div class="meta-line">Comments: Accepted at EMNLP 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16201v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16201v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video large language models (Vid-LLMs) have shown strong capabilities in
understanding video content. However, their reliance on dense video token
representations introduces substantial memory and computational overhead in
both prefilling and decoding. To mitigate the information loss of recent video
token reduction methods and accelerate the decoding stage of Vid-LLMs
losslessly, we introduce SpecVLM, a training-free speculative decoding (SD)
framework tailored for Vid-LLMs that incorporates staged video token pruning.
Building on our novel finding that the draft model&#x27;s speculation exhibits low
sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,
enabling efficient speculation without sacrificing accuracy. To achieve this,
it performs a two-stage pruning process: Stage I selects highly informative
tokens guided by attention signals from the verifier (target model), while
Stage II prunes remaining redundant ones in a spatially uniform manner.
Extensive experiments on four video understanding benchmarks demonstrate the
effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$
decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for
Qwen2.5-VL-32B.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
