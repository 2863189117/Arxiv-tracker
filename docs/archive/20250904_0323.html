<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-09-04 03:23</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250904_0323</div>
    <div class="row"><div class="card">
<div class="title">NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</div>
<div class="meta-line">Authors: Max Gandyra, Alessandro Santonicola, Michael Beetz</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-07-02T08:23:14+00:00 · Latest: 2025-09-02T11:45:02+00:00</div>
<div class="meta-line">Comments: 10 pages, 3 figures, 5 tables, ICLR 2026 preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2507.01463v2">Abs</a> · <a href="http://arxiv.org/pdf/2507.01463v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed for all kinds of novel objects
without (re-) training has proven to be a difficult task. To handle this, we
present a new training-free framework, called: Novel Object Cyclic Threshold
based Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models:
Grounded-SAM 2 for object proposals with precise bounding boxes and
corresponding segmentation masks; and DINOv2 for robust class and patch
embeddings, due to its zero-shot capabilities. Internally, the proposal-object
matching is realized by determining an object matching score based on the
similarity of the class embeddings and the average maximum similarity of the
patch embeddings with a new cyclic thresholding (CT) mechanism that mitigates
unstable matches caused by repetitive textures or visually similar patterns.
Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by
object selection bias; (ii) the usage of the average confidence of the
proposals bounding box and mask as a scoring component; and (iii) an RGB-only
pipeline that performs even better than RGB-D ones. We empirically show that
NOCTIS, without further training/fine tuning, attains state-of-the-art results
regarding the mean AP score, w.r.t. the best RGB and RGB-D methods on the seven
core datasets of the BOP 2023 challenge for the &quot;Model-based 2D segmentation of
unseen objects&quot; task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NOCTIS：基于新颖对象周期阈值实例分割</div>
<div class="mono" style="margin-top:8px">给定每种对象的一些示例图像，在RGB图像中进行新颖对象实例分割是一个在计算机视觉中广为人知的问题。设计一个适用于所有类型新颖对象的通用模型，而无需重新训练，证明是一个困难的任务。为此，我们提出了一种新的无需训练框架，称为：基于新颖对象周期阈值实例分割（NOCTIS）。NOCTIS 结合了两个预训练模型：Grounded-SAM 2 用于对象提议，具有精确的边界框和相应的分割掩码；以及 DINOv2 用于鲁棒的类别和补丁嵌入，由于其零样本能力。内部，通过确定基于类别嵌入相似性和补丁嵌入平均最大相似性的对象匹配得分，结合新的周期阈值（CT）机制来实现提议对象匹配，该机制可以缓解由重复纹理或视觉相似模式引起的不稳定匹配。除了CT，NOCTIS 还引入了：（i）不受对象选择偏差影响的外观得分；（ii）使用提议边界框和掩码的平均置信度作为评分组件；（iii）仅使用RGB的管道，其性能甚至优于RGB-D管道。我们实验证明，NOCTIS 在BOP 2023 挑战赛七个核心数据集的“基于模型的未见对象2D分割”任务中，无需进一步训练/微调，即可达到最先进的平均AP得分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">NOCTIS is a training-free framework for instance segmentation of novel objects in RGB images. It combines Grounded-SAM 2 for precise object proposals and DINOv2 for robust class and patch embeddings. NOCTIS introduces a cyclic thresholding mechanism to improve matching stability and includes an appearance score and confidence scoring component. Empirically, NOCTIS achieves state-of-the-art mean AP scores on seven core BOP 2023 datasets without further training.</div>
<div class="mono" style="margin-top:8px">NOCTIS 是一个无需训练的框架，用于 RGB 图像中新型物体的实例分割。它结合了 Grounded-SAM 进行对象提案和 DINOv2 进行类别和补丁嵌入。NOCTIS 使用新颖的循环阈值机制来匹配提案与对象，并引入了外观得分和基于提案边界框和掩码的置信度得分。实验表明，NOCTIS 在七个 BOP 2023 数据集上实现了最先进的平均 AP 分数，无需进一步训练。</div>
</details>
</div>
<div class="card">
<div class="title">SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache   Channel Pruning</div>
<div class="meta-line">Authors: Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu</div>
<div class="meta-line">First: 2025-08-21T03:48:28+00:00 · Latest: 2025-09-02T11:29:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15212v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.15212v2">PDF</a> · <a href="https://github.com/Xnhyacinth/SparK">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-context inference in large language models (LLMs) is increasingly
constrained by the KV cache bottleneck: memory usage grows linearly with
sequence length, while attention computation scales quadratically. Existing
approaches address this issue by compressing the KV cache along the temporal
axis through strategies such as token eviction or merging to reduce memory and
computational overhead. However, these methods often neglect fine-grained
importance variations across feature dimensions (i.e., the channel axis),
thereby limiting their ability to effectively balance efficiency and model
accuracy. In reality, we observe that channel saliency varies dramatically
across both queries and positions: certain feature channels carry near-zero
information for a given query, while others spike in relevance. To address this
oversight, we propose SPARK, a training-free plug-and-play method that applies
unstructured sparsity by pruning KV at the channel level, while dynamically
restoring the pruned entries during attention score computation. Notably, our
approach is orthogonal to existing KV compression and quantization techniques,
making it compatible for integration with them to achieve further acceleration.
By reducing channel-level redundancy, SPARK enables processing of longer
sequences within the same memory budget. For sequences of equal length, SPARK
not only preserves or improves model accuracy but also reduces KV cache storage
by over 30% compared to eviction-based methods. Furthermore, even with an
aggressive pruning ratio of 80%, SPARK maintains performance with less
degradation than 5% compared to the baseline eviction method, demonstrating its
robustness and effectiveness. Our code will be available at
https://github.com/Xnhyacinth/SparK.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SparK：查询感知的无结构稀疏性与可恢复的KV缓存通道剪枝</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）中的长上下文推理越来越受到KV缓存瓶颈的限制：内存使用量随着序列长度线性增长，而注意力计算则呈二次增长。现有方法通过沿时间轴压缩KV缓存（例如通过令牌移除或合并）来解决这一问题，以减少内存和计算开销。然而，这些方法往往忽略了特征维度（即通道轴）上的细粒度重要性变化，从而限制了它们在效率和模型准确性之间取得有效平衡的能力。实际上，我们观察到通道显著性随查询和位置的变化而变化：某些特征通道在给定查询中几乎不携带任何信息，而其他通道则在相关性上激增。为了解决这一疏忽，我们提出了SPARK，这是一种无需训练的即插即用方法，通过在通道级别剪枝KV来应用无结构稀疏性，并在注意力分数计算期间动态恢复剪枝的条目。值得注意的是，我们的方法与现有的KV压缩和量化技术是正交的，使其可以与它们集成以实现进一步加速。通过减少通道级别的冗余，SPARK能够在相同的内存预算下处理更长的序列。对于等长的序列，SPARK不仅保持或提高了模型准确性，而且与基于移除的方法相比，KV缓存存储减少了超过30%。此外，即使采用80%的激进剪枝比例，SPARK的性能下降也比基线移除方法少5%，这证明了其稳健性和有效性。我们的代码将在https://github.com/Xnhyacinth/SparK/提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SPARK is a query-aware unstructured sparsity method that prunes the KV cache at the channel level while dynamically restoring pruned entries during attention score computation. This approach addresses the KV cache bottleneck in long-context inference of large language models by reducing channel-level redundancy without affecting model accuracy. SPARK reduces KV cache storage by over 30% compared to token eviction methods and maintains performance even with an 80% pruning ratio, showing its robustness and effectiveness.</div>
<div class="mono" style="margin-top:8px">SPARK 是一种查询感知的无结构稀疏性方法，它在通道级别剪枝 KV 缓存，并在注意力分数计算过程中动态恢复被剪枝的条目。该方法通过减少通道级别的冗余来解决大型语言模型长上下文推理中的 KV 缓存瓶颈，而不影响模型准确性。与基于令牌移除的方法相比，SPARK 可将 KV 缓存存储减少超过 30%，即使在 80% 的剪枝比率下也能保持性能，显示出其稳健性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework</div>
<div class="meta-line">Authors: Chao Wang, Chunbai Zhang, Yongxiao Tian, Yang Zhou, Yan Peng</div>
<div class="meta-line">First: 2025-02-02T07:54:55+00:00 · Latest: 2025-09-02T05:28:29+00:00</div>
<div class="meta-line">Comments: 14 pages,17 figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.00711v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.00711v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reasoning refers to the task of solving questions about visual
information. Current visual reasoning methods typically employ pre-trained
vision-language model (VLM) strategies or deep neural network approaches.
However, existing efforts are constrained by limited reasoning
interpretability, while hindering by the phenomenon of underspecification in
the question text. Additionally, the absence of fine-grained visual knowledge
limits the precise understanding of subject behavior in visual reasoning tasks.
To address these issues, we propose VIKSER (Visual Knowledge-Driven
Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using
knowledge distilled from large language models, extracts fine-grained visual
knowledge with the assistance of visual relationship detection techniques.
Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the
question with underspecification. Additionally, we design a novel prompting
method called Chain-of-Evidence (CoE), which leverages the power of &quot;evidence
for reasoning&quot; to endow VIKSER with interpretable reasoning capabilities.
Meanwhile, the integration of self-reflection technology empowers VIKSER with
the ability to learn and improve from its mistakes. Experiments conducted on
widely used datasets demonstrate that VIKSER achieves new state-of-the-art
(SOTA) results in relevant tasks. Moreover, VIKSER achieves performance on par
with leading proprietary models, such as the latest ChatGPT-5.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKSER：视觉知识驱动的自我强化推理框架</div>
<div class="mono" style="margin-top:8px">视觉推理是指解决关于视觉信息的问题的任务。当前的视觉推理方法通常采用预训练的视觉-语言模型（VLM）策略或深度神经网络方法。然而，现有的努力受到有限的推理可解释性的限制，同时受到问题文本中欠定现象的阻碍。此外，缺乏细粒度的视觉知识限制了在视觉推理任务中对主题行为的精确理解。为了解决这些问题，我们提出了VIKSER（视觉知识驱动的自我强化推理框架）。具体而言，VIKSER通过从大型语言模型中提取的知识进行训练，并借助视觉关系检测技术提取细粒度的视觉知识。随后，VIKSER利用细粒度的视觉知识对欠定的问题进行改写。同时，我们设计了一种名为证据链（CoE）的新型提示方法，利用“推理证据”的力量赋予VIKSER可解释的推理能力。此外，自我反思技术的集成使VIKSER能够从错误中学习和改进。在广泛使用的数据集上进行的实验表明，VIKSER在相关任务中取得了新的最佳结果。此外，VIKSER在性能上与领先的专有模型（如最新的ChatGPT-5）相当。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
