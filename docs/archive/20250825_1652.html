<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-08-25 16:52</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250825_1652</div>
    <div class="row"><div class="card">
<div class="title">SLGaussian: Fast Language Gaussian Splatting in Sparse Views</div>
<div class="meta-line">Authors: Kangjie Chen, BingQuan Dai, Minghan Qin, Dongbin Zhang, Peihao Li, Yingshuang Zou, Haoqian Wang</div>
<div class="meta-line">Venue: ACM MM 2025</div>
<div class="meta-line">First: 2024-12-11T12:18:30+00:00 · Latest: 2025-08-18T08:08:13+00:00</div>
<div class="meta-line">Comments: Accepted by ACM MM 2025. Project page:
  https://chenkangjie1123.github.io/SLGaussian.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2412.08331v3">Abs</a> · <a href="http://arxiv.org/pdf/2412.08331v3">PDF</a> · <a href="https://chenkangjie1123.github.io/SLGaussian.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D semantic field learning is crucial for applications like autonomous
navigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from
limited viewpoints is essential. Existing methods struggle under sparse view
conditions, relying on inefficient per-scene multi-view optimizations, which
are impractical for many real-world tasks. To address this, we propose
SLGaussian, a feed-forward method for constructing 3D semantic fields from
sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring
consistent SAM segmentations through video tracking and using low-dimensional
indexing for high-dimensional CLIP features, SLGaussian efficiently embeds
language information in 3D space, offering a robust solution for accurate 3D
scene understanding under sparse view conditions. In experiments on two-view
sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,
SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,
and mIoU. Moreover, our model achieves scene inference in under 30 seconds and
open-vocabulary querying in just 0.011 seconds per query.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SLGaussian: 快速语言高斯点云在稀疏视图中的3D语义场构建</div>
<div class="mono" style="margin-top:8px">3D语义场学习对于自主导航、AR/VR和机器人技术等应用至关重要，这些应用需要从有限视角准确理解3D场景。现有方法在稀疏视图条件下表现不佳，依赖于低效的多视图优化，这在许多实际任务中是不切实际的。为了解决这一问题，我们提出了SLGaussian，这是一种用于从稀疏视角构建3D语义场的前馈方法，允许直接推断基于3DGS的场景。通过视频跟踪确保一致的SAM分割，并使用低维索引嵌入高维CLIP特征，SLGaussian高效地在3D空间中嵌入语言信息，为在稀疏视图条件下提供准确的3D场景理解提供了一种稳健的解决方案。在LERF和3D-OVS数据集上的两项稀疏3D对象查询和分割实验中，SLGaussian在选择的IoU、定位准确性和mIoU方面优于现有方法。此外，我们的模型在场景推断中只需不到30秒，在开放词汇查询中每次查询只需0.011秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SLGaussian is a feed-forward method designed to construct 3D semantic fields from sparse viewpoints, enabling efficient and accurate 3D scene understanding. By leveraging consistent SAM segmentations through video tracking and low-dimensional indexing for high-dimensional CLIP features, SLGaussian embeds language information in 3D space. Experimental results show that SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy, and mIoU, and achieves scene inference in under 30 seconds and open-vocabulary querying in just 0.011 seconds per query.</div>
<div class="mono" style="margin-top:8px">SLGaussian 是一种前馈方法，用于从稀疏视角构建 3D 语义场，解决了现有方法依赖于低效的多视角优化的问题。通过利用视频跟踪实现一致的分割，并使用低维索引 CLIP 特征，SLGaussian 在 3D 空间中高效地嵌入了语言信息，相比现有方法在选定的 IoU、定位准确性和 mIoU 上表现更优。此外，它能够在不到 30 秒内进行场景推理，并且在每次查询中实现开放词汇查询只需 0.011 秒。</div>
</details>
</div>
<div class="card">
<div class="title">Splat Feature Solver</div>
<div class="meta-line">Authors: Butian Xiong, Rong Liu, Kenneth Xu, Meida Chen, Andrew Feng</div>
<div class="meta-line">First: 2025-08-17T03:13:06+00:00 · Latest: 2025-08-17T03:13:06+00:00</div>
<div class="meta-line">Comments: webpage not that stable</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.12216v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.12216v1">PDF</a> · <a href="https://github.com/saliteta/splat-distiller.git}{\textbf{github">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve 3D scene understanding by optimally assigning rich image feature descriptors to 3D primitives. The method formulates the feature lifting problem as a sparse linear inverse problem, providing a closed-form solution with a provable upper bound on the global optimal error. Key findings show that the approach outperforms existing baselines on open-vocabulary 3D segmentation benchmarks, achieving state-of-the-art performance and producing lifted features in minutes.</div>
<div class="mono" style="margin-top:8px">研究旨在通过将丰富的图像特征描述符最优地分配给3D原语来提升3D场景理解。方法将特征提升问题表述为稀疏线性逆问题，并引入正则化策略以解决不一致性和噪声问题。实验表明，该方法在3D分割基准测试中优于其他基线，并且能够快速生成提升的特征。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense   Perception</div>
<div class="meta-line">Authors: Junjie Wang, Keyu Chen, Yulin Li, Bin Chen, Hengshuang Zhao, Xiaojuan Qi, Zhuotao Tian</div>
<div class="meta-line">First: 2025-08-15T06:43:51+00:00 · Latest: 2025-08-15T06:43:51+00:00</div>
<div class="meta-line">Comments: arXiv admin note: text overlap with arXiv:2505.04410</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.11256v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.11256v1">PDF</a> · <a href="https://github.com/xiaomoguhz/DeCLIP">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP&#x27;s image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content&#x27;&#x27; and ``context&#x27;&#x27; features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用解耦学习增强开放词汇密集感知</div>
<div class="mono" style="margin-top:8px">密集视觉感知任务受限于其对预定义类别的依赖，限制了其在视觉概念未定义的现实场景中的应用。尽管像CLIP这样的视觉-语言模型（VLMs）在开放词汇任务中显示出潜力，但它们直接应用于密集感知时，由于局部特征表示的局限性，往往导致性能不佳。在本文中，我们观察到CLIP的图像令牌难以有效地从空间上或语义上相关的区域聚合信息，导致特征缺乏局部可区分性和空间一致性。为了解决这一问题，我们提出了一种名为DeCLIP的新框架，通过解耦自注意力模块来分别获得“内容”和“上下文”特征。上下文特征通过联合从视觉基础模型（VFMs）中蒸馏语义相关性以及从扩散模型中提取对象完整性线索来增强，从而增强空间一致性。同时，内容特征与图像剪辑表示对齐，并受到VFMs中区域相关性的约束，以提高局部可区分性。广泛的实验表明，DeCLIP为开放词汇密集感知奠定了坚实的基础，一致地在包括2D检测和分割、3D实例分割、视频实例分割和6D物体姿态估计在内的广泛任务中实现了最先进的性能。代码可在https://github.com/xiaomoguhz/DeCLIP获取</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of dense visual perception tasks by proposing DeCLIP, which enhances CLIP through a decoupled self-attention mechanism to improve local discriminability and spatial consistency. Extensive experiments show that DeCLIP outperforms existing methods across various tasks including 2D detection, segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.</div>
<div class="mono" style="margin-top:8px">该研究通过提出DeCLIP，增强CLIP的局部可区分性和空间一致性，解决密集视觉感知任务的限制。大量实验表明，DeCLIP在2D检测、分割、3D实例分割、视频实例分割和6D物体姿态估计等多种任务上均优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
