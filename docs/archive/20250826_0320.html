<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-08-26 03:20</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250826_0320</div>
    <div class="row"><div class="card">
<div class="title">Flow Matching-Based Generative Modeling for Efficient and Scalable Data   Assimilation</div>
<div class="meta-line">Authors: Taos Transue, Bohan Chen, So Takao, Bao Wang</div>
<div class="meta-line">First: 2025-08-18T19:00:45+00:00 · Latest: 2025-08-22T15:54:49+00:00</div>
<div class="meta-line">Comments: correcting authorship footnote, reformatting figures</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.13313v2">Abs</a> · <a href="http://arxiv.org/pdf/2508.13313v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data assimilation (DA) is the problem of sequentially estimating the state of
a dynamical system from noisy observations. Recent advances in generative
modeling have inspired new approaches to DA in high-dimensional nonlinear
settings, especially the ensemble score filter (EnSF). However, these come at a
significant computational burden due to slow sampling. In this paper, we
introduce a new filtering framework based on flow matching (FM) -- called the
ensemble flow filter (EnFF) -- to accelerate sampling and enable flexible
design of probability paths. EnFF -- a training-free DA approach -- integrates
MC estimators for the marginal FM vector field (VF) and a localized guidance to
assimilate observations. EnFF has faster sampling and more flexibility in VF
design compared to existing generative modeling for DA. Theoretically, we show
that EnFF encompasses classical filtering methods such as the bootstrap
particle filter and the ensemble Kalman filter as special cases. Experiments on
high-dimensional filtering benchmarks demonstrate improved cost-accuracy
tradeoffs and the ability to leverage larger ensembles than prior methods. Our
results highlight the promise of FM as a scalable tool for filtering in
high-dimensional applications that enable the use of large ensembles.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Data assimilation (DA) is the problem of sequentially estimating the state of a dynamical system from noisy observations.</div>
</details>
</div>
<div class="card">
<div class="title">Modular Embedding Recomposition for Incremental Learning</div>
<div class="meta-line">Authors: Aniello Panariello, Emanuele Frascaroli, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</div>
<div class="meta-line">First: 2025-08-22T15:25:40+00:00 · Latest: 2025-08-22T15:25:40+00:00</div>
<div class="meta-line">Comments: Accepted to the 36th British Machine Vision Conference (BMVC 2025),
  Sheffield, UK</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16463v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16463v1">PDF</a> · <a href="https://github.com/aimagelab/mammoth">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities.</div>
</details>
</div>
<div class="card">
<div class="title">CAMA: Enhancing Multimodal In-Context Learning with Context-Aware   Modulated Attention</div>
<div class="meta-line">Authors: Yanshu Li, Jianjiang Yang, Ziteng Yang, Bozheng Li, Hongyang He, Zhengtao Yao, Ligong Han, Yingjie Victor Chen, Songlin Fei, Dongfang Liu, Ruixiang Tang</div>
<div class="meta-line">First: 2025-05-21T04:25:23+00:00 · Latest: 2025-08-22T14:44:22+00:00</div>
<div class="meta-line">Comments: 14 pages, 8 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.17097v2">Abs</a> · <a href="http://arxiv.org/pdf/2505.17097v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal in-context learning (ICL) is emerging as a key capability that
enables large vision-language models (LVLMs) to adapt to novel tasks without
parameter updates, expanding their utility across various real-world
applications. However, ICL remains unstable, even with well-matched in-context
demonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the
provided context. While existing efforts focus on prompt engineering or
post-hoc logit calibration, we instead investigate the underlying attention
dynamics to overcome LVLMs&#x27; inherent limitations. We identify two critical
deficits in their self-attention that impair effective ICL. To bridge the gap,
we propose \textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play
and training-free method that dynamically modulates LVLM&#x27;s attention logits
based on the input in-context sequence. CAMA employs a two-stage attention
modulation to address both identified deficits, enhancing the focus on
semantically significant tokens, particularly visual ones. Across four LVLMs
and seven benchmarks, CAMA consistently outperforms vanilla models and
baselines, demonstrating great effectiveness and generalization. It can also
activate the desired effects of prompt engineering methods and remains robust
under diverse sequence configurations. Thus, CAMA paves the way for deeper
explorations of attention dynamics to advance multimodal reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal in-context learning (ICL) is emerging as a key capability that enables large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, expanding their utility across various real-world applications.</div>
</details>
</div>
<div class="card">
<div class="title">Structuring GUI Elements through Vision Language Models: Towards Action   Space Generation</div>
<div class="meta-line">Authors: Yi Xu, Yesheng Zhang, jiajia Liu, Jingdong Chen</div>
<div class="meta-line">First: 2025-08-22T10:14:15+00:00 · Latest: 2025-08-22T10:14:15+00:00</div>
<div class="meta-line">Comments: 10pageV0</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.16271v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.16271v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have emerged as pivotal tools in
enhancing human-computer interaction. In this paper we focus on the application
of MLLMs in the field of graphical user interface (GUI) elements structuring,
where they assist in processing user instructions based on screen contents.
Despite the promise of MLLMs, their performance in precisely generating UI
element coordinates, a critical aspect of GUI understanding, is hindered by the
nature of next-token prediction training. This challenge arises from the
semantic void surrounding numerical UI coordinates in language representation
spaces, necessitating a substantial and diverse dataset to bolster visual
module capabilities. To address these limitations, we introduce an
IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our
approach involves a novel pipeline for IoU-based coordinate sampling to augment
the training data, which considers the proximity to ground truth coordinates.
This data augmentation strategy is then employed to fine-tune MLLMs under the
IAML paradigm, which is designed to mitigate the exposure bias problem inherent
in traditional maximum likelihood estimation. Through extensive experiments, we
demonstrate the superior performance of our IAML training approach over
traditional training paradigms.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have emerged as pivotal tools in enhancing human-computer interaction.</div>
</details>
</div>
<div class="card">
<div class="title">Top-Theta Attention: Sparsifying Transformers by Compensated   Thresholding</div>
<div class="meta-line">Authors: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli</div>
<div class="meta-line">First: 2025-02-12T12:50:15+00:00 · Latest: 2025-08-22T09:24:39+00:00</div>
<div class="meta-line">Comments: 11 pages, 11 figures + Appendix. work under submission</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2502.08363v2">Abs</a> · <a href="http://arxiv.org/pdf/2502.08363v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Top-Theta (Top-$\theta$) Attention, a training-free method for
sparsifying transformer attention during inference. Our key insight is that
static, per-head thresholds can be calibrated to retain the desired constant
number of significant elements per attention row. This approach enables
content-based sparsity without retraining, and it remains robust across data
domains. We further introduce compensation techniques to preserve accuracy
under aggressive sparsification, establishing attention thresholding as a
practical and principled alternative to top-k attention. We provide extensive
evaluation on natural language processing tasks, showing that Top-$\theta$
achieves 3-10x reduction in V-cache usage and up to 10x fewer attention
elements during inference while degrading no more than 1% in accuracy.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Top-Theta (Top-$\theta$) Attention, a training-free method for sparsifying transformer attention during inference.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
