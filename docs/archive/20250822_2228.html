<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-08-22 22:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20250822_2228</div>
    <div class="row"><div class="card">
<div class="title">Scaling Group Inference for Diverse and High-Quality Generation</div>
<div class="meta-line">Authors: Gaurav Parmar, Or Patashnik, Daniil Ostashev, Kuan-Chieh Wang, Kfir Aberman, Srinivasa Narasimhan, Jun-Yan Zhu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2025-08-21T17:59:57+00:00 · Latest: 2025-08-21T17:59:57+00:00</div>
<div class="meta-line">Comments: Project website: https://www.cs.cmu.edu/~group-inference, GitHub:
  https://github.com/GaParmar/group-inference</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15773v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15773v1">PDF</a> · <a href="https://github.com/GaParmar/group-inference">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>规模化群体推理：实现多样化与高质量生成</div>
<div class="mono" style="margin-top:8px">生成模型通常独立采样输出，而近期的推理时引导与扩展算法主要关注提升单一样本质量。然而在实际应用中，用户常需针对每个提示获取一组多图像（如4-8张），独立采样易导致结果冗余，限制用户选择并阻碍创意探索。本研究提出可扩展的群体推理方法，同步提升样本组的多样性与质量。我们将群体推理构建为二次整数分配问题：候选输出建模为图节点，通过选择子集优化样本质量（一元项）同时最大化群体多样性（二元项）。为显著提升运行效率，采用中间预测逐步剪枝候选集，使方法能扩展至大规模候选集。大量实验表明，相比独立采样基线及近期推理算法，本方法显著提升群体多样性与质量。该框架可泛化至文本生成图像、图像到图像转换、图像提示及视频生成等广泛任务，使生成模型将多输出视为有机整体而非独立样本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation is to address the redundancy in independently sampled outputs from generative models, which limits user choice and exploration when presented with multiple results per prompt. The method formulates group inference as a quadratic integer assignment problem, selecting a subset of candidate outputs to optimize both quality and diversity, and uses progressive pruning for scalability. Experimental results show significant improvements in group diversity and quality across text-to-image, image-to-image, image prompting, and video generation tasks compared to baselines.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决生成模型独立采样输出导致的冗余问题，这限制了用户在每次提示下获得多个结果时的选择和创意探索。方法引入了一种可扩展的群体推理方法，将选择问题建模为二次整数分配问题，通过将候选输出视为图节点并逐步剪枝以提高效率，同时优化质量和多样性。实验结果表明，该方法在文本到图像、图像到图像、图像提示和视频生成等多种任务中显著提高了群体的多样性和质量，使模型能够生成更具凝聚力的输出组。</div>
</details>
</div>
<div class="card">
<div class="title">CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</div>
<div class="meta-line">Authors: Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-21T17:59:57+00:00 · Latest: 2025-08-21T17:59:57+00:00</div>
<div class="meta-line">Comments: CineScale is an extended work of FreeScale (ICCV 2025). Project Page:
  https://eyeline-labs.github.io/CineScale/, Code Repo:
  https://github.com/Eyeline-Labs/CineScale</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15774v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15774v1">PDF</a> · <a href="https://github.com/Eyeline-Labs/CineScale">Code1</a> · <a href="https://eyeline-labs.github.io/CineScale/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. In this work, we propose CineScale, a novel inference
paradigm to enable higher-resolution visual generation. To tackle the various
issues introduced by the two types of video generation architectures, we
propose dedicated variants tailored to each. Unlike existing baseline methods
that are confined to high-resolution T2I and T2V generation, CineScale broadens
the scope by enabling high-resolution I2V and V2V synthesis, built atop
state-of-the-art open-source video generation frameworks. Extensive experiments
validate the superiority of our paradigm in extending the capabilities of
higher-resolution visual generation for both image and video models.
Remarkably, our approach enables 8k image generation without any fine-tuning,
and achieves 4k video generation with only minimal LoRA fine-tuning. Generated
video samples are available at our website:
https://eyeline-labs.github.io/CineScale/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CineScale：高分辨率电影级视觉生成中的免费午餐</div>
<div class="mono" style="margin-top:8px">视觉扩散模型取得显著进展，但由于缺乏高分辨率数据和有限的计算资源，通常只能在受限分辨率下训练，这限制了其生成高保真高分辨率图像或视频的能力。近期研究探索了无需调参的策略以释放预训练模型在高分辨率视觉生成方面的潜力，但这些方法仍易产生带有重复模式的低质量视觉内容。关键障碍在于当模型生成超出训练分辨率的视觉内容时，高频信息不可避免地增加，导致误差累积产生不良重复模式。本研究提出CineScale——一种实现更高分辨率视觉生成的新型推理范式。针对两类视频生成架构的不同问题，我们设计了专用变体。与现有局限于高分辨率文生图（T2I）和文生视频（T2V）的基线方法不同，CineScale基于顶尖开源视频生成框架，进一步实现了高分辨率图生视频（I2V）和视频生视频（V2V）合成。大量实验验证了我们的范式在扩展图像与视频模型高分辨率生成能力方面的优越性。值得注意的是，该方法无需微调即可实现8K图像生成，并通过极少量LoRA微调实现4K视频生成。生成视频样本请访问：https://eyeline-labs.github.io/CineScale/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind CineScale is to address the limitations of visual diffusion models, which are typically trained at low resolutions and struggle to generate high-fidelity, high-resolution content without repetitive artifacts due to accumulated high-frequency errors. The method introduces a novel inference paradigm with tailored variants for different video generation architectures, enabling higher-resolution text-to-video, image-to-video, and video-to-video synthesis without fine-tuning. Experimental results demonstrate that CineScale achieves superior performance, enabling 8k image generation without fine-tuning and 4k video generation with minimal LoRA fine-tuning, significantly expanding the resolution capabilities of existing models.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决使用预训练扩散模型生成高分辨率电影级视觉内容而不需额外训练的挑战，现有免调优方法在高分辨率下因高频信息增加易产生重复模式。作者提出了CineScale，一种新颖的推理范式，为不同视频生成架构定制变体，支持高分辨率文本到视频、图像到视频及视频到视频合成。实验结果表明该方法性能优越，实现了无需微调的8k图像生成和仅需最小LoRA微调的4k视频生成，显著扩展了先进视频生成框架的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Autoregressive Modeling for Instruction-Guided Image Editing</div>
<div class="meta-line">Authors: Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</div>
<div class="meta-line">First: 2025-08-21T17:59:32+00:00 · Latest: 2025-08-21T17:59:32+00:00</div>
<div class="meta-line">Comments: Source codes and models are available at
  https://github.com/HiDream-ai/VAREdit</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15772v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15772v1">PDF</a> · <a href="https://github.com/HiDream-ai/VAREdit">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion models have brought remarkable visual fidelity
to instruction-guided image editing. However, their global denoising process
inherently entangles the edited region with the entire image context, leading
to unintended spurious modifications and compromised adherence to editing
instructions. In contrast, autoregressive models offer a distinct paradigm by
formulating image synthesis as a sequential process over discrete visual
tokens. Their causal and compositional mechanism naturally circumvents the
adherence challenges of diffusion-based methods. In this paper, we present
VAREdit, a visual autoregressive (VAR) framework that reframes image editing as
a next-scale prediction problem. Conditioned on source image features and text
instructions, VAREdit generates multi-scale target features to achieve precise
edits. A core challenge in this paradigm is how to effectively condition the
source image tokens. We observe that finest-scale source features cannot
effectively guide the prediction of coarser target features. To bridge this
gap, we introduce a Scale-Aligned Reference (SAR) module, which injects
scale-matched conditioning information into the first self-attention layer.
VAREdit demonstrates significant advancements in both editing adherence and
efficiency. On standard benchmarks, it outperforms leading diffusion-based
methods by 30\%+ higher GPT-Balance score. Moreover, it completes a
$512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the
similarly sized UltraEdit. The models are available at
https://github.com/HiDream-ai/VAREdit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>指令引导图像编辑的视觉自回归建模</div>
<div class="mono" style="margin-top:8px">扩散模型的最新进展为指令引导图像编辑带来了卓越的视觉保真度。然而，其全局去噪过程本质上将编辑区域与整个图像上下文纠缠在一起，导致意外的伪修改并削弱对编辑指令的遵循。相比之下，自回归模型通过将图像合成构建为离散视觉标记的序列过程，提供了独特范式。其因果组合机制天然规避了基于扩散方法的遵循难题。本文提出VAREdit——一种将图像编辑重构为下一尺度预测问题的视觉自回归（VAR）框架。通过源图像特征和文本指令的条件化，VAREdit生成多尺度目标特征以实现精确编辑。该范式的核心挑战在于如何有效条件化源图像标记。我们发现最精细尺度的源特征无法有效指导较粗目标特征的预测。为弥合此差距，我们引入了尺度对齐参考（SAR）模块，将尺度匹配的条件信息注入首个自注意力层。VAREdit在编辑遵循度和效率方面均取得显著进展，在标准基准测试中，其GPT平衡分数比领先的扩散方法高出30%以上，且完成512×512编辑仅需1.2秒，比同等规模的UltraEdit快2.2倍。模型详见https://github.com/HiDream-ai/VAREdit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of diffusion models in instruction-guided image editing, which often cause unintended changes due to their global denoising process, this paper introduces VAREdit, a visual autoregressive framework that treats editing as a next-scale prediction task. The method conditions on source image features and text instructions to generate multi-scale target features, addressing conditioning challenges with a Scale-Aligned Reference module that aligns source and target scales in self-attention. Experimental results show VAREdit outperforms leading diffusion methods by over 30% in GPT-Balance score and achieves 1.2-second editing for 512x512 images, making it 2.2x faster than comparable models.</div>
<div class="mono" style="margin-top:8px">针对扩散模型在指令引导图像编辑中因全局去噪过程导致意外修改和指令遵循性差的问题，本文提出了VAREdit，一种将编辑视为下一尺度预测任务的视觉自回归框架。该方法基于源图像特征和文本指令生成多尺度目标特征，并通过尺度对齐参考模块在自注意力层中实现尺度匹配的条件注入。实验结果表明，VAREdit在标准基准测试中比领先的扩散方法GPT-Balance分数高30%以上，并在1.2秒内完成512x512编辑，速度提升2.2倍，展现出更高的精确性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</div>
<div class="meta-line">Authors: Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie</div>
<div class="meta-line">First: 2025-08-21T17:59:16+00:00 · Latest: 2025-08-21T17:59:16+00:00</div>
<div class="meta-line">Comments: Technical Report; Project Page: https://mengmouxu.github.io/SceneGen</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15769v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15769v1">PDF</a> · <a href="https://mengmouxu.github.io/SceneGen">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D content generation has recently attracted significant research interest
due to its applications in VR/AR and embodied AI. In this work, we address the
challenging task of synthesizing multiple 3D assets within a single scene
image. Concretely, our contributions are fourfold: (i) we present SceneGen, a
novel framework that takes a scene image and corresponding object masks as
input, simultaneously producing multiple 3D assets with geometry and texture.
Notably, SceneGen operates with no need for optimization or asset retrieval;
(ii) we introduce a novel feature aggregation module that integrates local and
global scene information from visual and geometric encoders within the feature
extraction module. Coupled with a position head, this enables the generation of
3D assets and their relative spatial positions in a single feedforward pass;
(iii) we demonstrate SceneGen&#x27;s direct extensibility to multi-image input
scenarios. Despite being trained solely on single-image inputs, our
architectural design enables improved generation performance with multi-image
inputs; and (iv) extensive quantitative and qualitative evaluations confirm the
efficiency and robust generation abilities of our approach. We believe this
paradigm offers a novel solution for high-quality 3D content generation,
potentially advancing its practical applications in downstream tasks. The code
and model will be publicly available at: https://mengmouxu.github.io/SceneGen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SceneGen：单图像三维场景单次前馈生成</div>
<div class="mono" style="margin-top:8px">三维内容生成因其在VR/AR和具身智能中的应用近期引发广泛研究关注。本研究致力于解决从单张场景图像合成多个三维资产的挑战性任务。具体贡献包括：(i)提出SceneGen新型框架，以场景图像及对应物体掩码为输入，同步生成带几何与纹理的多项三维资产，且无需优化过程或资产检索；(ii)设计新型特征聚合模块，在特征提取阶段整合视觉与几何编码器的局部与全局场景信息，结合位置预测头实现单次前馈生成三维资产及其相对空间位置；(iii)展示框架对多图像输入的直接扩展性——尽管仅使用单图像训练，架构设计支持多图像输入提升生成质量；(iv)通过大量定量与定性实验验证方法的高效性与强健生成能力。该范式为高质量三维内容生成提供新解决方案，有望推动下游任务的实际应用。代码与模型将公开于：https://mengmouxu.github.io/SceneGen</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing demand for 3D content in VR/AR and embodied AI, this paper introduces SceneGen, a framework that generates multiple 3D assets with geometry and texture from a single scene image and object masks in one feedforward pass, eliminating the need for optimization or retrieval. The method employs a feature aggregation module that combines local and global scene information from visual and geometric encoders, along with a position head to determine spatial relationships. Experimental results demonstrate its efficiency, robust generation quality, and extensibility to multi-image inputs despite single-image training, showing strong performance in both quantitative and qualitative evaluations.</div>
<div class="mono" style="margin-top:8px">本文针对VR/AR和具身AI中对3D内容日益增长的需求，提出了SceneGen框架，能够从单张场景图像和物体掩码中一次性前向生成多个带有几何和纹理的3D资产，无需优化或检索。该方法采用特征聚合模块整合视觉和几何编码器的局部与全局场景信息，并通过位置头确定空间关系。实验结果表明，该方法高效、生成质量稳健，且尽管仅使用单图像训练，仍可扩展至多图像输入，展现了在下游任务中的实际应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ATLAS: Decoupling Skeletal and Shape Parameters for Expressive   Parametric Human Modeling</div>
<div class="meta-line">Authors: Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2025-08-21T17:58:56+00:00 · Latest: 2025-08-21T17:58:56+00:00</div>
<div class="meta-line">Comments: ICCV 2025; Website: https://jindapark.github.io/projects/atlas/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15767v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15767v1">PDF</a> · <a href="https://jindapark.github.io/projects/atlas/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parametric body models offer expressive 3D representation of humans across a
wide range of poses, shapes, and facial expressions, typically derived by
learning a basis over registered 3D meshes. However, existing human mesh
modeling approaches struggle to capture detailed variations across diverse body
poses and shapes, largely due to limited training data diversity and
restrictive modeling assumptions. Moreover, the common paradigm first optimizes
the external body surface using a linear basis, then regresses internal
skeletal joints from surface vertices. This approach introduces problematic
dependencies between internal skeleton and outer soft tissue, limiting direct
control over body height and bone lengths. To address these issues, we present
ATLAS, a high-fidelity body model learned from 600k high-resolution scans
captured using 240 synchronized cameras. Unlike previous methods, we explicitly
decouple the shape and skeleton bases by grounding our mesh representation in
the human skeleton. This decoupling enables enhanced shape expressivity,
fine-grained customization of body attributes, and keypoint fitting independent
of external soft-tissue characteristics. ATLAS outperforms existing methods by
fitting unseen subjects in diverse poses more accurately, and quantitative
evaluations show that our non-linear pose correctives more effectively capture
complex poses compared to linear models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ATLAS：解耦骨骼与形态参数以实现富有表现力的参数化人体建模</div>
<div class="mono" style="margin-top:8px">参数化人体模型通过基于配准三维网格学习基向量，提供了跨姿态、体型和面部表情的丰富三维人体表示。然而，现有方法因训练数据多样性不足和建模假设限制，难以捕捉多样体态下的细节变化。传统范式先通过线性基优化体表，再从表面顶点回归内部骨骼关节点，导致骨骼与软组织间存在不良依赖，限制了对身高和骨长的直接控制。为此，我们提出ATLAS——一个基于240台同步相机采集的60万次高分辨率扫描构建的高保真人体模型。该方法通过将网格表示锚定在人体骨骼上，显式解耦形态与骨骼基向量，从而增强形态表现力、实现细粒度身体属性定制，并支持独立于软组织特征的关键点拟合。定量评估表明，ATLAS能更精准地拟合未知对象的多姿态数据，其非线性姿态校正比线性模型更能有效捕捉复杂姿态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of existing parametric human models in capturing detailed variations across poses and shapes due to data constraints and modeling assumptions, ATLAS introduces a novel approach that explicitly decouples shape and skeletal parameters by grounding the mesh representation in the human skeleton. This method, trained on 600k high-resolution scans, enhances shape expressivity and allows fine-grained customization and keypoint fitting independent of soft tissue. Experimental results demonstrate that ATLAS outperforms prior methods in fitting unseen subjects across diverse poses, with quantitative evaluations confirming its non-linear pose correctives better capture complex poses compared to linear models.</div>
<div class="mono" style="margin-top:8px">ATLAS的动机是解决现有参数化人体模型在姿势和形状细节变化上的局限性，这些限制源于数据多样性和建模假设，并导致骨骼与表面参数纠缠。该方法利用240台相机捕获的60万高分辨率扫描数据，通过将网格基于人体骨骼显式解耦形状和骨骼基，实现独立控制和增强的表现力。实验结果表明，ATLAS在多样姿势下对未见主体的拟合更准确，非线性姿势校正比线性模型更有效地捕捉复杂姿势。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Hidden Algebraic Structures via Transformers with Rank-Aware   Beam GRPO</div>
<div class="meta-line">Authors: Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU</div>
<div class="meta-line">First: 2025-08-21T17:58:50+00:00 · Latest: 2025-08-21T17:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15766v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15766v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent efforts have extended the capabilities of transformers in logical
reasoning and symbolic computations. In this work, we investigate their
capacity for non-linear latent pattern discovery in the context of functional
decomposition, focusing on the challenging algebraic task of multivariate
polynomial decomposition. This problem, with widespread applications in science
and engineering, is proved to be NP-hard, and demands both precision and
insight. Our contributions are threefold: First, we develop a synthetic data
generation pipeline providing fine-grained control over problem complexity.
Second, we train transformer models via supervised learning and evaluate them
across four key dimensions involving scaling behavior and generalizability.
Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a
rank-aware reinforcement learning method suitable for hard algebraic problems.
Finetuning with BGRPO improves accuracy while reducing beam width by up to
half, resulting in approximately 75% lower inference compute. Additionally, our
model demonstrates competitive performance in polynomial simplification,
outperforming Mathematica in various cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用具有秩感知束GRPO的Transformer发现隐藏代数结构</div>
<div class="mono" style="margin-top:8px">近期研究扩展了Transformer在逻辑推理和符号计算方面的能力。本文探讨了其在函数分解背景下发现非线性潜在模式的能力，重点关注多元多项式分解这一具有挑战性的代数任务。该问题在科学与工程领域应用广泛，已被证明是NP难问题，需要精确性与洞察力。我们的贡献有三：首先开发了能精细控制问题复杂度的合成数据生成流程；其次通过监督学习训练Transformer模型，并在涉及扩展行为和泛化能力的四个关键维度进行评估；第三提出了束分组相对策略优化（BGRPO），这是一种适用于困难代数问题的秩感知强化学习方法。使用BGRPO进行微调可在将束宽减半的同时提升准确率，推理计算量降低约75%。此外，我们的模型在多项式简化任务中展现出竞争优势，在多类案例中超越Mathematica。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to enhance transformers&#x27; ability to discover hidden algebraic structures, specifically targeting the NP-hard problem of multivariate polynomial decomposition, which has broad applications in science and engineering. The authors develop a synthetic data generation pipeline for controlled complexity, train transformers via supervised learning, and introduce Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method tailored for hard algebraic tasks. Experimental results show that finetuning with BGRPO improves accuracy while reducing beam width by up to half, cutting inference compute by approximately 75%, and the model outperforms Mathematica in polynomial simplification in various cases.</div>
<div class="mono" style="margin-top:8px">本文旨在提升Transformer在复杂代数任务中发现非线性潜在模式的能力，专注于解决NP难的多变量多项式分解问题。方法包括生成复杂度可控的合成数据，通过监督学习训练Transformer模型，并提出了适用于硬代数问题的秩感知强化学习方法Beam Grouped Relative Policy Optimization (BGRPO)。实验结果表明，BGRPO微调在提高精度的同时将波束宽度减少多达一半，推理计算量降低约75%，且在多项式简化任务中表现优于Mathematica。</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Detection of Adversarial Attacks in Multi-Agent   Reinforcement Learning with Continuous Action Space</div>
<div class="meta-line">Authors: Kiarash Kazari, Ezzeldin Shereen, György Dán</div>
<div class="meta-line">First: 2025-08-21T17:58:36+00:00 · Latest: 2025-08-21T17:58:36+00:00</div>
<div class="meta-line">Comments: Accepted for publication at ECAI 2025</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15764v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15764v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address the problem of detecting adversarial attacks against cooperative
multi-agent reinforcement learning with continuous action space. We propose a
decentralized detector that relies solely on the local observations of the
agents and makes use of a statistical characterization of the normal behavior
of observable agents. The proposed detector utilizes deep neural networks to
approximate the normal behavior of agents as parametric multivariate Gaussian
distributions. Based on the predicted density functions, we define a normality
score and provide a characterization of its mean and variance. This
characterization allows us to employ a two-sided CUSUM procedure for detecting
deviations of the normality score from its mean, serving as a detector of
anomalous behavior in real-time. We evaluate our scheme on various multi-agent
PettingZoo benchmarks against different state-of-the-art attack methods, and
our results demonstrate the effectiveness of our method in detecting impactful
adversarial attacks. Particularly, it outperforms the discrete counterpart by
achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all
evaluated environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续动作空间多智能体强化学习中对抗攻击的分布式检测</div>
<div class="mono" style="margin-top:8px">本文研究连续动作空间下协作型多智能体强化学习系统遭受对抗攻击的检测问题。提出一种仅依赖智能体局部观测的分布式检测器，通过统计建模智能体正常行为特征，利用深度神经网络将智能体行为拟合为参数化多元高斯分布。基于预测密度函数定义正态性评分并解析其均值与方差特性，进而采用双端CUSUM算法实时检测评分偏离均值的异常行为。在多种PettingZoo多智能体测试环境中对比前沿攻击方法的实验表明，本方法能有效检测高影响力对抗攻击，尤其在所有测试场景中对最强攻击的AUC-ROC分数均超过0.95，显著优于离散动作空间的对应方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need to detect adversarial attacks in cooperative multi-agent reinforcement learning with continuous action spaces, motivated by the vulnerability of such systems to malicious interference. The method involves a decentralized detector that uses deep neural networks to model each agent&#x27;s normal behavior as multivariate Gaussian distributions, then applies a two-sided CUSUM procedure on a derived normality score to identify deviations in real-time. Experimental results on PettingZoo benchmarks show high effectiveness, with AUC-ROC scores exceeding 0.95 against impactful attacks, outperforming discrete action space detectors.</div>
<div class="mono" style="margin-top:8px">本文旨在解决连续动作空间下协作多智能体强化学习中的对抗攻击检测问题，动机是实现分散式和实时的安全防护。方法利用深度神经网络将智能体的正常行为建模为参数化多元高斯分布，定义正态性评分并采用双侧CUSUM程序检测异常行为。在多个PettingZoo基准测试中，实验结果表明该方法高效，针对最具影响力的攻击实现了超过0.95的AUC-ROC分数，优于离散空间对应方法。</div>
</details>
</div>
<div class="card">
<div class="title">Intern-S1: A Scientific Multimodal Foundation Model</div>
<div class="meta-line">Authors: Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou</div>
<div class="meta-line">First: 2025-08-21T17:58:00+00:00 · Latest: 2025-08-21T17:58:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15763v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15763v1">PDF</a> · <a href="https://huggingface.co/internlm/Intern-S1">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a plethora of open-source foundation models have emerged,
achieving remarkable progress in some widely attended fields, with performance
being quite close to that of closed-source models. However, in high-value but
more challenging scientific professional fields, either the fields still rely
on expert models, or the progress of general foundation models lags
significantly compared to those in popular areas, far from sufficient for
transforming scientific research and leaving substantial gap between
open-source models and closed-source models in these scientific domains. To
mitigate this gap and explore a step further toward Artificial General
Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped
with general understanding and reasoning capabilities with expertise to analyze
multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)
model with 28 billion activated parameters and 241 billion total parameters,
continually pre-trained on 5T tokens, including over 2.5T tokens from
scientific domains. In the post-training stage, Intern-S1 undergoes offline and
then online reinforcement learning (RL) in InternBootCamp, where we propose
Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks
simultaneously. Through integrated innovations in algorithms, data, and
training systems, Intern-S1 achieved top-tier performance in online RL
training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates
competitive performance on general reasoning tasks among open-source models and
significantly outperforms open-source models in scientific domains, surpassing
closed-source state-of-the-art models in professional tasks, such as molecular
synthesis planning, reaction condition prediction, predicting thermodynamic
stabilities for crystals. Our models are available at
https://huggingface.co/internlm/Intern-S1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Intern-S1：科学多模态基础模型</div>
<div class="mono" style="margin-top:8px">近年来，开源基础模型大量涌现，在部分广受关注的领域取得显著进展，性能已十分接近闭源模型。然而，在高价值但更具挑战性的科学专业领域，这些领域仍依赖专家模型，或通用基础模型的进展远滞后于热门领域，远不足以变革科学研究，且开源模型与闭源模型在这些科学领域存在巨大差距。为缩小这一差距并探索迈向通用人工智能（AGI）的下一步，我们推出Intern-S1——一个具备通用理解与推理能力，并能分析多科学模态数据的专业通才模型。Intern-S1是多模态混合专家（MoE）模型，拥有280亿激活参数和2410亿总参数，基于5T token（其中包含超过2.5T科学领域token）持续预训练。在后训练阶段，该模型通过InternBootCamp先后进行离线和在线强化学习（RL），我们提出混合奖励机制（MoR）以协同千余项任务的RL训练。通过算法、数据和训练系统的集成创新，Intern-S1在在线RL训练中达到顶尖性能。在综合评估基准测试中，Intern-S1在开源模型中展现出通用推理任务的竞争优势，并在科学领域显著超越开源模型，在分子合成规划、反应条件预测、晶体热力学稳定性预测等专业任务中超越闭源最先进模型。模型详见：https://huggingface.co/internlm/Intern-S1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the significant performance gap between open-source and closed-source models in scientific domains, this work introduces Intern-S1, a multimodal Mixture-of-Experts foundation model with 28B activated parameters, continually pre-trained on 5T tokens including substantial scientific data and fine-tuned using reinforcement learning with a Mixture-of-Rewards approach. The method integrates algorithmic, data, and system innovations to enable general reasoning and specialized scientific analysis. Experimental results show that Intern-S1 achieves competitive performance on general reasoning tasks among open-source models and significantly outperforms them in scientific domains, even surpassing state-of-the-art closed-source models in specialized tasks like molecular synthesis planning and crystal stability prediction.</div>
<div class="mono" style="margin-top:8px">Intern-S1的动机是解决开源与闭源基础模型在科学领域存在的性能差距问题，因为通用模型在这些高价值专业领域进展滞后，阻碍了科学研究转型和AGI发展。方法上，它构建了一个280亿激活参数的多模态混合专家模型，基于5T token（其中超2.5T来自科学数据）持续预训练，并通过离线和在线强化学习及新颖的混合奖励机制在1000多项任务上协同优化。实验结果表明，Intern-S1在在线强化学习中表现顶尖，在通用推理任务上媲美开源模型，在科学领域显著优于开源模型，甚至在分子合成规划、晶体稳定性预测等专业任务上超越了闭源最先进模型。</div>
</details>
</div>
<div class="card">
<div class="title">Waver: Wave Your Way to Lifelike Video Generation</div>
<div class="meta-line">Authors: Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng</div>
<div class="meta-line">First: 2025-08-21T17:56:10+00:00 · Latest: 2025-08-21T17:56:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15761v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15761v1">PDF</a> · <a href="https://github.com/FoundationVision/Waver">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Waver, a high-performance foundation model for unified image and
video generation. Waver can directly generate videos with durations ranging
from 5 to 10 seconds at a native resolution of 720p, which are subsequently
upscaled to 1080p. The model simultaneously supports text-to-video (T2V),
image-to-video (I2V), and text-to-image (T2I) generation within a single,
integrated framework. We introduce a Hybrid Stream DiT architecture to enhance
modality alignment and accelerate training convergence. To ensure training data
quality, we establish a comprehensive data curation pipeline and manually
annotate and train an MLLM-based video quality model to filter for the
highest-quality samples. Furthermore, we provide detailed training and
inference recipes to facilitate the generation of high-quality videos. Building
on these contributions, Waver excels at capturing complex motion, achieving
superior motion amplitude and temporal consistency in video synthesis. Notably,
it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial
Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming
existing open-source models and matching or surpassing state-of-the-art
commercial solutions. We hope this technical report will help the community
more efficiently train high-quality video generation models and accelerate
progress in video generation technologies. Official page:
https://github.com/FoundationVision/Waver.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Waver：以挥手之姿实现逼真视频生成</div>
<div class="mono" style="margin-top:8px">我们推出Waver，一个用于统一图像与视频生成的高性能基础模型。该模型能直接生成长度5至10秒、原生分辨率720p的视频，并后续升级至1080p。在单一集成框架内，Waver同步支持文本生成视频（T2V）、图像生成视频（I2V）及文本生成图像（T2I）功能。通过引入混合流式DiT架构，我们增强了模态对齐能力并加速了训练收敛。为确保训练数据质量，建立了全流程数据筛选机制，并人工标注训练基于MLLM的视频质量评估模型以筛选最优样本。此外，我们提供详尽的训练与推理方案以促进高质量视频生成。基于这些创新，Waver在捕捉复杂运动方面表现卓越，实现了视频合成中卓越的运动幅度与时间一致性。值得注意的是，截至2025年7月30日北京时间10时，该模型在Artificial Analysis平台的T2V和I2V排行榜均位列前三，持续超越现有开源模型，媲美或领先最先进商业解决方案。我们希望本技术报告能助力社区更高效训练高质量视频生成模型，加速视频生成技术发展。官方页面：https://github.com/FoundationVision/Waver。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for high-quality unified image and video generation, Waver introduces a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence, alongside a rigorous data curation pipeline using an MLLM-based video quality model to filter training samples. The method supports text-to-video, image-to-video, and text-to-image generation in a single framework, producing 5-10 second videos at 720p native resolution upscaled to 1080p. Experimental results show Waver excels in complex motion capture and temporal consistency, ranking Top 3 on T2V and I2V leaderboards, outperforming open-source models and matching state-of-the-art commercial solutions.</div>
<div class="mono" style="margin-top:8px">Waver 是一个高性能的基础模型，旨在统一图像和视频生成，其动机是在单一框架中生成逼真的高分辨率视频并捕捉复杂运动。该方法采用混合流 DiT 架构以增强模态对齐和训练收敛，同时通过基于 MLLM 的视频质量模型构建严格的数据筛选流程来确保训练样本质量。实验结果表明，Waver 可生成长达 5-10 秒、原生 720p（可放大至 1080p）的视频，具有出色的运动幅度和时间一致性，在 T2V 和 I2V 排行榜中位列前三，性能匹配或超越当前最先进的商业模型。</div>
</details>
</div>
<div class="card">
<div class="title">LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on   Challenging Queries</div>
<div class="meta-line">Authors: Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song</div>
<div class="meta-line">First: 2025-08-21T17:55:54+00:00 · Latest: 2025-08-21T17:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2508.15760v1">Abs</a> · <a href="http://arxiv.org/pdf/2508.15760v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tool calling has emerged as a critical capability for AI agents to interact
with the real world and solve complex tasks. While the Model Context Protocol
(MCP) provides a powerful standardized framework for tool integration, there is
a significant gap in benchmarking how well AI agents can effectively solve
multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In
this work, we present LiveMCP-101, a benchmark of 101 carefully curated
real-world queries, refined through iterative LLM rewriting and manual review,
that require coordinated use of multiple MCP tools including web search, file
operations, mathematical reasoning, and data analysis. Moreover, we introduce a
novel evaluation approach that leverages ground-truth execution plans rather
than raw API outputs, better reflecting the evolving nature of real-world
environments. Experiments show that even frontier LLMs achieve a success rate
below 60\%, highlighting major challenges in tool orchestration. Detailed
ablations and error analysis further reveal distinct failure modes and
inefficiencies in token usage, pointing to concrete directions for advancing
current models. LiveMCP-101 sets a rigorous standard for evaluating real-world
agent capabilities, advancing toward autonomous AI systems that reliably
execute complex tasks through tool use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiveMCP-101：对支持MCP的智能体进行压力测试与复杂查询诊断</div>
<div class="mono" style="margin-top:8px">工具调用已成为AI智能体与现实世界交互并解决复杂任务的关键能力。虽然模型上下文协议（MCP）为工具集成提供了强大的标准化框架，但在基准测试AI智能体如何在真实动态场景中有效使用多样化MCP工具解决多步骤任务方面存在显著空白。本研究推出LiveMCP-101基准测试集，包含101个精心筛选的真实世界查询，通过迭代式LLM重写和人工审核优化，要求协调使用包括网络搜索、文件操作、数学推理和数据分析在内的多种MCP工具。此外，我们引入了一种新颖的评估方法，利用真实执行计划而非原始API输出，更好地反映现实环境的动态特性。实验表明，即使前沿LLMs的成功率也低于60%，突显了工具协调方面的重大挑战。详细的消融研究和错误分析进一步揭示了不同的故障模式和令牌使用效率低下的问题，为改进现有模型指明了具体方向。LiveMCP-101为评估真实世界智能体能力设立了严格标准，推动通过工具使用可靠执行复杂任务的自主AI系统发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to benchmark AI agents&#x27; real-world tool-use capabilities beyond simple API calls, this work introduces LiveMCP-101, a benchmark of 101 complex queries requiring multi-step tool orchestration via the Model Context Protocol. The method involves curating realistic queries through LLM rewriting and manual refinement, and employs a novel evaluation based on ground-truth execution plans to reflect dynamic environments. Experimental results show that even top-performing LLMs achieve below 60% success, with detailed error analysis revealing tool coordination failures and token inefficiencies, highlighting critical challenges for future agent development.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过模型上下文协议（MCP）评估AI代理在现实场景中使用多样化工具解决多步骤任务的能力，为此引入了LiveMCP-101基准，包含101个需要协调工具使用的真实查询。方法上采用迭代式LLM重写和人工审查来精炼查询，并创新性地使用真实执行计划进行评估以更好地反映动态环境。实验结果表明，即使前沿大语言模型成功率也低于60%，详细错误分析揭示了不同的失败模式和令牌使用低效问题，突显了工具协调的挑战并为模型改进指明了方向。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
